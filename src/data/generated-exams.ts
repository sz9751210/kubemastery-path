// Auto-generated by scripts/generate-exams.mjs
import { Lesson } from './types';

export const generatedExams: Record<string, Lesson> = {
  "1": {
    "id": "1",
    "title": "Container Basics",
    "category": "CORE",
    "duration": "20 mins",
    "markdown": "\n# Container Basics: Beyond Docker\n\nTo master Kubernetes, you must strictly understand concepts like **Namespaces**, **Cgroups**, and the **CRI**.\n\n## What is a Container?\nIt's just a process! But it's isolated.\n- **Namespaces**: Isolate what the process *sees* (PID, Network, Mounts).\n- **Cgroups**: Isolate what the process *uses* (CPU, RAM).\n- **Union Filesystem**: Efficient, layered storage (OverlayFS).\n\n## Docker vs CRI\nKubernetes uses the **Container Runtime Interface (CRI)**.\n> [!IMPORTANT]\n> \\`dockershim\\` is dead. You are likely using **containerd** or **CRI-O**. Learn \\`crictl\\`.\n\n\\`\\`\\`bash\n# Check the container runtime version\ncrictl version\n\n# List running containers\ncrictl ps\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "2": {
    "id": "2",
    "title": "Pod Lifecycle",
    "category": "CORE",
    "duration": "25 mins",
    "markdown": "\n# The Pod Lifecycle\n\nA **Pod** is the atomic unit of K8s. It wraps one or more containers.\n\n## States\n1. **Pending**: Scheduler is finding a node.\n2. **ContainerCreating**: Pulling images.\n3. **Running**: At least one container is up.\n4. **Succeeded/Failed**: Process exited.\n\n\\`\\`\\`bash\n# Create a simple Nginx pod\nkubectl run nginx-demo --image=nginx\n\n# Watch the pod status change\nkubectl get pod nginx-demo -w\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "3": {
    "id": "3",
    "title": "YAML Configuration",
    "category": "CORE",
    "duration": "30 mins",
    "markdown": "\n# Mastering YAML\n\nKubernetes is **Declarative**. syntax matters.\n\n## The 4 Pillars\n1. **apiVersion**: \\`v1\\`, \\`apps/v1\\`.\n2. **kind**: \\`Pod\\`, \\`Service\\`.\n3. **metadata**: \\`name\\`, \\`labels\\`.\n4. **spec**: The desired state.\n\n\\`\\`\\`yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: demo\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "4": {
    "id": "4",
    "title": "Multi-Container Pods",
    "category": "CORE",
    "duration": "35 mins",
    "markdown": "\n# Multi-Container Pods\n\nSharing is caring. Sometimes one container isn't enough.\n\n## Patterns\n1.  **Sidecar**: Helper container (e.g., Log shipper, Proxy).\n2.  **Adapter**: Standardizes output (e.g., Metrics converter).\n3.  **Ambassador**: Proxies connection to outside world.\n\n## Shared Resources\nContainers in a Pod share:\n-   **Network Namespace**: Same IP, same localhost.\n-   **Volumes**: Shared filesystems.\n\n\\`\\`\\`yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sidecar-demo\nspec:\n  containers:\n  - name: main-app\n    image: nginx\n    volumeMounts:\n    - name: shared-logs\n      mountPath: /var/log/nginx\n  - name: log-shipper\n    image: busybox\n    command: [\"sh\", \"-c\", \"tail -f /var/log/nginx/access.log\"]\n    volumeMounts:\n    - name: shared-logs\n      mountPath: /var/log/nginx\n  volumes:\n  - name: shared-logs\n    emptyDir: {}\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "5": {
    "id": "5",
    "title": "Cluster Architecture",
    "category": "CKA/CKAD",
    "duration": "40 mins",
    "markdown": "\n# Cluster Architecture\n\nUnderstand the machine you are driving.\n\n## The Two Planes\n1.  **Control Plane** (The Brain):\n    -   Manages the state of the cluster.\n    -   Components: API Server, Etcd, Scheduler, Controller Manager.\n2.  **Data Plane** (The Muscle):\n    -   Runs the workloads.\n    -   Components: Kubelet, Kube-Proxy, Container Runtime.\n\n## Kubelet\nThe agent that runs on every node. It registers the node with the apiserver and ensures Pods are running and healthy.\n\n\\`\\`\\`bash\n# Check node status\nkubectl get nodes -o wide\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "6": {
    "id": "6",
    "title": "Services & Networking",
    "category": "CKA/CKAD",
    "duration": "45 mins",
    "markdown": "\n# Services: Exposing Applications\n\n## Service Types\n1. **ClusterIP**: Internal only.\n2. **NodePort**: Exposure on static port (30000+).\n3. **LoadBalancer**: Cloud provider LB.\n\n\\`\\`\\`bash\n# Expose a deployment\nkubectl expose deployment my-dep --port=80 --target-port=80 --type=NodePort --name=my-svc\n\n# Get the service details\nkubectl get svc my-svc\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "7": {
    "id": "7",
    "title": "Storage (PV/PVC)",
    "category": "CKA/CKAD",
    "duration": "35 mins",
    "markdown": "\n# Storage Orchestration\n\nDecoupling storage from Pod lifecycle.\n\n## PV vs PVC\n- **PV (PersistentVolume)**: The physical storage resource.\n- **PVC (Claim)**: The request for storage.\n\n\\`\\`\\`bash\n# List Persistent Volumes\nkubectl get pv\n\\`\\`\\`\n\n## Access Modes\n- **RWO**: ReadWriteOnce (Block storage usually).\n- **RWX**: ReadWriteMany (NFS/File storage).\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "8": {
    "id": "8",
    "title": "Scheduling",
    "category": "CKA/CKAD",
    "duration": "30 mins",
    "markdown": "\n# Scheduling\n\nControlling where Pods go.\n\n## Taints & Tolerations\n\"Repel\" pods from nodes.\n- Master nodes are tainted \\`NoSchedule\\`.\n\n\\`\\`\\`bash\n# View taints on nodes\nkubectl describe node controlplane | grep Taint\n\\`\\`\\`\n\n## Affinity\n\"Attract\" pods to nodes.\n- \\`nodeAffinity\\`: Run on nodes with SSD.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "9": {
    "id": "9",
    "title": "Cluster Hardening",
    "category": "CKS",
    "duration": "50 mins",
    "markdown": "\n# Cluster Hardening\n\nLocking down the fortress.\n\n## Center for Internet Security (CIS) Benchmarks\nThe gold standard for K8s security. \\`kube-bench\\` is a common tool to check these.\n\n## Key Principles\n1.  **Least Privilege**: RBAC, SecurityContexts.\n2.  **Minimize Attack Surface**: Remove shells, use distroless images.\n3.  **encrypt-secret-data-at-rest**: Check etcd configuration.\n\n## Platform Binary Verification\nEnsure the binaries (kubelet, kubectl, kube-apiserver) are legitimate.\n- Check SHA512 sums against official releases.\n\n## Minimize GUI Use\nThe Kubernetes Dashboard is a frequent attack vector.\n- **Best Practice**: Disable it in production.\n\n\\`\\`\\`bash\n# Check if anonymous auth is enabled (Should be false)\nps -ef | grep kube-apiserver | grep anonymous-auth\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "10": {
    "id": "10",
    "title": "Network Policies",
    "category": "CKS",
    "duration": "35 mins",
    "markdown": "\n# Network Policies\n\nThe firewall for Kubernetes.\n\n> [!WARNING]\n> By default, K8s is a **Flat Network**. Any pod can talk to any pod.\n\n## Default Deny Logic\nStart by denying everything, then allow specific traffic.\n\n\\`\\`\\`bash\n# See if any policies exist\nkubectl get networkpolicies\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "11": {
    "id": "11",
    "title": "System Hardening",
    "category": "CKS",
    "duration": "45 mins",
    "markdown": "\n# System Hardening\n\nKubernetes is only as secure as the Linux nodes it runs on.\n\n## Kernel Hardening\n-   **Seccomp**: Restrict syscalls.\n-   **AppArmor**: Restrict file access / capabilities.\n\n## Container Sandboxing\nFor high-risk workloads, standard containers (shared kernel) might not be enough.\n-   **gVisor (runsc)**: Userspace kernel, intercepts syscalls.\n-   **Kata Containers**: Lightweight VMs for strong isolation.\n\n## Reducing Attack Surface\n-   Disable unused services (SSH, FTP).\n-   Firewall rules (UFW/IPTables) to restrict node-to-node access outside of K8s ports.\n\n\\`\\`\\`bash\n# Check open ports on the node\nnetstat -tulpn\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "12": {
    "id": "12",
    "title": "ReplicaSets & Deployments",
    "category": "CORE",
    "duration": "25 mins",
    "markdown": "\n# ReplicaSets & Deployments\n\nEnsuring your application is always running and easily updated.\n\n## ReplicaSet\nThe primary goal of a **ReplicaSet** is to maintain a stable set of replica Pods running at any given time.\n\n\\`\\`\\`bash\n# Look at the replicasets in your cluster\nkubectl get rs\n\\`\\`\\`\n\n## Deployment\n**Deployments** allow you to manage the rollout of new versions of your application.\n\n\\`\\`\\`bash\n# Create a deployment with 3 replicas\nkubectl create deployment web-server --image=nginx --replicas=3\n\n# Update the image\nkubectl set image deployment/web-server nginx=nginx:1.16.1\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "13": {
    "id": "13",
    "title": "Jobs & CronJobs",
    "category": "CORE",
    "duration": "20 mins",
    "markdown": "\n# Jobs & CronJobs\n\nFor tasks that are intended to run to completion.\n\n## Jobs\nA **Job** creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.\n\n\\`\\`\\`bash\n# Run a one-off job\nkubectl create job hello --image=busybox -- echo \"Hello Kubernetes\"\n\\`\\`\\`\n\n## CronJobs\nA **CronJob** creates Jobs on a repeating schedule.\n\n\\`\\`\\`bash\n# Create a cronjob\nkubectl create cronjob heartbeat --image=busybox --schedule=\"*/1 * * * *\" -- echo \"Beating...\"\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "14": {
    "id": "14",
    "title": "Control Plane & API Server",
    "category": "CKA/CKAD",
    "duration": "45 mins",
    "markdown": "\n# Control Plane & API Server\n\nThe brains of the cluster.\n\n## Components\n- **kube-apiserver**: The gateway for all REST commands.\n- **etcd**: Consistent and highly-available key value store.\n- **kube-scheduler**: Watches for newly created Pods with no assigned node.\n- **kube-controller-manager**: Runs controller processes.\n\n\\`\\`\\`bash\n# Check the status of control plane components\nkubectl get pods -n kube-system -l tier=control-plane\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "15": {
    "id": "15",
    "title": "Troubleshooting & Logs",
    "category": "CKA/CKAD",
    "duration": "40 mins",
    "markdown": "\n# Troubleshooting & Logs\n\nWhen things go wrong, here is how to find out why.\n\n## Basic Commands\n- \\`kubectl describe\\`: Detailed information about a resource.\n- \\`kubectl logs\\`: Print the logs for a container in a pod.\n\n## Common Pod States\n- **Pending**: Scheduler can't find a node (CPU/RAM insufficient, Taints).\n- **CrashLoopBackOff**: Application is starting and immediately failing. Check logs!\n- **ImagePullBackOff**: Can't pull image (Auth, Typo, Network).\n\n## Debugging Services\nService not working? Check the **Endpoints**.\n\\`\\`\\`bash\n# Does the service have endpoints?\nkubectl get endpoints my-svc\n\\`\\`\\`\n\n\\`\\`\\`bash\n# Describe a failing pod\nkubectl describe pod <pod-name>\n\n# Check logs for a specific pod (and previous instance if crashed)\nkubectl logs <pod-name> --previous\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "16": {
    "id": "16",
    "title": "Admission Controllers",
    "category": "CKS",
    "duration": "50 mins",
    "markdown": "\n# Admission Controllers\n\nIntercepting requests to the API server before an object is persisted.\n\n## Types\n- **Mutating**: Can modify the object.\n- **Validating**: Can only accept or reject.\n\n\\`\\`\\`bash\n# Check which admission plugins are enabled\nkubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep enable-admission-plugins\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "17": {
    "id": "17",
    "title": "Runtime Security",
    "category": "CKS",
    "duration": "45 mins",
    "markdown": "\n# Runtime Security\n\nProtecting the running process.\n\n## Tools\n- **Falco**: The runtime security engine. Uses rules to detect abnormal behavior (e.g., shell in container, modifying /etc).\n- **AppArmor**: Restrict programs' capabilities with per-program profiles.\n- **Seccomp**: Restrict system calls a process can make.\n\n## Behavioral Analytics\nDetecting threats based on *patterns* rather than known signatures.\n- Process spawning unexpected children.\n- Unexpected outbound network connections.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "18": {
    "id": "18",
    "title": "CRDs & API Extensions",
    "category": "EXPERT",
    "duration": "60 mins",
    "markdown": "\n# CRDs & API Extensions (Deep Dive)\n\nMaking Kubernetes your own.\n\n## Aggregation Layer\nAllows the Kubernetes API server to be extended with additional APIs.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "20": {
    "id": "20",
    "title": "Workloads: Deployments",
    "category": "CKA/CKAD",
    "duration": "45 mins",
    "markdown": "\n# Workloads: Deployments & ReplicaSets\n\nPods are ephemeral. **Deployments** provide declarative updates for Pods and ReplicaSets.\n\n## The Hierarchy\n\\`Deployment\\` manages \\`ReplicaSet\\` manages \\`Pod\\`.\n\n\\`\\`\\`bash\n# Create a deployment\nkubectl create deployment my-dep --image=nginx --replicas=3\n\\`\\`\\`\n\n## Features\n- **Self-healing**: Restarts crashed pods.\n- **Scaling**: \n\\`\\`\\`bash\nkubectl scale deployment my-dep --replicas=5\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "21": {
    "id": "21",
    "title": "StatefulSets & DaemonSets",
    "category": "CKA/CKAD",
    "duration": "40 mins",
    "markdown": "\n# StatefulSets & DaemonSets\n\nNot everything is a stateless web server.\n\n## StatefulSet\nUsed for databases or apps needing:\n- **Stable Network ID**: \\`web-0\\`, \\`web-1\\`.\n- **Stable Storage**: VolumeClaims templates.\n\n## DaemonSet\nEnsures a copy of a Pod runs on **all** (or some) Nodes.\n- Examples: \\`kube-proxy\\`, \\`fluentd\\` (logging).\n\n\\`\\`\\`bash\n# Check the kube-proxy daemonset\nkubectl get ds -n kube-system\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "22": {
    "id": "22",
    "title": "Ingress & DNS",
    "category": "CKA/CKAD",
    "duration": "50 mins",
    "markdown": "\n# Ingress & Advanced Networking\n\n**Services** operate at Layer 4 (TCP/UDP). **Ingress** operates at Layer 7 (HTTP/HTTPS).\n\n## Ingress Controller\nAn Ingress resource does nothing without a controller (e.g., Nginx, Traefik).\n\n\\`\\`\\`bash\n# Check for ingress controllers\nkubectl get pods -n ingress-nginx\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "30": {
    "id": "30",
    "title": "RBAC: Authorization",
    "category": "CKS",
    "duration": "60 mins",
    "markdown": "\n# Role Based Access Control (RBAC)\n\n**Authentication** (Who are you?) vs **Authorization** (What can you do?).\n\n## Core Objects\n1. **Role**: Rules (verbs + resources) scoped to a **Namespace**.\n2. **ClusterRole**: Rules scoped linearly (Cluster-wide).\n3. **RoleBinding**: Connecting a Subject (User/ServiceAccount) to a Role.\n4. **ClusterRoleBinding**: Connecting a Subject to a ClusterRole.\n\n\\`\\`\\`bash\n# Check my permissions\nkubectl auth can-i create pods\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "31": {
    "id": "31",
    "title": "Secrets Management",
    "category": "CKS",
    "duration": "40 mins",
    "markdown": "\n# Secrets & Security Contexts\n\n## Secrets\nBase64 encoded data. NOT encrypted by default unless EncryptionAtRest is enabled in etcd.\n\n\\`\\`\\`bash\n# Create a generic secret\nkubectl create secret generic my-pass --from-literal=password=secret123\n\\`\\`\\`\n\n## Security Contexts\nDefine privileges and access control for a Pod/Container.\n- \\`runAsUser\\`: UID to run process.\n- \\`fsGroup\\`: GID for volumes.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "40": {
    "id": "40",
    "title": "Helm: Package Management",
    "category": "EXPERT",
    "duration": "50 mins",
    "markdown": "\n# Helm: Kubernetes Package Manager\n\nManaging thousands of YAML files is painful. **Helm** solves this using **Charts**.\n\n## Concepts\n- **Chart**: A bundle of information necessary to create an instance of a Kubernetes application.\n- **Release**: A running instance of a chart.\n\n\\`\\`\\`bash\n# List helm releases\nhelm list -A\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "41": {
    "id": "41",
    "title": "Operators & CRDs",
    "category": "EXPERT",
    "duration": "60 mins",
    "markdown": "\n# Operators & Custom Resource Definitions (CRDs)\n\nExtending the Kubernetes API.\n\n## CRD (Custom Resource Definition)\nAllows you to define your own API resources.\n\n\\`\\`\\`bash\n# List custom resources\nkubectl get crd\n\\`\\`\\`\n\n## The Operator Pattern\nAn Operator is a Controller that watches a CRD and acts on it.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "42": {
    "id": "42",
    "title": "Service Mesh (Istio)",
    "category": "EXPERT",
    "duration": "60 mins",
    "markdown": "\n# Service Mesh\n\nManaging network traffic between services (East-West traffic).\n\n## Why use a Mesh?\n1. **Observability**: Tracing, Metrics.\n2. **Traffic Control**: Canary types.\n3. **Security**: mTLS.\n\n\\`\\`\\`bash\n# Check for istio proxy sidecars\nkubectl get pods -l istio-injection=enabled\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "50": {
    "id": "50",
    "title": "Cluster Maintenance",
    "category": "CKA",
    "duration": "45 mins",
    "markdown": "\n# Cluster Maintenance\n\nKeeping the lights on.\n\n## Cluster Upgrade\nUsing \\`kubeadm\\` to upgrade the cluster.\n1.  Upgrade Control Plane node.\n2.  Upgrade Worker nodes.\n\n\\`\\`\\`bash\n# Drain the node first\nkubectl drain node-1 --ignore-daemonsets\n\n# Upgrade kubeadm\napt-get update && apt-get install -y kubeadm=1.27.0-00\n\n# Plan the upgrade\nkubeadm upgrade plan\n\n# Apply\nkubeadm upgrade apply v1.27.0\n\\`\\`\\`\n\n## Backup & Restore\n**Etcd** is the source of truth. You MUST know how to snapshot it.\n\n\\`\\`\\`bash\n# Take a snapshot\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\\\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\\\n  snapshot save /tmp/etcd-backup.db\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "51": {
    "id": "51",
    "title": "Security & Identity",
    "category": "CKA",
    "duration": "40 mins",
    "markdown": "\n# Security & Identity\n\n## Authentication (AuthN)\nWho are you?\n-   **Certificates**: Most common for users/admins (CN=User, O=Group).\n-   **ServiceAccounts**: For Pods.\n\n\\`\\`\\`bash\n# Create a CSR (CertificateSigningRequest)\nkubectl create -f my-user-csr.yaml\nkubectl certificate approve my-user\n\\`\\`\\`\n\n## Authorization (AuthZ) & RBAC\nWhat can you do?\nSee 'RBAC: Authorization' lesson for details. Kubeadm enables Node and RBAC authorizers by default.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "52": {
    "id": "52",
    "title": "Advanced Storage",
    "category": "CKA/CKAD",
    "duration": "35 mins",
    "markdown": "\n# Advanced Storage\n\n## Storage Class\nDefines \"classes\" of storage (e.g., \"fast\", \"slow\"). Enables **Dynamic Provisioning**.\n\n\\`\\`\\`yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast\nprovisioner: k8s.io/minikube-hostpath\n\\`\\`\\`\n\n## Dynamic Provisioning\nInstead of creating a PV manually, creating a PVC with a \\`storageClassName\\` triggers the provisioner to create the PV automatically.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "53": {
    "id": "53",
    "title": "Cluster Networking",
    "category": "CKA",
    "duration": "50 mins",
    "markdown": "\n# Cluster Networking\n\n## CNI (Container Network Interface)\nThe plugin that configures network interfaces.\n-   **Flannel, Calico, Weave**.\n-   Every Pod gets a unique IP.\n\n## DNS (CoreDNS)\nService discovery.\n-   Service: \\`my-svc.my-ns.svc.cluster.local\\`\n-   Pod: \\`1-2-3-4.my-ns.pod.cluster.local\\`\n\n\\`\\`\\`bash\n# Debug DNS\nkubectl run busybox --image=busybox:1.28 --restart=Never -- sleep 3600\nkubectl exec -it busybox -- nslookup kubernetes\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "54": {
    "id": "54",
    "title": "Observability",
    "category": "CKA",
    "duration": "30 mins",
    "markdown": "\n# Observability\n\n## Metrics Server\nAggregates resource usage data. Required for \\`kubectl top\\` and HPA (Horizontal Pod Autoscaler).\n\n\\`\\`\\`bash\n# Check node usage\nkubectl top nodes\n\n# Check pod usage\nkubectl top pods\n\\`\\`\\`\n\n## Logging architecture\n-   **Node level**: \\`/var/log/containers\\`\n-   **Cluster level**: Fluentd/Prometheus stack (usually).\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "60": {
    "id": "60",
    "title": "Supply Chain Security",
    "category": "CKS",
    "duration": "45 mins",
    "markdown": "\n# Supply Chain Security\n\nSecuring the pipeline from code to cluster.\n\n## Image Scanning\nFind vulnerabilities (CVEs) in your images *before* they run.\n- **Tools**: Trivy, Clair.\n- **Action**: Fix Critical/High CVEs.\n\n\\`\\`\\`bash\n# Scan an image with Trivy\ntrivy image nginx:latest\n\\`\\`\\`\n\n## Image Signing\nProve the image comes from you and hasn't been tampered with.\n- **Cosign (Sigstore)**: Sign and verify container images.\n\n## Static Analysis\nScan your YAML manifests for misconfigurations.\n- **Kubesec**: Security risk analysis for Kubernetes resources.\n- **KubeLinter**: Static analysis tool.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "61": {
    "id": "61",
    "title": "Audit Logging & Monitoring",
    "category": "CKS",
    "duration": "50 mins",
    "markdown": "\n# Audit Logging & Monitoring\n\nIf you didn't log it, it didn't happen.\n\n## Audit Logs\nRecords the sequence of actions taken by the cluster (API Server).\n- **Stages**: RequestReceived, ResponseStarted, ResponseComplete, Panic.\n- **Policy**: Defines rules for what to log and how much detail.\n\n\\`\\`\\`yaml\n# audit-policy.yaml example\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"secrets\"]\n\\`\\`\\`\n\n## Monitoring (Syscall)\nUsing **Falco** to monitor system calls at the kernel level.\n- Detects: File access, Process execution, Network activity.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "62": {
    "id": "62",
    "title": "Advanced Pod Security",
    "category": "CKS",
    "duration": "45 mins",
    "markdown": "\n# Advanced Pod Security\n\n## Pod Security Standards (PSS)\nReplaced PodSecurityPolicies (PSP). Defined in 3 levels:\n1. **Privileged**: Unrestricted (Admin level).\n2. **Baseline**: Minimally restrictive (Default).\n3. **Restricted**: Heavily restricted (Best practice).\n\n## Pod Security Admission (PSA)\nThe built-in admission controller to enforce PSS via Namespace labels.\n\n\\`\\`\\`bash\n# Enforce restricted standard on 'dev' namespace\nkubectl label ns dev pod-security.kubernetes.io/enforce=restricted\n\\`\\`\\`\n\n## mTLS (Mutual TLS)\nEncrypting traffic *between* pods.\n- Usually handled by a Service Mesh (Linkerd, Istio), but understanding the concept is key for CKS.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "70": {
    "id": "70",
    "title": "Application Build & Images",
    "category": "CKAD",
    "duration": "40 mins",
    "markdown": "\n# Application Build & Images\n\n## Dockerfile Best Practices\n- **Multi-stage builds**: Keep the final image small by separating build and runtime environments.\n- **User Permissions**: Never run as root. Use \\`USER\\`.\n- **Layers**: Combine commands to reduce layers.\n\n\\`\\`\\`dockerfile\n# Multi-stage example\nFROM golang:1.21 as builder\nWORKDIR /app\nCOPY . .\nRUN go build -o myapp\n\nFROM alpine:latest\nWORKDIR /root/\nCOPY --from=builder /app/myapp .\nCMD [\"./myapp\"]\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "71": {
    "id": "71",
    "title": "Advanced Deployment Strategies",
    "category": "CKAD",
    "duration": "50 mins",
    "markdown": "\n# Advanced Deployment Strategies\n\n## Blue/Green Deployment\n- Two identical environments.\n- **Switch**: Update the Service selector to point to the new version.\n- **Pros**: Instant rollback. **Cons**: Double resources.\n\n## Canary Deployment\n- Roll out to a small subset of users.\n- **Implementation**: Two Deployments (Stable & Canary) with common labels targeted by one Service.\n- **Traffic Splitting**: Use Ingress or Service Mesh for % based splitting.\n\n## Rolling Update\n- Default strategy.\n- **Parameters**: \\`maxSurge\\` (how many extra) and \\`maxUnavailable\\` (how many down).\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "72": {
    "id": "72",
    "title": "Application Observability",
    "category": "CKAD",
    "duration": "45 mins",
    "markdown": "\n# Application Observability\n\n## Probes\nKubelet needs to know if your app is okay.\n1.  **Liveness**: Restart if dead (deadlock).\n2.  **Readiness**: Don't send traffic until ready (loading data).\n3.  **Startup**: Wait for slow starts before other probes kick in.\n\n\\`\\`\\`yaml\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 3\n  periodSeconds: 3\n\\`\\`\\`\n\n## Debugging\n- **Logs**: \\`kubectl logs my-pod -c my-container\\`\n- **Exec**: \\`kubectl exec -it my-pod -- sh\\`\n- **Ephemeral Containers**: \\`kubectl debug -it my-pod --image=busybox --target=my-container\\` (Great for distroless images).\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "73": {
    "id": "73",
    "title": "Services & Networking Design",
    "category": "CKAD",
    "duration": "40 mins",
    "markdown": "\n# Services & Networking Design\n\n## Network Policies\nIsolate your applications.\n- **Ingress**: Incoming traffic.\n- **Egress**: Outgoing traffic.\n\n## Service Discovery\n- **DNS**: Every Service gets a DNS name.\n- **Debugging DNS**: Use \\`nslookup\\` or \\`dig\\` from within a pod.\n\n\\`\\`\\`bash\n# Test connectivity\nkubectl run test --rm -it --image=busybox -- wget -O- http://my-service\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "90": {
    "id": "90",
    "title": "Command Line Fu: JSONPath",
    "category": "CKA/CKAD",
    "duration": "45 mins",
    "markdown": "\n# Command Line Fu: JSONPath & Custom Columns\n\nMastering \\`kubectl\\` output is mandatory for CKA/CKAD.\n\n## JSONPath\nFilter and format output programmatically.\n- **Syntax**: \\`{.items[*].metadata.name}\\`\n\n\\`\\`\\`bash\n# Get all pod names\nkubectl get pods -o jsonpath='{.items[*].metadata.name}'\n\n# Get InternalIP of all nodes\nkubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"InternalIP\")].address}'\n\\`\\`\\`\n\n## Custom Columns\nCreate your own table output.\n\\`\\`\\`bash\n# Show Pod Name and Node Name\nkubectl get pods -o custom-columns=POD:.metadata.name,NODE:.spec.nodeName\n\\`\\`\\`\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "91": {
    "id": "91",
    "title": "Cluster Bootstrap Deep Dive",
    "category": "CKA",
    "duration": "60 mins",
    "markdown": "\n# Cluster Bootstrap: Kubeadm\n\nUnderstanding how the cluster comes alive.\n\n## The Init Process (\\`kubeadm init\\`)\n1.  **Pre-flight checks**: Kernel version, Swap disabled?\n2.  **Certs**: Generates CA, API server, Etcd certs in \\`/etc/kubernetes/pki\\`.\n3.  **Kubeconfigs**: Generates admin.conf, kubelet.conf.\n4.  **Manifests**: Static pods for Control Plane (API, Controller, Sched, Etcd) in \\`/etc/kubernetes/manifests\\`.\n5.  **Taints**: Marks node as ControlPlane (NoSchedule).\n6.  **Bootstrap Token**: For workers to join.\n\n## CNI Installation\nThe cluster is **NotReady** until a CNI (Network Plugin) is installed.\n- The CNI binaires go to \\`/opt/cni/bin\\`.\n- The config goes to \\`/etc/cni/net.d\\`.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "99": {
    "id": "99",
    "title": "Chaos Challenge: The Broken Cluster",
    "category": "EXPERT",
    "duration": "60 mins",
    "markdown": "\n# Chaos Challenge: The Broken Cluster\n\n> [!WARNING]\n> This is a **live fire** exercise. We have deliberately broken this cluster.\n\n## Scenario\nYou are the on-call Site Reliability Engineer. Minutes ago, all \\`kubectl\\` commands started failing.\n\n## Your Mission\n1.  **Diagnose** why the API Server is failing.\n2.  **Fix** the underlying issue.\n3.  **Restore** cluster connectivity.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "100": {
    "id": "100",
    "title": "Service Internals: Iptables vs IPVS",
    "category": "INTERNALS",
    "duration": "60 mins",
    "markdown": "\n# Service Implementation: Deep Dive\n\nHow does a Service IP (ClusterIP) actually work? It's virtual! It doesn't exist on any interface.\n\n## kube-proxy\nThe component responsible for watching the API Server for Services/Endpoints and configuring rules.\n\n## Iptables Mode (Default)\nTraffic is captured by PREROUTING/OUTPUT chains and redirected.\n- **Chain KUBE-SERVICES**: The entry point.\n- **Chain KUBE-SVC-***: Round-robin load balancing (using statistic mode random probability).\n- **Chain KUBE-SEP-***: Service EndPoint (DNAT to actual Pod IP).\n\n\\`\\`\\`bash\n# View service rules\nsudo iptables -t nat -L KUBE-SERVICES\n\\`\\`\\`\n\n## IPVS Mode\nUses Linux Kernel's IP Virtual Server (Netfilter).\n- **Performance**: O(1) matching vs O(n) for iptables (sequential scan).\n- **Scalability**: Better for thousands of services.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "101": {
    "id": "101",
    "title": "CNI Deep Dive: Pod Networking",
    "category": "INTERNALS",
    "duration": "75 mins",
    "markdown": "\n# CNI Deep Dive\n\nHow do Pods talk to each other across nodes?\n\n## The \"Pause\" Container\nExists solely to hold the Network Namespace. Application containers join this namespace (share localhost).\n\n## VETH Pairs (Virtual Ethernet)\nConnects the Pod namespace to the Host namespace.\n- **eth0 (Pod)** <--> **veth*** (Host).\n\n## Bridge Mode (e.g., cbr0)\nVETH ends on the host are connected to a Bridge. The Bridge acts as a virtual switch.\n\n## Overlay Networks (VXLAN / IPIP)\nEncapsulating L2 frames inside L3 packets to cross node boundaries.\n- **Flannel**: UDP/VXLAN.\n- **Calico**: BGP (Direct Routing) or IPIP.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "102": {
    "id": "102",
    "title": "Etcd & Raft Consensus",
    "category": "INTERNALS",
    "duration": "60 mins",
    "markdown": "\n# Etcd & The Raft Consensus Algorithm\n\nHow does Kubernetes maintain a consistent state across multiple control plane nodes? It relies entirely on **Etcd**.\n\n## What is Etcd?\nEtcd is a distributed, reliable key-value store. It is the \"source of truth\" for Kubernetes.\n\n## The Raft Algorithm\nEtcd uses **Raft** to achieve consensus.\n1.  **Leader Election**: One node is elected leader. All writes go to the leader.\n2.  **Log Replication**: The leader replicates the entry to followers.\n3.  **Commit**: Once a majority (Quorum) acknowledges the entry, it is committed.\n\n### Quorum\n$N/2 + 1$. For a 3-node cluster, you need 2 nodes to agree.\n-   **3 Nodes**: Tolerate 1 failure.\n-   **5 Nodes**: Tolerate 2 failures.\n\n```bash\n# Check endpoint health\nETCDCTL_API=3 etcdctl endpoint health\n```\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "103": {
    "id": "103",
    "title": "The Controller Pattern",
    "category": "INTERNALS",
    "duration": "70 mins",
    "markdown": "\n# The Controller Pattern & Reconciliation Loop\n\nKubernetes is a system of **eventually consistent** control loops.\n\n## The Loop\n1.  **Observe**: Watch the current state (via API Server).\n2.  **Diff**: Compare against desired state (Spec).\n3.  **Act**: Make changes to move closer to desired state.\n\n## Under the Hood (client-go)\nHow does a controller actually work?\n1.  **Reflector**: Watches the API and puts objects in a localized Queue (`DeltaFIFO`).\n2.  **Informer**: Reads from Queue and updates the **Local Store (Cache)**. This avoids hammering the API server.\n3.  **Workqueue**: When an object changes, its key (e.g., `default/my-pod`) is added here.\n4.  **Reconciler**: The worker logic that pops the key and does the business logic.\n\n> [!NOTE]\n> This is why \"Level Triggered\" logic is preferred over \"Edge Triggered\". If you miss an event, you just reconcile the current state next time.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "104": {
    "id": "104",
    "title": "OCI & Low-Level Runtimes",
    "category": "INTERNALS",
    "duration": "60 mins",
    "markdown": "\n# OCI & RunC: How a Container Starts\n\nWhen you ask K8s to run a Pod, `kubelet` calls the CRI (e.g., `containerd`). But what does `containerd` do?\n\n## The OCI (Open Container Initiative)\nIt defines two specs:\n1.  **Image Spec**: How the filesystem and metadata are bundled.\n2.  **Runtime Spec**: `config.json` that defines *how* to run it (namespaces, cgroups, commands).\n\n## The Flow\n`Kubelet` -> `CRI (containerd)` -> `Shim (containerd-shim)` -> `OCI Runtime (runc)` -> `Kernel`.\n\n### What is runc?\nIt is a CLI tool for spawning and running containers according to the OCI spec. It interacts directly with Linux kernel features like:\n-   **unshare**: To create namespaces.\n-   **cgroups**: To set limits.\n-   **pivot_root**: To change the root filesystem.\n\n```bash\n# You can run a container manually with runc!\nmkdir -p mycontainer/rootfs\n# Export a docker image to rootfs...\nrunc spec # Generates config.json\nrunc run mycontainer\n```\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "201": {
    "id": "201",
    "title": "CKA Mock Exam 1",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# CKA Mock Exam 1\n\nTime: 2 Hours. Pass: 66%.\n\n## Task 1: Backup Etcd\nSave a snapshot of etcd to \\`/opt/etcd-backup.db\\`.\n\n## Task 2: Upgrade Master Node\nUpgrade the controlplane node to version \\`1.27.0\\`.\n\n## Task 3: Create a Pod with Sidecar\nCreate a pod named \\`legacy-app\\` with image \\`busybox\\` that logs to \\`/var/log/legacy.log\\`. Add a sidecar container that prints this log file to stdout.\n\n## Task 4: Troubleshoot Node\nNode \\`worker-1\\` is NotReady. Fix it.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "202": {
    "id": "202",
    "title": "CKA Mock Exam 2",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# CKA Mock Exam 2\n\nTime: 2 Hours. Pass: 66%.\n\n## Task 1: Network Policy\nCreate a NetworkPolicy named \\`deny-all\\` in namespace \\`probation\\` that denies all ingress traffic.\n\n## Task 2: RBAC\nCreate a Role \\`developer\\` that can \\`create\\`, \\`get\\`, \\`list\\` pods in namespace \\`dev\\`. Bind it to user \\`jane\\`.\n\n## Task 3: Ingress\nExpose service \\`frontend\\` on path \\`/store\\` using an Ingress resource.\n\n## Task 4: Persistent Volume\nCreate a PV named \\`local-pv\\` with capacity \\`1Gi\\`, access mode \\`RWO\\`, hostPath \\`/mnt/data\\`.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": []
  },
  "auto-cka-1": {
    "id": "auto-cka-1",
    "title": "Auto CKA Practice (Batch 1)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 1)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-wa4tfj` in namespace `prod` using image `redis`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cabwwz` in namespace `dev`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ynvtnm` in namespace `kube-system`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 4: Scale Deployment\nCreate a deployment named `resource-s3ymjk` in namespace `prod` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 5: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 6: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ot0kp1` in namespace `staging`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 7: Scale Deployment\nCreate a deployment named `resource-tx51yg` in namespace `backend` using image `mysql`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n# Task 8: Scale Deployment\nCreate a deployment named `resource-jkmiv4` in namespace `backend` using image `nginx`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 9: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-6b4k6y` in namespace `kube-system`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 10: Scale Deployment\nCreate a deployment named `resource-a3uvnl` in namespace `staging` using image `nginx`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-j8hyct` in namespace `staging` using image `alpine`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-0f6mlc` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 13: Expose Service\nExpose the deployment `resource-kat4m0-dep` as a Service named `resource-kat4m0` in namespace `backend`.\nThe service should listen on port `6356` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 14: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 15: Expose Service\nExpose the deployment `resource-xfztjj-dep` as a Service named `resource-xfztjj` in namespace `kube-system`.\nThe service should listen on port `5796` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 16: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 17: Scale Deployment\nCreate a deployment named `resource-m58squ` in namespace `test` using image `python:3.9`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n# Task 18: Scale Deployment\nCreate a deployment named `resource-k06siv` in namespace `kube-system` using image `python:3.9`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n# Task 19: Expose Service\nExpose the deployment `resource-zeid4j-dep` as a Service named `resource-zeid4j` in namespace `kube-system`.\nThe service should listen on port `4588` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 20: Create a Pod\nCreate a pod named `resource-4glzo0` in namespace `staging` using image `node:14`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-wa4tfj -n prod --no-headers | grep Running\nkubectl get pod resource-wa4tfj -n prod -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get pvc resource-cabwwz -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-cabwwz -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pvc resource-ynvtnm -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-ynvtnm -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get deploy resource-s3ymjk -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-s3ymjk -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-ot0kp1 -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-ot0kp1 -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get deploy resource-tx51yg -n backend -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-tx51yg -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\nkubectl get deploy resource-jkmiv4 -n backend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-jkmiv4 -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get pvc resource-6b4k6y -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-6b4k6y -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get deploy resource-a3uvnl -n staging -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-a3uvnl -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get pod resource-j8hyct -n staging --no-headers | grep Running\nkubectl get pod resource-j8hyct -n staging -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pvc resource-0f6mlc -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-0f6mlc -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get svc resource-kat4m0 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 6356\nkubectl get svc resource-kat4m0 -n backend -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-xfztjj -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 5796\nkubectl get svc resource-xfztjj -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-m58squ -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-m58squ -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\nkubectl get deploy resource-k06siv -n kube-system -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-k06siv -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\nkubectl get svc resource-zeid4j -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4588\nkubectl get svc resource-zeid4j -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-4glzo0 -n staging --no-headers | grep Running\nkubectl get pod resource-4glzo0 -n staging -o jsonpath='{.metadata.labels.team}' | grep blue\n",
    "setupScript": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-wa4tfj -n prod --force --grace-period=0 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cabwwz -n dev 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ynvtnm -n kube-system 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-s3ymjk -n prod 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ot0kp1 -n staging 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-tx51yg -n backend 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-jkmiv4 -n backend 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-6b4k6y -n kube-system 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-a3uvnl -n staging 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-j8hyct -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-0f6mlc -n backend 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-kat4m0-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-kat4m0 -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-xfztjj-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-xfztjj -n kube-system 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-m58squ -n test 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-k06siv -n kube-system 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-zeid4j-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-zeid4j -n kube-system 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4glzo0 -n staging --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 1)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-wa4tfj` in namespace `prod` using image `redis`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-wa4tfj -n prod --no-headers | grep Running\nkubectl get pod resource-wa4tfj -n prod -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-wa4tfj -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cabwwz` in namespace `dev`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-cabwwz -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-cabwwz -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cabwwz -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ynvtnm` in namespace `kube-system`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-ynvtnm -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-ynvtnm -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ynvtnm -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Scale Deployment\nCreate a deployment named `resource-s3ymjk` in namespace `prod` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-s3ymjk -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-s3ymjk -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-s3ymjk -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ot0kp1` in namespace `staging`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-ot0kp1 -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-ot0kp1 -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ot0kp1 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Scale Deployment\nCreate a deployment named `resource-tx51yg` in namespace `backend` using image `mysql`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-tx51yg -n backend -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-tx51yg -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-tx51yg -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Scale Deployment\nCreate a deployment named `resource-jkmiv4` in namespace `backend` using image `nginx`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-jkmiv4 -n backend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-jkmiv4 -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-jkmiv4 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-6b4k6y` in namespace `kube-system`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-6b4k6y -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-6b4k6y -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-6b4k6y -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Scale Deployment\nCreate a deployment named `resource-a3uvnl` in namespace `staging` using image `nginx`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-a3uvnl -n staging -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-a3uvnl -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-a3uvnl -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-j8hyct` in namespace `staging` using image `alpine`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-j8hyct -n staging --no-headers | grep Running\nkubectl get pod resource-j8hyct -n staging -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-j8hyct -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-0f6mlc` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-0f6mlc -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-0f6mlc -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-0f6mlc -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Expose Service\nExpose the deployment `resource-kat4m0-dep` as a Service named `resource-kat4m0` in namespace `backend`.\nThe service should listen on port `6356` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-kat4m0 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 6356\nkubectl get svc resource-kat4m0 -n backend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-kat4m0-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-kat4m0 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Expose Service\nExpose the deployment `resource-xfztjj-dep` as a Service named `resource-xfztjj` in namespace `kube-system`.\nThe service should listen on port `5796` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-xfztjj -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 5796\nkubectl get svc resource-xfztjj -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-xfztjj-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-xfztjj -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Scale Deployment\nCreate a deployment named `resource-m58squ` in namespace `test` using image `python:3.9`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-m58squ -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-m58squ -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-m58squ -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Scale Deployment\nCreate a deployment named `resource-k06siv` in namespace `kube-system` using image `python:3.9`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-k06siv -n kube-system -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-k06siv -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-k06siv -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Expose Service\nExpose the deployment `resource-zeid4j-dep` as a Service named `resource-zeid4j` in namespace `kube-system`.\nThe service should listen on port `4588` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-zeid4j -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4588\nkubectl get svc resource-zeid4j -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-zeid4j-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-zeid4j -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Create a Pod\nCreate a pod named `resource-4glzo0` in namespace `staging` using image `node:14`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-4glzo0 -n staging --no-headers | grep Running\nkubectl get pod resource-4glzo0 -n staging -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4glzo0 -n staging --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-10": {
    "id": "auto-cka-10",
    "title": "Auto CKA Practice (Batch 10)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 10)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-8zb2kb` in namespace `staging`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 2: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-90h870` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 4: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 5: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 6: Scale Deployment\nCreate a deployment named `resource-y228dw` in namespace `dev` using image `python:3.9`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n# Task 7: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-t3hsff` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 9: Create a Pod\nCreate a pod named `resource-odpn25` in namespace `default` using image `busybox`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 10: Create a Pod\nCreate a pod named `resource-b1lzhx` in namespace `staging` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-kkrkq1` in namespace `test` using image `python:3.9`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 12: Expose Service\nExpose the deployment `resource-8vcqap-dep` as a Service named `resource-8vcqap` in namespace `dev`.\nThe service should listen on port `5504` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 13: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 14: Create a Pod\nCreate a pod named `resource-3y5uij` in namespace `backend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 15: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-vxam4e-dep` as a Service named `resource-vxam4e` in namespace `default`.\nThe service should listen on port `6670` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 17: Scale Deployment\nCreate a deployment named `resource-68xdy7` in namespace `kube-system` using image `memcached`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n# Task 18: Expose Service\nExpose the deployment `resource-1fil5e-dep` as a Service named `resource-1fil5e` in namespace `frontend`.\nThe service should listen on port `5922` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 19: Create a Pod\nCreate a pod named `resource-qt0j6d` in namespace `prod` using image `nginx`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 20: Create a Pod\nCreate a pod named `resource-f8ajdy` in namespace `kube-system` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pvc resource-8zb2kb -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-8zb2kb -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-90h870 -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-90h870 -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-y228dw -n dev -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-y228dw -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-t3hsff -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-t3hsff -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pod resource-odpn25 -n default --no-headers | grep Running\nkubectl get pod resource-odpn25 -n default -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-b1lzhx -n staging --no-headers | grep Running\nkubectl get pod resource-b1lzhx -n staging -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-kkrkq1 -n test --no-headers | grep Running\nkubectl get pod resource-kkrkq1 -n test -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get svc resource-8vcqap -n dev -o jsonpath='{.spec.ports[0].port}' | grep 5504\nkubectl get svc resource-8vcqap -n dev -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-3y5uij -n backend --no-headers | grep Running\nkubectl get pod resource-3y5uij -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-vxam4e -n default -o jsonpath='{.spec.ports[0].port}' | grep 6670\nkubectl get svc resource-vxam4e -n default -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-68xdy7 -n kube-system -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-68xdy7 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\nkubectl get svc resource-1fil5e -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 5922\nkubectl get svc resource-1fil5e -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-qt0j6d -n prod --no-headers | grep Running\nkubectl get pod resource-qt0j6d -n prod -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get pod resource-f8ajdy -n kube-system --no-headers | grep Running\nkubectl get pod resource-f8ajdy -n kube-system -o jsonpath='{.metadata.labels.team}' | grep blue\n",
    "setupScript": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-8zb2kb -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-90h870 -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-y228dw -n dev 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-t3hsff -n kube-system 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-odpn25 -n default --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-b1lzhx -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-kkrkq1 -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-8vcqap-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-8vcqap -n dev 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3y5uij -n backend --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vxam4e-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vxam4e -n default 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-68xdy7 -n kube-system 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-1fil5e-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-1fil5e -n frontend 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-qt0j6d -n prod --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-f8ajdy -n kube-system --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 10)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-8zb2kb` in namespace `staging`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-8zb2kb -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-8zb2kb -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-8zb2kb -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-90h870` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-90h870 -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-90h870 -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-90h870 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Scale Deployment\nCreate a deployment named `resource-y228dw` in namespace `dev` using image `python:3.9`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-y228dw -n dev -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-y228dw -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-y228dw -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-t3hsff` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-t3hsff -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-t3hsff -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-t3hsff -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Create a Pod\nCreate a pod named `resource-odpn25` in namespace `default` using image `busybox`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-odpn25 -n default --no-headers | grep Running\nkubectl get pod resource-odpn25 -n default -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-odpn25 -n default --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Create a Pod\nCreate a pod named `resource-b1lzhx` in namespace `staging` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-b1lzhx -n staging --no-headers | grep Running\nkubectl get pod resource-b1lzhx -n staging -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-b1lzhx -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-kkrkq1` in namespace `test` using image `python:3.9`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-kkrkq1 -n test --no-headers | grep Running\nkubectl get pod resource-kkrkq1 -n test -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-kkrkq1 -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Expose Service\nExpose the deployment `resource-8vcqap-dep` as a Service named `resource-8vcqap` in namespace `dev`.\nThe service should listen on port `5504` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-8vcqap -n dev -o jsonpath='{.spec.ports[0].port}' | grep 5504\nkubectl get svc resource-8vcqap -n dev -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-8vcqap-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-8vcqap -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Create a Pod\nCreate a pod named `resource-3y5uij` in namespace `backend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-3y5uij -n backend --no-headers | grep Running\nkubectl get pod resource-3y5uij -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3y5uij -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-vxam4e-dep` as a Service named `resource-vxam4e` in namespace `default`.\nThe service should listen on port `6670` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-vxam4e -n default -o jsonpath='{.spec.ports[0].port}' | grep 6670\nkubectl get svc resource-vxam4e -n default -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vxam4e-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vxam4e -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Scale Deployment\nCreate a deployment named `resource-68xdy7` in namespace `kube-system` using image `memcached`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-68xdy7 -n kube-system -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-68xdy7 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-68xdy7 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Expose Service\nExpose the deployment `resource-1fil5e-dep` as a Service named `resource-1fil5e` in namespace `frontend`.\nThe service should listen on port `5922` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-1fil5e -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 5922\nkubectl get svc resource-1fil5e -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-1fil5e-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-1fil5e -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Create a Pod\nCreate a pod named `resource-qt0j6d` in namespace `prod` using image `nginx`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-qt0j6d -n prod --no-headers | grep Running\nkubectl get pod resource-qt0j6d -n prod -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-qt0j6d -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Create a Pod\nCreate a pod named `resource-f8ajdy` in namespace `kube-system` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-f8ajdy -n kube-system --no-headers | grep Running\nkubectl get pod resource-f8ajdy -n kube-system -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-f8ajdy -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-11": {
    "id": "auto-cka-11",
    "title": "Auto CKA Practice (Batch 11)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 11)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Scale Deployment\nCreate a deployment named `resource-n0f6x1` in namespace `frontend` using image `redis`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 2: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 3: Create a Pod\nCreate a pod named `resource-4i1g93` in namespace `staging` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 4: Expose Service\nExpose the deployment `resource-6rooj3-dep` as a Service named `resource-6rooj3` in namespace `backend`.\nThe service should listen on port `7752` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 5: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 6: Scale Deployment\nCreate a deployment named `resource-kqhpm8` in namespace `default` using image `memcached`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n# Task 7: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 8: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 9: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9r5o17` in namespace `default`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 10: Expose Service\nExpose the deployment `resource-raeo8q-dep` as a Service named `resource-raeo8q` in namespace `prod`.\nThe service should listen on port `8243` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 11: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-161eqq` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 12: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 13: Expose Service\nExpose the deployment `resource-elfukx-dep` as a Service named `resource-elfukx` in namespace `frontend`.\nThe service should listen on port `6434` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 14: Scale Deployment\nCreate a deployment named `resource-qzkbgu` in namespace `default` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-yw5w4f` in namespace `kube-system`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 16: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-r53ss8` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-fsje9q` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-qtaosj` in namespace `backend`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get deploy resource-n0f6x1 -n frontend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-n0f6x1 -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-4i1g93 -n staging --no-headers | grep Running\nkubectl get pod resource-4i1g93 -n staging -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get svc resource-6rooj3 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 7752\nkubectl get svc resource-6rooj3 -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-kqhpm8 -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-kqhpm8 -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-9r5o17 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-9r5o17 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get svc resource-raeo8q -n prod -o jsonpath='{.spec.ports[0].port}' | grep 8243\nkubectl get svc resource-raeo8q -n prod -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-161eqq -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-161eqq -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-elfukx -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 6434\nkubectl get svc resource-elfukx -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get deploy resource-qzkbgu -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-qzkbgu -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\nkubectl get pvc resource-yw5w4f -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-yw5w4f -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-r53ss8 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-r53ss8 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-fsje9q -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-fsje9q -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-qtaosj -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-qtaosj -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
    "setupScript": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-n0f6x1 -n frontend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4i1g93 -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-6rooj3-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-6rooj3 -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-kqhpm8 -n default 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9r5o17 -n default 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-raeo8q-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-raeo8q -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-161eqq -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-elfukx-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-elfukx -n frontend 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-qzkbgu -n default 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-yw5w4f -n kube-system 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-r53ss8 -n kube-system 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-fsje9q -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-qtaosj -n backend 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 11)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Scale Deployment\nCreate a deployment named `resource-n0f6x1` in namespace `frontend` using image `redis`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-n0f6x1 -n frontend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-n0f6x1 -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-n0f6x1 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Create a Pod\nCreate a pod named `resource-4i1g93` in namespace `staging` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-4i1g93 -n staging --no-headers | grep Running\nkubectl get pod resource-4i1g93 -n staging -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4i1g93 -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Expose Service\nExpose the deployment `resource-6rooj3-dep` as a Service named `resource-6rooj3` in namespace `backend`.\nThe service should listen on port `7752` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-6rooj3 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 7752\nkubectl get svc resource-6rooj3 -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-6rooj3-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-6rooj3 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Scale Deployment\nCreate a deployment named `resource-kqhpm8` in namespace `default` using image `memcached`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-kqhpm8 -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-kqhpm8 -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-kqhpm8 -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9r5o17` in namespace `default`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-9r5o17 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-9r5o17 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9r5o17 -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Expose Service\nExpose the deployment `resource-raeo8q-dep` as a Service named `resource-raeo8q` in namespace `prod`.\nThe service should listen on port `8243` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-raeo8q -n prod -o jsonpath='{.spec.ports[0].port}' | grep 8243\nkubectl get svc resource-raeo8q -n prod -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-raeo8q-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-raeo8q -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-161eqq` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-161eqq -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-161eqq -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-161eqq -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Expose Service\nExpose the deployment `resource-elfukx-dep` as a Service named `resource-elfukx` in namespace `frontend`.\nThe service should listen on port `6434` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-elfukx -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 6434\nkubectl get svc resource-elfukx -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-elfukx-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-elfukx -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Scale Deployment\nCreate a deployment named `resource-qzkbgu` in namespace `default` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-qzkbgu -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-qzkbgu -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-qzkbgu -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-yw5w4f` in namespace `kube-system`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-yw5w4f -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-yw5w4f -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-yw5w4f -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-r53ss8` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-r53ss8 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-r53ss8 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-r53ss8 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-fsje9q` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-fsje9q -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-fsje9q -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-fsje9q -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-qtaosj` in namespace `backend`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-qtaosj -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-qtaosj -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-qtaosj -n backend 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-12": {
    "id": "auto-cka-12",
    "title": "Auto CKA Practice (Batch 12)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 12)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9s3758` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 2: Scale Deployment\nCreate a deployment named `resource-lotqkw` in namespace `staging` using image `postgres`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 3: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 4: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 5: Expose Service\nExpose the deployment `resource-j0cqbr-dep` as a Service named `resource-j0cqbr` in namespace `backend`.\nThe service should listen on port `5155` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 6: Scale Deployment\nCreate a deployment named `resource-8ugvus` in namespace `frontend` using image `busybox`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n# Task 7: Create a Pod\nCreate a pod named `resource-ufrib3` in namespace `dev` using image `alpine`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 8: Expose Service\nExpose the deployment `resource-fcw8l4-dep` as a Service named `resource-fcw8l4` in namespace `kube-system`.\nThe service should listen on port `8943` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 9: Scale Deployment\nCreate a deployment named `resource-7748o6` in namespace `test` using image `python:3.9`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n# Task 10: Create a Pod\nCreate a pod named `resource-0k2tjk` in namespace `default` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-f7c3tr` in namespace `backend` using image `mysql`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 12: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 13: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 14: Expose Service\nExpose the deployment `resource-miup0j-dep` as a Service named `resource-miup0j` in namespace `kube-system`.\nThe service should listen on port `8306` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-0r32tc` in namespace `dev` using image `nginx`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-zwionp-dep` as a Service named `resource-zwionp` in namespace `dev`.\nThe service should listen on port `3026` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 17: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lkg4ap` in namespace `frontend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 20: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pvc resource-9s3758 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-9s3758 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get deploy resource-lotqkw -n staging -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-lotqkw -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-j0cqbr -n backend -o jsonpath='{.spec.ports[0].port}' | grep 5155\nkubectl get svc resource-j0cqbr -n backend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-8ugvus -n frontend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-8ugvus -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\nkubectl get pod resource-ufrib3 -n dev --no-headers | grep Running\nkubectl get pod resource-ufrib3 -n dev -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get svc resource-fcw8l4 -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 8943\nkubectl get svc resource-fcw8l4 -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-7748o6 -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-7748o6 -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\nkubectl get pod resource-0k2tjk -n default --no-headers | grep Running\nkubectl get pod resource-0k2tjk -n default -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pod resource-f7c3tr -n backend --no-headers | grep Running\nkubectl get pod resource-f7c3tr -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-miup0j -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 8306\nkubectl get svc resource-miup0j -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-0r32tc -n dev --no-headers | grep Running\nkubectl get pod resource-0r32tc -n dev -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get svc resource-zwionp -n dev -o jsonpath='{.spec.ports[0].port}' | grep 3026\nkubectl get svc resource-zwionp -n dev -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-lkg4ap -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-lkg4ap -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
    "setupScript": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9s3758 -n prod 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-lotqkw -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-j0cqbr-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-j0cqbr -n backend 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-8ugvus -n frontend 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ufrib3 -n dev --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-fcw8l4-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-fcw8l4 -n kube-system 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-7748o6 -n test 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0k2tjk -n default --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-f7c3tr -n backend --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-miup0j-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-miup0j -n kube-system 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0r32tc -n dev --force --grace-period=0 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-zwionp-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-zwionp -n dev 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lkg4ap -n frontend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 12)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9s3758` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-9s3758 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-9s3758 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9s3758 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Scale Deployment\nCreate a deployment named `resource-lotqkw` in namespace `staging` using image `postgres`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-lotqkw -n staging -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-lotqkw -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-lotqkw -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Expose Service\nExpose the deployment `resource-j0cqbr-dep` as a Service named `resource-j0cqbr` in namespace `backend`.\nThe service should listen on port `5155` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-j0cqbr -n backend -o jsonpath='{.spec.ports[0].port}' | grep 5155\nkubectl get svc resource-j0cqbr -n backend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-j0cqbr-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-j0cqbr -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Scale Deployment\nCreate a deployment named `resource-8ugvus` in namespace `frontend` using image `busybox`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-8ugvus -n frontend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-8ugvus -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-8ugvus -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Create a Pod\nCreate a pod named `resource-ufrib3` in namespace `dev` using image `alpine`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-ufrib3 -n dev --no-headers | grep Running\nkubectl get pod resource-ufrib3 -n dev -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ufrib3 -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Expose Service\nExpose the deployment `resource-fcw8l4-dep` as a Service named `resource-fcw8l4` in namespace `kube-system`.\nThe service should listen on port `8943` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-fcw8l4 -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 8943\nkubectl get svc resource-fcw8l4 -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-fcw8l4-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-fcw8l4 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Scale Deployment\nCreate a deployment named `resource-7748o6` in namespace `test` using image `python:3.9`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-7748o6 -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-7748o6 -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-7748o6 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Create a Pod\nCreate a pod named `resource-0k2tjk` in namespace `default` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-0k2tjk -n default --no-headers | grep Running\nkubectl get pod resource-0k2tjk -n default -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0k2tjk -n default --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-f7c3tr` in namespace `backend` using image `mysql`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-f7c3tr -n backend --no-headers | grep Running\nkubectl get pod resource-f7c3tr -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-f7c3tr -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Expose Service\nExpose the deployment `resource-miup0j-dep` as a Service named `resource-miup0j` in namespace `kube-system`.\nThe service should listen on port `8306` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-miup0j -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 8306\nkubectl get svc resource-miup0j -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-miup0j-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-miup0j -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-0r32tc` in namespace `dev` using image `nginx`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-0r32tc -n dev --no-headers | grep Running\nkubectl get pod resource-0r32tc -n dev -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0r32tc -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-zwionp-dep` as a Service named `resource-zwionp` in namespace `dev`.\nThe service should listen on port `3026` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-zwionp -n dev -o jsonpath='{.spec.ports[0].port}' | grep 3026\nkubectl get svc resource-zwionp -n dev -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-zwionp-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-zwionp -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lkg4ap` in namespace `frontend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-lkg4ap -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-lkg4ap -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lkg4ap -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-13": {
    "id": "auto-cka-13",
    "title": "Auto CKA Practice (Batch 13)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 13)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cbupzp` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 2: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 3: Create a Pod\nCreate a pod named `resource-23q06u` in namespace `default` using image `mysql`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 4: Scale Deployment\nCreate a deployment named `resource-3sagh0` in namespace `kube-system` using image `postgres`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 5: Scale Deployment\nCreate a deployment named `resource-morx1w` in namespace `test` using image `nginx`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 6: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-4blrsb` in namespace `staging`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 7: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 8: Create a Pod\nCreate a pod named `resource-qjkfdf` in namespace `backend` using image `nginx`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 9: Scale Deployment\nCreate a deployment named `resource-wm71c5` in namespace `prod` using image `httpd`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n# Task 10: Scale Deployment\nCreate a deployment named `resource-y42w2l` in namespace `backend` using image `redis`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-m6b6g9` in namespace `frontend` using image `mysql`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-icjgn4` in namespace `prod`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3wne29` in namespace `default`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 14: Expose Service\nExpose the deployment `resource-647fqq-dep` as a Service named `resource-647fqq` in namespace `backend`.\nThe service should listen on port `3833` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 15: Expose Service\nExpose the deployment `resource-9wopd1-dep` as a Service named `resource-9wopd1` in namespace `staging`.\nThe service should listen on port `5203` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 16: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 17: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 18: Expose Service\nExpose the deployment `resource-2rftli-dep` as a Service named `resource-2rftli` in namespace `prod`.\nThe service should listen on port `5541` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-jtj724` in namespace `backend`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 20: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pvc resource-cbupzp -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-cbupzp -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-23q06u -n default --no-headers | grep Running\nkubectl get pod resource-23q06u -n default -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get deploy resource-3sagh0 -n kube-system -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-3sagh0 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\nkubectl get deploy resource-morx1w -n test -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-morx1w -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get pvc resource-4blrsb -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-4blrsb -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-qjkfdf -n backend --no-headers | grep Running\nkubectl get pod resource-qjkfdf -n backend -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get deploy resource-wm71c5 -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-wm71c5 -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\nkubectl get deploy resource-y42w2l -n backend -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-y42w2l -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get pod resource-m6b6g9 -n frontend --no-headers | grep Running\nkubectl get pod resource-m6b6g9 -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pvc resource-icjgn4 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-icjgn4 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-3wne29 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-3wne29 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get svc resource-647fqq -n backend -o jsonpath='{.spec.ports[0].port}' | grep 3833\nkubectl get svc resource-647fqq -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get svc resource-9wopd1 -n staging -o jsonpath='{.spec.ports[0].port}' | grep 5203\nkubectl get svc resource-9wopd1 -n staging -o jsonpath='{.spec.type}' | grep ClusterIP\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-2rftli -n prod -o jsonpath='{.spec.ports[0].port}' | grep 5541\nkubectl get svc resource-2rftli -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pvc resource-jtj724 -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-jtj724 -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
    "setupScript": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cbupzp -n kube-system 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-23q06u -n default --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-3sagh0 -n kube-system 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-morx1w -n test 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-4blrsb -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-qjkfdf -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-wm71c5 -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-y42w2l -n backend 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-m6b6g9 -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-icjgn4 -n prod 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3wne29 -n default 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-647fqq-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-647fqq -n backend 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-9wopd1-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-9wopd1 -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-2rftli-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-2rftli -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-jtj724 -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 13)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cbupzp` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-cbupzp -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-cbupzp -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cbupzp -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Create a Pod\nCreate a pod named `resource-23q06u` in namespace `default` using image `mysql`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-23q06u -n default --no-headers | grep Running\nkubectl get pod resource-23q06u -n default -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-23q06u -n default --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Scale Deployment\nCreate a deployment named `resource-3sagh0` in namespace `kube-system` using image `postgres`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-3sagh0 -n kube-system -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-3sagh0 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-3sagh0 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Scale Deployment\nCreate a deployment named `resource-morx1w` in namespace `test` using image `nginx`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-morx1w -n test -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-morx1w -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-morx1w -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-4blrsb` in namespace `staging`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-4blrsb -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-4blrsb -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-4blrsb -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Create a Pod\nCreate a pod named `resource-qjkfdf` in namespace `backend` using image `nginx`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-qjkfdf -n backend --no-headers | grep Running\nkubectl get pod resource-qjkfdf -n backend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-qjkfdf -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Scale Deployment\nCreate a deployment named `resource-wm71c5` in namespace `prod` using image `httpd`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-wm71c5 -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-wm71c5 -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-wm71c5 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Scale Deployment\nCreate a deployment named `resource-y42w2l` in namespace `backend` using image `redis`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-y42w2l -n backend -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-y42w2l -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-y42w2l -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-m6b6g9` in namespace `frontend` using image `mysql`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-m6b6g9 -n frontend --no-headers | grep Running\nkubectl get pod resource-m6b6g9 -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-m6b6g9 -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-icjgn4` in namespace `prod`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-icjgn4 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-icjgn4 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-icjgn4 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3wne29` in namespace `default`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-3wne29 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-3wne29 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3wne29 -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Expose Service\nExpose the deployment `resource-647fqq-dep` as a Service named `resource-647fqq` in namespace `backend`.\nThe service should listen on port `3833` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-647fqq -n backend -o jsonpath='{.spec.ports[0].port}' | grep 3833\nkubectl get svc resource-647fqq -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-647fqq-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-647fqq -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Expose Service\nExpose the deployment `resource-9wopd1-dep` as a Service named `resource-9wopd1` in namespace `staging`.\nThe service should listen on port `5203` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-9wopd1 -n staging -o jsonpath='{.spec.ports[0].port}' | grep 5203\nkubectl get svc resource-9wopd1 -n staging -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-9wopd1-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-9wopd1 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Expose Service\nExpose the deployment `resource-2rftli-dep` as a Service named `resource-2rftli` in namespace `prod`.\nThe service should listen on port `5541` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-2rftli -n prod -o jsonpath='{.spec.ports[0].port}' | grep 5541\nkubectl get svc resource-2rftli -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-2rftli-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-2rftli -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-jtj724` in namespace `backend`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-jtj724 -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-jtj724 -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-jtj724 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-14": {
    "id": "auto-cka-14",
    "title": "Auto CKA Practice (Batch 14)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 14)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-5bdri5` in namespace `frontend` using image `alpine`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 2: Create a Pod\nCreate a pod named `resource-3rb4k1` in namespace `prod` using image `mysql`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-78v90t` in namespace `backend`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 4: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-er5onz` in namespace `default`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 5: Expose Service\nExpose the deployment `resource-3a9cdg-dep` as a Service named `resource-3a9cdg` in namespace `staging`.\nThe service should listen on port `3029` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 6: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 7: Scale Deployment\nCreate a deployment named `resource-hytd3r` in namespace `backend` using image `mysql`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n# Task 8: Expose Service\nExpose the deployment `resource-e83grv-dep` as a Service named `resource-e83grv` in namespace `kube-system`.\nThe service should listen on port `4494` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 9: Expose Service\nExpose the deployment `resource-5517cy-dep` as a Service named `resource-5517cy` in namespace `kube-system`.\nThe service should listen on port `7087` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3wg5r5` in namespace `dev`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 11: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 12: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 13: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 14: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-x9taqs` in namespace `dev`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-n345ys` in namespace `test` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-af2nyo-dep` as a Service named `resource-af2nyo` in namespace `backend`.\nThe service should listen on port `7671` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-8i4dj1` in namespace `prod`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Scale Deployment\nCreate a deployment named `resource-eb8jkq` in namespace `prod` using image `memcached`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-dyrwee` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-5bdri5 -n frontend --no-headers | grep Running\nkubectl get pod resource-5bdri5 -n frontend -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get pod resource-3rb4k1 -n prod --no-headers | grep Running\nkubectl get pod resource-3rb4k1 -n prod -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pvc resource-78v90t -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-78v90t -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-er5onz -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-er5onz -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get svc resource-3a9cdg -n staging -o jsonpath='{.spec.ports[0].port}' | grep 3029\nkubectl get svc resource-3a9cdg -n staging -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-hytd3r -n backend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-hytd3r -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\nkubectl get svc resource-e83grv -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4494\nkubectl get svc resource-e83grv -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get svc resource-5517cy -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7087\nkubectl get svc resource-5517cy -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-3wg5r5 -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-3wg5r5 -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-x9taqs -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-x9taqs -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get pod resource-n345ys -n test --no-headers | grep Running\nkubectl get pod resource-n345ys -n test -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get svc resource-af2nyo -n backend -o jsonpath='{.spec.ports[0].port}' | grep 7671\nkubectl get svc resource-af2nyo -n backend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-8i4dj1 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-8i4dj1 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-eb8jkq -n prod -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-eb8jkq -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\nkubectl get pvc resource-dyrwee -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-dyrwee -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
    "setupScript": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-5bdri5 -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3rb4k1 -n prod --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-78v90t -n backend 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-er5onz -n default 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-3a9cdg-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-3a9cdg -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-hytd3r -n backend 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-e83grv-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-e83grv -n kube-system 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-5517cy-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-5517cy -n kube-system 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3wg5r5 -n dev 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-x9taqs -n dev 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-n345ys -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-af2nyo-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-af2nyo -n backend 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-8i4dj1 -n prod 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-eb8jkq -n prod 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-dyrwee -n kube-system 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 14)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-5bdri5` in namespace `frontend` using image `alpine`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-5bdri5 -n frontend --no-headers | grep Running\nkubectl get pod resource-5bdri5 -n frontend -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-5bdri5 -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Create a Pod\nCreate a pod named `resource-3rb4k1` in namespace `prod` using image `mysql`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-3rb4k1 -n prod --no-headers | grep Running\nkubectl get pod resource-3rb4k1 -n prod -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3rb4k1 -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-78v90t` in namespace `backend`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-78v90t -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-78v90t -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-78v90t -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-er5onz` in namespace `default`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-er5onz -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-er5onz -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-er5onz -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Expose Service\nExpose the deployment `resource-3a9cdg-dep` as a Service named `resource-3a9cdg` in namespace `staging`.\nThe service should listen on port `3029` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-3a9cdg -n staging -o jsonpath='{.spec.ports[0].port}' | grep 3029\nkubectl get svc resource-3a9cdg -n staging -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-3a9cdg-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-3a9cdg -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Scale Deployment\nCreate a deployment named `resource-hytd3r` in namespace `backend` using image `mysql`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-hytd3r -n backend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-hytd3r -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-hytd3r -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Expose Service\nExpose the deployment `resource-e83grv-dep` as a Service named `resource-e83grv` in namespace `kube-system`.\nThe service should listen on port `4494` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-e83grv -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4494\nkubectl get svc resource-e83grv -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-e83grv-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-e83grv -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Expose Service\nExpose the deployment `resource-5517cy-dep` as a Service named `resource-5517cy` in namespace `kube-system`.\nThe service should listen on port `7087` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-5517cy -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7087\nkubectl get svc resource-5517cy -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-5517cy-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-5517cy -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3wg5r5` in namespace `dev`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-3wg5r5 -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-3wg5r5 -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3wg5r5 -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-x9taqs` in namespace `dev`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-x9taqs -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-x9taqs -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-x9taqs -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-n345ys` in namespace `test` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-n345ys -n test --no-headers | grep Running\nkubectl get pod resource-n345ys -n test -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-n345ys -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-af2nyo-dep` as a Service named `resource-af2nyo` in namespace `backend`.\nThe service should listen on port `7671` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-af2nyo -n backend -o jsonpath='{.spec.ports[0].port}' | grep 7671\nkubectl get svc resource-af2nyo -n backend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-af2nyo-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-af2nyo -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-8i4dj1` in namespace `prod`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-8i4dj1 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-8i4dj1 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-8i4dj1 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Scale Deployment\nCreate a deployment named `resource-eb8jkq` in namespace `prod` using image `memcached`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-eb8jkq -n prod -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-eb8jkq -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-eb8jkq -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-dyrwee` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-dyrwee -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-dyrwee -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-dyrwee -n kube-system 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-15": {
    "id": "auto-cka-15",
    "title": "Auto CKA Practice (Batch 15)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 15)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Scale Deployment\nCreate a deployment named `resource-7wgaag` in namespace `default` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 2: Expose Service\nExpose the deployment `resource-xek6la-dep` as a Service named `resource-xek6la` in namespace `frontend`.\nThe service should listen on port `3896` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 3: Expose Service\nExpose the deployment `resource-mws61h-dep` as a Service named `resource-mws61h` in namespace `prod`.\nThe service should listen on port `5329` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 4: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-6umkm0` in namespace `prod`.\nRequest `2Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 5: Expose Service\nExpose the deployment `resource-464nd3-dep` as a Service named `resource-464nd3` in namespace `backend`.\nThe service should listen on port `4077` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 6: Expose Service\nExpose the deployment `resource-6zndmk-dep` as a Service named `resource-6zndmk` in namespace `dev`.\nThe service should listen on port `3833` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 7: Expose Service\nExpose the deployment `resource-tmxvja-dep` as a Service named `resource-tmxvja` in namespace `frontend`.\nThe service should listen on port `8900` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-5oyu1d` in namespace `default`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 9: Create a Pod\nCreate a pod named `resource-seaps0` in namespace `kube-system` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 10: Expose Service\nExpose the deployment `resource-vmr69n-dep` as a Service named `resource-vmr69n` in namespace `staging`.\nThe service should listen on port `3005` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-siskbg` in namespace `default` using image `nginx`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 12: Create a Pod\nCreate a pod named `resource-de8z9z` in namespace `kube-system` using image `mysql`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 13: Create a Pod\nCreate a pod named `resource-9ws993` in namespace `staging` using image `python:3.9`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 14: Create a Pod\nCreate a pod named `resource-0e7gkr` in namespace `prod` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 15: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 16: Scale Deployment\nCreate a deployment named `resource-1e1mmk` in namespace `frontend` using image `mysql`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n# Task 17: Expose Service\nExpose the deployment `resource-md87ra-dep` as a Service named `resource-md87ra` in namespace `frontend`.\nThe service should listen on port `8047` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-uyze8x` in namespace `default`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get deploy resource-7wgaag -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-7wgaag -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\nkubectl get svc resource-xek6la -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 3896\nkubectl get svc resource-xek6la -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get svc resource-mws61h -n prod -o jsonpath='{.spec.ports[0].port}' | grep 5329\nkubectl get svc resource-mws61h -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pvc resource-6umkm0 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-6umkm0 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get svc resource-464nd3 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 4077\nkubectl get svc resource-464nd3 -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get svc resource-6zndmk -n dev -o jsonpath='{.spec.ports[0].port}' | grep 3833\nkubectl get svc resource-6zndmk -n dev -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get svc resource-tmxvja -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 8900\nkubectl get svc resource-tmxvja -n frontend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-5oyu1d -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-5oyu1d -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pod resource-seaps0 -n kube-system --no-headers | grep Running\nkubectl get pod resource-seaps0 -n kube-system -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get svc resource-vmr69n -n staging -o jsonpath='{.spec.ports[0].port}' | grep 3005\nkubectl get svc resource-vmr69n -n staging -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pod resource-siskbg -n default --no-headers | grep Running\nkubectl get pod resource-siskbg -n default -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pod resource-de8z9z -n kube-system --no-headers | grep Running\nkubectl get pod resource-de8z9z -n kube-system -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get pod resource-9ws993 -n staging --no-headers | grep Running\nkubectl get pod resource-9ws993 -n staging -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-0e7gkr -n prod --no-headers | grep Running\nkubectl get pod resource-0e7gkr -n prod -o jsonpath='{.metadata.labels.team}' | grep blue\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-1e1mmk -n frontend -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-1e1mmk -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\nkubectl get svc resource-md87ra -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 8047\nkubectl get svc resource-md87ra -n frontend -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-uyze8x -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-uyze8x -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
    "setupScript": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-7wgaag -n default 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-xek6la-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-xek6la -n frontend 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-mws61h-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-mws61h -n prod 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-6umkm0 -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-464nd3-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-464nd3 -n backend 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-6zndmk-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-6zndmk -n dev 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-tmxvja-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-tmxvja -n frontend 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-5oyu1d -n default 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-seaps0 -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vmr69n-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vmr69n -n staging 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-siskbg -n default --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-de8z9z -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-9ws993 -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0e7gkr -n prod --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-1e1mmk -n frontend 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-md87ra-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-md87ra -n frontend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-uyze8x -n default 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 15)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Scale Deployment\nCreate a deployment named `resource-7wgaag` in namespace `default` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-7wgaag -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-7wgaag -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-7wgaag -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Expose Service\nExpose the deployment `resource-xek6la-dep` as a Service named `resource-xek6la` in namespace `frontend`.\nThe service should listen on port `3896` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-xek6la -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 3896\nkubectl get svc resource-xek6la -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-xek6la-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-xek6la -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Expose Service\nExpose the deployment `resource-mws61h-dep` as a Service named `resource-mws61h` in namespace `prod`.\nThe service should listen on port `5329` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-mws61h -n prod -o jsonpath='{.spec.ports[0].port}' | grep 5329\nkubectl get svc resource-mws61h -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-mws61h-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-mws61h -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-6umkm0` in namespace `prod`.\nRequest `2Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-6umkm0 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-6umkm0 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-6umkm0 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Expose Service\nExpose the deployment `resource-464nd3-dep` as a Service named `resource-464nd3` in namespace `backend`.\nThe service should listen on port `4077` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-464nd3 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 4077\nkubectl get svc resource-464nd3 -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-464nd3-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-464nd3 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Expose Service\nExpose the deployment `resource-6zndmk-dep` as a Service named `resource-6zndmk` in namespace `dev`.\nThe service should listen on port `3833` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-6zndmk -n dev -o jsonpath='{.spec.ports[0].port}' | grep 3833\nkubectl get svc resource-6zndmk -n dev -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-6zndmk-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-6zndmk -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Expose Service\nExpose the deployment `resource-tmxvja-dep` as a Service named `resource-tmxvja` in namespace `frontend`.\nThe service should listen on port `8900` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-tmxvja -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 8900\nkubectl get svc resource-tmxvja -n frontend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-tmxvja-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-tmxvja -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-5oyu1d` in namespace `default`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-5oyu1d -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-5oyu1d -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-5oyu1d -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Create a Pod\nCreate a pod named `resource-seaps0` in namespace `kube-system` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-seaps0 -n kube-system --no-headers | grep Running\nkubectl get pod resource-seaps0 -n kube-system -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-seaps0 -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Expose Service\nExpose the deployment `resource-vmr69n-dep` as a Service named `resource-vmr69n` in namespace `staging`.\nThe service should listen on port `3005` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-vmr69n -n staging -o jsonpath='{.spec.ports[0].port}' | grep 3005\nkubectl get svc resource-vmr69n -n staging -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vmr69n-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vmr69n -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-siskbg` in namespace `default` using image `nginx`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-siskbg -n default --no-headers | grep Running\nkubectl get pod resource-siskbg -n default -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-siskbg -n default --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Create a Pod\nCreate a pod named `resource-de8z9z` in namespace `kube-system` using image `mysql`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-de8z9z -n kube-system --no-headers | grep Running\nkubectl get pod resource-de8z9z -n kube-system -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-de8z9z -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Create a Pod\nCreate a pod named `resource-9ws993` in namespace `staging` using image `python:3.9`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-9ws993 -n staging --no-headers | grep Running\nkubectl get pod resource-9ws993 -n staging -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-9ws993 -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Create a Pod\nCreate a pod named `resource-0e7gkr` in namespace `prod` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-0e7gkr -n prod --no-headers | grep Running\nkubectl get pod resource-0e7gkr -n prod -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0e7gkr -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Scale Deployment\nCreate a deployment named `resource-1e1mmk` in namespace `frontend` using image `mysql`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-1e1mmk -n frontend -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-1e1mmk -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-1e1mmk -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Expose Service\nExpose the deployment `resource-md87ra-dep` as a Service named `resource-md87ra` in namespace `frontend`.\nThe service should listen on port `8047` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-md87ra -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 8047\nkubectl get svc resource-md87ra -n frontend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-md87ra-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-md87ra -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-uyze8x` in namespace `default`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-uyze8x -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-uyze8x -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-uyze8x -n default 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-16": {
    "id": "auto-cka-16",
    "title": "Auto CKA Practice (Batch 16)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 16)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-37snqm` in namespace `staging`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 3: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 4: Create a Pod\nCreate a pod named `resource-upup4j` in namespace `prod` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 5: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ejgz8t` in namespace `test`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 6: Create a Pod\nCreate a pod named `resource-tz0b8p` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 7: Scale Deployment\nCreate a deployment named `resource-i533wq` in namespace `prod` using image `alpine`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n# Task 8: Expose Service\nExpose the deployment `resource-6pqfzr-dep` as a Service named `resource-6pqfzr` in namespace `frontend`.\nThe service should listen on port `3697` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 9: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-04830r` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 10: Scale Deployment\nCreate a deployment named `resource-pwobt5` in namespace `staging` using image `redis`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-75a7ok` in namespace `test` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 12: Create a Pod\nCreate a pod named `resource-m6fgrt` in namespace `frontend` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 13: Create a Pod\nCreate a pod named `resource-9u7ew1` in namespace `prod` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 14: Scale Deployment\nCreate a deployment named `resource-z7sm28` in namespace `frontend` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-emn9n5` in namespace `dev` using image `postgres`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 16: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-bntd4k` in namespace `staging`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 17: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lklw59` in namespace `frontend`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 20: Scale Deployment\nCreate a deployment named `resource-rlbv2y` in namespace `test` using image `postgres`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n",
    "verifyScript": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-37snqm -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-37snqm -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-upup4j -n prod --no-headers | grep Running\nkubectl get pod resource-upup4j -n prod -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pvc resource-ejgz8t -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-ejgz8t -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pod resource-tz0b8p -n kube-system --no-headers | grep Running\nkubectl get pod resource-tz0b8p -n kube-system -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get deploy resource-i533wq -n prod -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-i533wq -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\nkubectl get svc resource-6pqfzr -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 3697\nkubectl get svc resource-6pqfzr -n frontend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-04830r -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-04830r -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get deploy resource-pwobt5 -n staging -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-pwobt5 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get pod resource-75a7ok -n test --no-headers | grep Running\nkubectl get pod resource-75a7ok -n test -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get pod resource-m6fgrt -n frontend --no-headers | grep Running\nkubectl get pod resource-m6fgrt -n frontend -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get pod resource-9u7ew1 -n prod --no-headers | grep Running\nkubectl get pod resource-9u7ew1 -n prod -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get deploy resource-z7sm28 -n frontend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-z7sm28 -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\nkubectl get pod resource-emn9n5 -n dev --no-headers | grep Running\nkubectl get pod resource-emn9n5 -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pvc resource-bntd4k -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-bntd4k -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-lklw59 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-lklw59 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get deploy resource-rlbv2y -n test -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-rlbv2y -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
    "setupScript": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-37snqm -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-upup4j -n prod --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ejgz8t -n test 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-tz0b8p -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-i533wq -n prod 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-6pqfzr-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-6pqfzr -n frontend 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-04830r -n kube-system 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-pwobt5 -n staging 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-75a7ok -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-m6fgrt -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-9u7ew1 -n prod --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-z7sm28 -n frontend 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-emn9n5 -n dev --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-bntd4k -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lklw59 -n frontend 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-rlbv2y -n test 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 16)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-37snqm` in namespace `staging`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-37snqm -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-37snqm -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-37snqm -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Create a Pod\nCreate a pod named `resource-upup4j` in namespace `prod` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-upup4j -n prod --no-headers | grep Running\nkubectl get pod resource-upup4j -n prod -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-upup4j -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ejgz8t` in namespace `test`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-ejgz8t -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-ejgz8t -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ejgz8t -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Create a Pod\nCreate a pod named `resource-tz0b8p` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-tz0b8p -n kube-system --no-headers | grep Running\nkubectl get pod resource-tz0b8p -n kube-system -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-tz0b8p -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Scale Deployment\nCreate a deployment named `resource-i533wq` in namespace `prod` using image `alpine`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-i533wq -n prod -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-i533wq -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-i533wq -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Expose Service\nExpose the deployment `resource-6pqfzr-dep` as a Service named `resource-6pqfzr` in namespace `frontend`.\nThe service should listen on port `3697` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-6pqfzr -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 3697\nkubectl get svc resource-6pqfzr -n frontend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-6pqfzr-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-6pqfzr -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-04830r` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-04830r -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-04830r -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-04830r -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Scale Deployment\nCreate a deployment named `resource-pwobt5` in namespace `staging` using image `redis`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-pwobt5 -n staging -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-pwobt5 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-pwobt5 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-75a7ok` in namespace `test` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-75a7ok -n test --no-headers | grep Running\nkubectl get pod resource-75a7ok -n test -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-75a7ok -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Create a Pod\nCreate a pod named `resource-m6fgrt` in namespace `frontend` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-m6fgrt -n frontend --no-headers | grep Running\nkubectl get pod resource-m6fgrt -n frontend -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-m6fgrt -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Create a Pod\nCreate a pod named `resource-9u7ew1` in namespace `prod` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-9u7ew1 -n prod --no-headers | grep Running\nkubectl get pod resource-9u7ew1 -n prod -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-9u7ew1 -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Scale Deployment\nCreate a deployment named `resource-z7sm28` in namespace `frontend` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-z7sm28 -n frontend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-z7sm28 -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-z7sm28 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-emn9n5` in namespace `dev` using image `postgres`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-emn9n5 -n dev --no-headers | grep Running\nkubectl get pod resource-emn9n5 -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-emn9n5 -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-bntd4k` in namespace `staging`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-bntd4k -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-bntd4k -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-bntd4k -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lklw59` in namespace `frontend`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-lklw59 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-lklw59 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lklw59 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Scale Deployment\nCreate a deployment named `resource-rlbv2y` in namespace `test` using image `postgres`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-rlbv2y -n test -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-rlbv2y -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-rlbv2y -n test 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-17": {
    "id": "auto-cka-17",
    "title": "Auto CKA Practice (Batch 17)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 17)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-e8jame` in namespace `test` using image `python:3.9`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-p6l23n` in namespace `prod`.\nRequest `2Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 3: Create a Pod\nCreate a pod named `resource-0laz5d` in namespace `backend` using image `memcached`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 4: Expose Service\nExpose the deployment `resource-9fckmz-dep` as a Service named `resource-9fckmz` in namespace `prod`.\nThe service should listen on port `5587` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 5: Expose Service\nExpose the deployment `resource-pywfkl-dep` as a Service named `resource-pywfkl` in namespace `default`.\nThe service should listen on port `7222` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 6: Scale Deployment\nCreate a deployment named `resource-ady5jc` in namespace `kube-system` using image `redis`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 7: Create a Pod\nCreate a pod named `resource-52r700` in namespace `dev` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 8: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 9: Create a Pod\nCreate a pod named `resource-u7lxiv` in namespace `dev` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 10: Expose Service\nExpose the deployment `resource-abiqcb-dep` as a Service named `resource-abiqcb` in namespace `test`.\nThe service should listen on port `7540` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 11: Scale Deployment\nCreate a deployment named `resource-ne3gun` in namespace `staging` using image `redis`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 12: Scale Deployment\nCreate a deployment named `resource-qy548m` in namespace `kube-system` using image `httpd`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n# Task 13: Expose Service\nExpose the deployment `resource-qblw73-dep` as a Service named `resource-qblw73` in namespace `frontend`.\nThe service should listen on port `5853` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 14: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-4pgn7x` in namespace `test`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 15: Scale Deployment\nCreate a deployment named `resource-k1cmle` in namespace `dev` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 16: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 17: Expose Service\nExpose the deployment `resource-4cb8e5-dep` as a Service named `resource-4cb8e5` in namespace `kube-system`.\nThe service should listen on port `7124` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Scale Deployment\nCreate a deployment named `resource-124q62` in namespace `kube-system` using image `python:3.9`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-d99xom` in namespace `backend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-e8jame -n test --no-headers | grep Running\nkubectl get pod resource-e8jame -n test -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get pvc resource-p6l23n -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-p6l23n -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get pod resource-0laz5d -n backend --no-headers | grep Running\nkubectl get pod resource-0laz5d -n backend -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get svc resource-9fckmz -n prod -o jsonpath='{.spec.ports[0].port}' | grep 5587\nkubectl get svc resource-9fckmz -n prod -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get svc resource-pywfkl -n default -o jsonpath='{.spec.ports[0].port}' | grep 7222\nkubectl get svc resource-pywfkl -n default -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-ady5jc -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-ady5jc -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get pod resource-52r700 -n dev --no-headers | grep Running\nkubectl get pod resource-52r700 -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-u7lxiv -n dev --no-headers | grep Running\nkubectl get pod resource-u7lxiv -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get svc resource-abiqcb -n test -o jsonpath='{.spec.ports[0].port}' | grep 7540\nkubectl get svc resource-abiqcb -n test -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get deploy resource-ne3gun -n staging -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-ne3gun -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get deploy resource-qy548m -n kube-system -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-qy548m -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\nkubectl get svc resource-qblw73 -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 5853\nkubectl get svc resource-qblw73 -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pvc resource-4pgn7x -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-4pgn7x -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get deploy resource-k1cmle -n dev -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-k1cmle -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-4cb8e5 -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7124\nkubectl get svc resource-4cb8e5 -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-124q62 -n kube-system -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-124q62 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\nkubectl get pvc resource-d99xom -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-d99xom -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
    "setupScript": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-e8jame -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-p6l23n -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0laz5d -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-9fckmz-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-9fckmz -n prod 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-pywfkl-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-pywfkl -n default 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-ady5jc -n kube-system 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-52r700 -n dev --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-u7lxiv -n dev --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-abiqcb-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-abiqcb -n test 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-ne3gun -n staging 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-qy548m -n kube-system 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-qblw73-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-qblw73 -n frontend 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-4pgn7x -n test 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-k1cmle -n dev 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-4cb8e5-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-4cb8e5 -n kube-system 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-124q62 -n kube-system 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-d99xom -n backend 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 17)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-e8jame` in namespace `test` using image `python:3.9`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-e8jame -n test --no-headers | grep Running\nkubectl get pod resource-e8jame -n test -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-e8jame -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-p6l23n` in namespace `prod`.\nRequest `2Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-p6l23n -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-p6l23n -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-p6l23n -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Create a Pod\nCreate a pod named `resource-0laz5d` in namespace `backend` using image `memcached`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-0laz5d -n backend --no-headers | grep Running\nkubectl get pod resource-0laz5d -n backend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0laz5d -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Expose Service\nExpose the deployment `resource-9fckmz-dep` as a Service named `resource-9fckmz` in namespace `prod`.\nThe service should listen on port `5587` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-9fckmz -n prod -o jsonpath='{.spec.ports[0].port}' | grep 5587\nkubectl get svc resource-9fckmz -n prod -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-9fckmz-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-9fckmz -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Expose Service\nExpose the deployment `resource-pywfkl-dep` as a Service named `resource-pywfkl` in namespace `default`.\nThe service should listen on port `7222` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-pywfkl -n default -o jsonpath='{.spec.ports[0].port}' | grep 7222\nkubectl get svc resource-pywfkl -n default -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-pywfkl-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-pywfkl -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Scale Deployment\nCreate a deployment named `resource-ady5jc` in namespace `kube-system` using image `redis`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-ady5jc -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-ady5jc -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-ady5jc -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Create a Pod\nCreate a pod named `resource-52r700` in namespace `dev` using image `httpd`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-52r700 -n dev --no-headers | grep Running\nkubectl get pod resource-52r700 -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-52r700 -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Create a Pod\nCreate a pod named `resource-u7lxiv` in namespace `dev` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-u7lxiv -n dev --no-headers | grep Running\nkubectl get pod resource-u7lxiv -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-u7lxiv -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Expose Service\nExpose the deployment `resource-abiqcb-dep` as a Service named `resource-abiqcb` in namespace `test`.\nThe service should listen on port `7540` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-abiqcb -n test -o jsonpath='{.spec.ports[0].port}' | grep 7540\nkubectl get svc resource-abiqcb -n test -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-abiqcb-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-abiqcb -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Scale Deployment\nCreate a deployment named `resource-ne3gun` in namespace `staging` using image `redis`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-ne3gun -n staging -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-ne3gun -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-ne3gun -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Scale Deployment\nCreate a deployment named `resource-qy548m` in namespace `kube-system` using image `httpd`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-qy548m -n kube-system -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-qy548m -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-qy548m -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Expose Service\nExpose the deployment `resource-qblw73-dep` as a Service named `resource-qblw73` in namespace `frontend`.\nThe service should listen on port `5853` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-qblw73 -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 5853\nkubectl get svc resource-qblw73 -n frontend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-qblw73-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-qblw73 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-4pgn7x` in namespace `test`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-4pgn7x -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-4pgn7x -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-4pgn7x -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Scale Deployment\nCreate a deployment named `resource-k1cmle` in namespace `dev` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-k1cmle -n dev -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-k1cmle -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-k1cmle -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Expose Service\nExpose the deployment `resource-4cb8e5-dep` as a Service named `resource-4cb8e5` in namespace `kube-system`.\nThe service should listen on port `7124` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-4cb8e5 -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7124\nkubectl get svc resource-4cb8e5 -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-4cb8e5-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-4cb8e5 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Scale Deployment\nCreate a deployment named `resource-124q62` in namespace `kube-system` using image `python:3.9`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-124q62 -n kube-system -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-124q62 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-124q62 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-d99xom` in namespace `backend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-d99xom -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-d99xom -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-d99xom -n backend 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-18": {
    "id": "auto-cka-18",
    "title": "Auto CKA Practice (Batch 18)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 18)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-66gkjh` in namespace `prod` using image `mysql`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 2: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 3: Create a Pod\nCreate a pod named `resource-2umvx8` in namespace `test` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 4: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 5: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 6: Scale Deployment\nCreate a deployment named `resource-4bp2qs` in namespace `frontend` using image `httpd`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n# Task 7: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-l59e86` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 8: Create a Pod\nCreate a pod named `resource-vaihxb` in namespace `kube-system` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 9: Scale Deployment\nCreate a deployment named `resource-r7y0pa` in namespace `staging` using image `alpine`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n# Task 10: Scale Deployment\nCreate a deployment named `resource-trx7jm` in namespace `prod` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-cvjv7x` in namespace `backend` using image `postgres`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 12: Scale Deployment\nCreate a deployment named `resource-kx3952` in namespace `staging` using image `alpine`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-mvm17s` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 14: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 15: Scale Deployment\nCreate a deployment named `resource-73g1r8` in namespace `kube-system` using image `redis`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-33banv-dep` as a Service named `resource-33banv` in namespace `dev`.\nThe service should listen on port `7705` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 17: Expose Service\nExpose the deployment `resource-y2ziyw-dep` as a Service named `resource-y2ziyw` in namespace `test`.\nThe service should listen on port `5918` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 18: Scale Deployment\nCreate a deployment named `resource-bwpnuc` in namespace `test` using image `nginx`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-4ttzeo` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3sd9s1` in namespace `frontend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-66gkjh -n prod --no-headers | grep Running\nkubectl get pod resource-66gkjh -n prod -o jsonpath='{.metadata.labels.app}' | grep web\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-2umvx8 -n test --no-headers | grep Running\nkubectl get pod resource-2umvx8 -n test -o jsonpath='{.metadata.labels.release}' | grep stable\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-4bp2qs -n frontend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-4bp2qs -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\nkubectl get pvc resource-l59e86 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-l59e86 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pod resource-vaihxb -n kube-system --no-headers | grep Running\nkubectl get pod resource-vaihxb -n kube-system -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get deploy resource-r7y0pa -n staging -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-r7y0pa -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\nkubectl get deploy resource-trx7jm -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-trx7jm -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\nkubectl get pod resource-cvjv7x -n backend --no-headers | grep Running\nkubectl get pod resource-cvjv7x -n backend -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get deploy resource-kx3952 -n staging -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-kx3952 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\nkubectl get pvc resource-mvm17s -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-mvm17s -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-73g1r8 -n kube-system -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-73g1r8 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get svc resource-33banv -n dev -o jsonpath='{.spec.ports[0].port}' | grep 7705\nkubectl get svc resource-33banv -n dev -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get svc resource-y2ziyw -n test -o jsonpath='{.spec.ports[0].port}' | grep 5918\nkubectl get svc resource-y2ziyw -n test -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-bwpnuc -n test -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-bwpnuc -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get pvc resource-4ttzeo -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-4ttzeo -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pvc resource-3sd9s1 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-3sd9s1 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
    "setupScript": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-66gkjh -n prod --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-2umvx8 -n test --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-4bp2qs -n frontend 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-l59e86 -n test 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-vaihxb -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-r7y0pa -n staging 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-trx7jm -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-cvjv7x -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-kx3952 -n staging 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-mvm17s -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-73g1r8 -n kube-system 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-33banv-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-33banv -n dev 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-y2ziyw-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-y2ziyw -n test 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-bwpnuc -n test 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-4ttzeo -n kube-system 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3sd9s1 -n frontend 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 18)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-66gkjh` in namespace `prod` using image `mysql`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-66gkjh -n prod --no-headers | grep Running\nkubectl get pod resource-66gkjh -n prod -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-66gkjh -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Create a Pod\nCreate a pod named `resource-2umvx8` in namespace `test` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-2umvx8 -n test --no-headers | grep Running\nkubectl get pod resource-2umvx8 -n test -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-2umvx8 -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Scale Deployment\nCreate a deployment named `resource-4bp2qs` in namespace `frontend` using image `httpd`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-4bp2qs -n frontend -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-4bp2qs -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-4bp2qs -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-l59e86` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-l59e86 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-l59e86 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-l59e86 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Create a Pod\nCreate a pod named `resource-vaihxb` in namespace `kube-system` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-vaihxb -n kube-system --no-headers | grep Running\nkubectl get pod resource-vaihxb -n kube-system -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-vaihxb -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Scale Deployment\nCreate a deployment named `resource-r7y0pa` in namespace `staging` using image `alpine`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-r7y0pa -n staging -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-r7y0pa -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-r7y0pa -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Scale Deployment\nCreate a deployment named `resource-trx7jm` in namespace `prod` using image `postgres`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-trx7jm -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-trx7jm -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-trx7jm -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-cvjv7x` in namespace `backend` using image `postgres`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-cvjv7x -n backend --no-headers | grep Running\nkubectl get pod resource-cvjv7x -n backend -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-cvjv7x -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Scale Deployment\nCreate a deployment named `resource-kx3952` in namespace `staging` using image `alpine`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-kx3952 -n staging -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-kx3952 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-kx3952 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-mvm17s` in namespace `backend`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-mvm17s -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-mvm17s -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-mvm17s -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Scale Deployment\nCreate a deployment named `resource-73g1r8` in namespace `kube-system` using image `redis`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-73g1r8 -n kube-system -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-73g1r8 -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-73g1r8 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-33banv-dep` as a Service named `resource-33banv` in namespace `dev`.\nThe service should listen on port `7705` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-33banv -n dev -o jsonpath='{.spec.ports[0].port}' | grep 7705\nkubectl get svc resource-33banv -n dev -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-33banv-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-33banv -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Expose Service\nExpose the deployment `resource-y2ziyw-dep` as a Service named `resource-y2ziyw` in namespace `test`.\nThe service should listen on port `5918` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-y2ziyw -n test -o jsonpath='{.spec.ports[0].port}' | grep 5918\nkubectl get svc resource-y2ziyw -n test -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-y2ziyw-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-y2ziyw -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Scale Deployment\nCreate a deployment named `resource-bwpnuc` in namespace `test` using image `nginx`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-bwpnuc -n test -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-bwpnuc -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-bwpnuc -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-4ttzeo` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-4ttzeo -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-4ttzeo -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-4ttzeo -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3sd9s1` in namespace `frontend`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-3sd9s1 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-3sd9s1 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3sd9s1 -n frontend 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-19": {
    "id": "auto-cka-19",
    "title": "Auto CKA Practice (Batch 19)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 19)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-ohy82d` in namespace `frontend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 2: Create a Pod\nCreate a pod named `resource-pwf7sk` in namespace `test` using image `httpd`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 3: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 4: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 5: Scale Deployment\nCreate a deployment named `resource-sjohsx` in namespace `frontend` using image `alpine`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n# Task 6: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-gwhdi9` in namespace `prod`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 7: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-bkbgpt` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-73lrm6` in namespace `default`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 9: Create a Pod\nCreate a pod named `resource-p3bi2o` in namespace `dev` using image `postgres`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 10: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 11: Expose Service\nExpose the deployment `resource-0rmkbk-dep` as a Service named `resource-0rmkbk` in namespace `prod`.\nThe service should listen on port `6864` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 12: Scale Deployment\nCreate a deployment named `resource-kj6e5l` in namespace `kube-system` using image `redis`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 13: Create a Pod\nCreate a pod named `resource-2z9dey` in namespace `kube-system` using image `python:3.9`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 14: Create a Pod\nCreate a pod named `resource-j9wzvf` in namespace `test` using image `postgres`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-7qbqmm` in namespace `kube-system`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 16: Create a Pod\nCreate a pod named `resource-8n6meh` in namespace `test` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 17: Scale Deployment\nCreate a deployment named `resource-3pwmxq` in namespace `frontend` using image `redis`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 18: Scale Deployment\nCreate a deployment named `resource-ctln61` in namespace `staging` using image `memcached`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n# Task 19: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 20: Expose Service\nExpose the deployment `resource-x8ube8-dep` as a Service named `resource-x8ube8` in namespace `test`.\nThe service should listen on port `4586` and be of type `NodePort`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-ohy82d -n frontend --no-headers | grep Running\nkubectl get pod resource-ohy82d -n frontend -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get pod resource-pwf7sk -n test --no-headers | grep Running\nkubectl get pod resource-pwf7sk -n test -o jsonpath='{.metadata.labels.tier}' | grep frontend\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-sjohsx -n frontend -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-sjohsx -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\nkubectl get pvc resource-gwhdi9 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-gwhdi9 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get pvc resource-bkbgpt -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-bkbgpt -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pvc resource-73lrm6 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-73lrm6 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pod resource-p3bi2o -n dev --no-headers | grep Running\nkubectl get pod resource-p3bi2o -n dev -o jsonpath='{.metadata.labels.env}' | grep prod\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-0rmkbk -n prod -o jsonpath='{.spec.ports[0].port}' | grep 6864\nkubectl get svc resource-0rmkbk -n prod -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-kj6e5l -n kube-system -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-kj6e5l -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get pod resource-2z9dey -n kube-system --no-headers | grep Running\nkubectl get pod resource-2z9dey -n kube-system -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pod resource-j9wzvf -n test --no-headers | grep Running\nkubectl get pod resource-j9wzvf -n test -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pvc resource-7qbqmm -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-7qbqmm -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pod resource-8n6meh -n test --no-headers | grep Running\nkubectl get pod resource-8n6meh -n test -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get deploy resource-3pwmxq -n frontend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-3pwmxq -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get deploy resource-ctln61 -n staging -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-ctln61 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-x8ube8 -n test -o jsonpath='{.spec.ports[0].port}' | grep 4586\nkubectl get svc resource-x8ube8 -n test -o jsonpath='{.spec.type}' | grep NodePort\n",
    "setupScript": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ohy82d -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-pwf7sk -n test --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-sjohsx -n frontend 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-gwhdi9 -n prod 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-bkbgpt -n test 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-73lrm6 -n default 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-p3bi2o -n dev --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-0rmkbk-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-0rmkbk -n prod 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-kj6e5l -n kube-system 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-2z9dey -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-j9wzvf -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-7qbqmm -n kube-system 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-8n6meh -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-3pwmxq -n frontend 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-ctln61 -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-x8ube8-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-x8ube8 -n test 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 19)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-ohy82d` in namespace `frontend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-ohy82d -n frontend --no-headers | grep Running\nkubectl get pod resource-ohy82d -n frontend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ohy82d -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Create a Pod\nCreate a pod named `resource-pwf7sk` in namespace `test` using image `httpd`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-pwf7sk -n test --no-headers | grep Running\nkubectl get pod resource-pwf7sk -n test -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-pwf7sk -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Scale Deployment\nCreate a deployment named `resource-sjohsx` in namespace `frontend` using image `alpine`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-sjohsx -n frontend -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-sjohsx -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-sjohsx -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-gwhdi9` in namespace `prod`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-gwhdi9 -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-gwhdi9 -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-gwhdi9 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-bkbgpt` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-bkbgpt -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-bkbgpt -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-bkbgpt -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-73lrm6` in namespace `default`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-73lrm6 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-73lrm6 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-73lrm6 -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Create a Pod\nCreate a pod named `resource-p3bi2o` in namespace `dev` using image `postgres`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-p3bi2o -n dev --no-headers | grep Running\nkubectl get pod resource-p3bi2o -n dev -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-p3bi2o -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Expose Service\nExpose the deployment `resource-0rmkbk-dep` as a Service named `resource-0rmkbk` in namespace `prod`.\nThe service should listen on port `6864` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-0rmkbk -n prod -o jsonpath='{.spec.ports[0].port}' | grep 6864\nkubectl get svc resource-0rmkbk -n prod -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-0rmkbk-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-0rmkbk -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Scale Deployment\nCreate a deployment named `resource-kj6e5l` in namespace `kube-system` using image `redis`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-kj6e5l -n kube-system -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-kj6e5l -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-kj6e5l -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Create a Pod\nCreate a pod named `resource-2z9dey` in namespace `kube-system` using image `python:3.9`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-2z9dey -n kube-system --no-headers | grep Running\nkubectl get pod resource-2z9dey -n kube-system -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-2z9dey -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Create a Pod\nCreate a pod named `resource-j9wzvf` in namespace `test` using image `postgres`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-j9wzvf -n test --no-headers | grep Running\nkubectl get pod resource-j9wzvf -n test -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-j9wzvf -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-7qbqmm` in namespace `kube-system`.\nRequest `10Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-7qbqmm -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-7qbqmm -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-7qbqmm -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Create a Pod\nCreate a pod named `resource-8n6meh` in namespace `test` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-8n6meh -n test --no-headers | grep Running\nkubectl get pod resource-8n6meh -n test -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-8n6meh -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Scale Deployment\nCreate a deployment named `resource-3pwmxq` in namespace `frontend` using image `redis`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-3pwmxq -n frontend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-3pwmxq -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-3pwmxq -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Scale Deployment\nCreate a deployment named `resource-ctln61` in namespace `staging` using image `memcached`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-ctln61 -n staging -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-ctln61 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-ctln61 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Expose Service\nExpose the deployment `resource-x8ube8-dep` as a Service named `resource-x8ube8` in namespace `test`.\nThe service should listen on port `4586` and be of type `NodePort`.\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-x8ube8 -n test -o jsonpath='{.spec.ports[0].port}' | grep 4586\nkubectl get svc resource-x8ube8 -n test -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-x8ube8-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-x8ube8 -n test 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-2": {
    "id": "auto-cka-2",
    "title": "Auto CKA Practice (Batch 2)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 2)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-5nvnbe` in namespace `frontend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ezwkjp` in namespace `frontend`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 3: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 4: Expose Service\nExpose the deployment `resource-awwhye-dep` as a Service named `resource-awwhye` in namespace `kube-system`.\nThe service should listen on port `6974` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 5: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-dpxgy9` in namespace `frontend`.\nRequest `2Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 6: Expose Service\nExpose the deployment `resource-lcas2a-dep` as a Service named `resource-lcas2a` in namespace `default`.\nThe service should listen on port `5654` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 7: Create a Pod\nCreate a pod named `resource-d0xnxn` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 8: Create a Pod\nCreate a pod named `resource-qq888l` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 9: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 10: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 11: Expose Service\nExpose the deployment `resource-k9irdd-dep` as a Service named `resource-k9irdd` in namespace `default`.\nThe service should listen on port `5047` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 12: Create a Pod\nCreate a pod named `resource-s0ro8w` in namespace `backend` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 13: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 14: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-zwkpke` in namespace `backend` using image `nginx`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 16: Scale Deployment\nCreate a deployment named `resource-hcfsbc` in namespace `kube-system` using image `httpd`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-gbk7sh` in namespace `dev`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 18: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-fo8xeo` in namespace `backend`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 19: Expose Service\nExpose the deployment `resource-qtys04-dep` as a Service named `resource-qtys04` in namespace `backend`.\nThe service should listen on port `8300` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 20: Create a Pod\nCreate a pod named `resource-o2agjj` in namespace `backend` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-5nvnbe -n frontend --no-headers | grep Running\nkubectl get pod resource-5nvnbe -n frontend -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get pvc resource-ezwkjp -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-ezwkjp -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-awwhye -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 6974\nkubectl get svc resource-awwhye -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pvc resource-dpxgy9 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-dpxgy9 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get svc resource-lcas2a -n default -o jsonpath='{.spec.ports[0].port}' | grep 5654\nkubectl get svc resource-lcas2a -n default -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-d0xnxn -n kube-system --no-headers | grep Running\nkubectl get pod resource-d0xnxn -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-qq888l -n kube-system --no-headers | grep Running\nkubectl get pod resource-qq888l -n kube-system -o jsonpath='{.metadata.labels.env}' | grep prod\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-k9irdd -n default -o jsonpath='{.spec.ports[0].port}' | grep 5047\nkubectl get svc resource-k9irdd -n default -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-s0ro8w -n backend --no-headers | grep Running\nkubectl get pod resource-s0ro8w -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-zwkpke -n backend --no-headers | grep Running\nkubectl get pod resource-zwkpke -n backend -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get deploy resource-hcfsbc -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-hcfsbc -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\nkubectl get pvc resource-gbk7sh -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-gbk7sh -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-fo8xeo -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-fo8xeo -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get svc resource-qtys04 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 8300\nkubectl get svc resource-qtys04 -n backend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pod resource-o2agjj -n backend --no-headers | grep Running\nkubectl get pod resource-o2agjj -n backend -o jsonpath='{.metadata.labels.release}' | grep stable\n",
    "setupScript": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-5nvnbe -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ezwkjp -n frontend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-awwhye-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-awwhye -n kube-system 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-dpxgy9 -n frontend 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-lcas2a-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-lcas2a -n default 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-d0xnxn -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-qq888l -n kube-system --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-k9irdd-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-k9irdd -n default 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-s0ro8w -n backend --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-zwkpke -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-hcfsbc -n kube-system 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-gbk7sh -n dev 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-fo8xeo -n backend 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-qtys04-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-qtys04 -n backend 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-o2agjj -n backend --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 2)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-5nvnbe` in namespace `frontend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-5nvnbe -n frontend --no-headers | grep Running\nkubectl get pod resource-5nvnbe -n frontend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-5nvnbe -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ezwkjp` in namespace `frontend`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-ezwkjp -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-ezwkjp -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ezwkjp -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Expose Service\nExpose the deployment `resource-awwhye-dep` as a Service named `resource-awwhye` in namespace `kube-system`.\nThe service should listen on port `6974` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-awwhye -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 6974\nkubectl get svc resource-awwhye -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-awwhye-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-awwhye -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-dpxgy9` in namespace `frontend`.\nRequest `2Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-dpxgy9 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-dpxgy9 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-dpxgy9 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Expose Service\nExpose the deployment `resource-lcas2a-dep` as a Service named `resource-lcas2a` in namespace `default`.\nThe service should listen on port `5654` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-lcas2a -n default -o jsonpath='{.spec.ports[0].port}' | grep 5654\nkubectl get svc resource-lcas2a -n default -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-lcas2a-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-lcas2a -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Create a Pod\nCreate a pod named `resource-d0xnxn` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-d0xnxn -n kube-system --no-headers | grep Running\nkubectl get pod resource-d0xnxn -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-d0xnxn -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Create a Pod\nCreate a pod named `resource-qq888l` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-qq888l -n kube-system --no-headers | grep Running\nkubectl get pod resource-qq888l -n kube-system -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-qq888l -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Expose Service\nExpose the deployment `resource-k9irdd-dep` as a Service named `resource-k9irdd` in namespace `default`.\nThe service should listen on port `5047` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-k9irdd -n default -o jsonpath='{.spec.ports[0].port}' | grep 5047\nkubectl get svc resource-k9irdd -n default -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-k9irdd-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-k9irdd -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Create a Pod\nCreate a pod named `resource-s0ro8w` in namespace `backend` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-s0ro8w -n backend --no-headers | grep Running\nkubectl get pod resource-s0ro8w -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-s0ro8w -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-zwkpke` in namespace `backend` using image `nginx`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-zwkpke -n backend --no-headers | grep Running\nkubectl get pod resource-zwkpke -n backend -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-zwkpke -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Scale Deployment\nCreate a deployment named `resource-hcfsbc` in namespace `kube-system` using image `httpd`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-hcfsbc -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-hcfsbc -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-hcfsbc -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-gbk7sh` in namespace `dev`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-gbk7sh -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-gbk7sh -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-gbk7sh -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-fo8xeo` in namespace `backend`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-fo8xeo -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-fo8xeo -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-fo8xeo -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Expose Service\nExpose the deployment `resource-qtys04-dep` as a Service named `resource-qtys04` in namespace `backend`.\nThe service should listen on port `8300` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-qtys04 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 8300\nkubectl get svc resource-qtys04 -n backend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-qtys04-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-qtys04 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Create a Pod\nCreate a pod named `resource-o2agjj` in namespace `backend` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-o2agjj -n backend --no-headers | grep Running\nkubectl get pod resource-o2agjj -n backend -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-o2agjj -n backend --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-20": {
    "id": "auto-cka-20",
    "title": "Auto CKA Practice (Batch 20)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 20)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-nc47ez` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 2: Create a Pod\nCreate a pod named `resource-xqahz3` in namespace `backend` using image `postgres`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-mykq3e` in namespace `default`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 4: Create a Pod\nCreate a pod named `resource-32ii6d` in namespace `frontend` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 5: Scale Deployment\nCreate a deployment named `resource-xz8ilm` in namespace `dev` using image `busybox`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n# Task 6: Expose Service\nExpose the deployment `resource-z2ea64-dep` as a Service named `resource-z2ea64` in namespace `frontend`.\nThe service should listen on port `3990` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 7: Create a Pod\nCreate a pod named `resource-e1d8ml` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 8: Create a Pod\nCreate a pod named `resource-c8bqq5` in namespace `dev` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 9: Expose Service\nExpose the deployment `resource-kfpb2q-dep` as a Service named `resource-kfpb2q` in namespace `prod`.\nThe service should listen on port `7972` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-1oq5r9` in namespace `test`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 11: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 12: Create a Pod\nCreate a pod named `resource-kaz3nc` in namespace `kube-system` using image `redis`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 13: Expose Service\nExpose the deployment `resource-8ajkoz-dep` as a Service named `resource-8ajkoz` in namespace `dev`.\nThe service should listen on port `5436` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 14: Expose Service\nExpose the deployment `resource-hps3h7-dep` as a Service named `resource-hps3h7` in namespace `test`.\nThe service should listen on port `3048` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-6e5q0a` in namespace `test`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 16: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 17: Create a Pod\nCreate a pod named `resource-0snnwz` in namespace `frontend` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 18: Create a Pod\nCreate a pod named `resource-ze939j` in namespace `staging` using image `python:3.9`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 19: Expose Service\nExpose the deployment `resource-b5dllz-dep` as a Service named `resource-b5dllz` in namespace `default`.\nThe service should listen on port `7655` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 20: Create a Pod\nCreate a pod named `resource-hcptkk` in namespace `kube-system` using image `python:3.9`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pvc resource-nc47ez -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-nc47ez -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pod resource-xqahz3 -n backend --no-headers | grep Running\nkubectl get pod resource-xqahz3 -n backend -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pvc resource-mykq3e -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-mykq3e -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get pod resource-32ii6d -n frontend --no-headers | grep Running\nkubectl get pod resource-32ii6d -n frontend -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get deploy resource-xz8ilm -n dev -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-xz8ilm -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\nkubectl get svc resource-z2ea64 -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 3990\nkubectl get svc resource-z2ea64 -n frontend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pod resource-e1d8ml -n kube-system --no-headers | grep Running\nkubectl get pod resource-e1d8ml -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-c8bqq5 -n dev --no-headers | grep Running\nkubectl get pod resource-c8bqq5 -n dev -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get svc resource-kfpb2q -n prod -o jsonpath='{.spec.ports[0].port}' | grep 7972\nkubectl get svc resource-kfpb2q -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pvc resource-1oq5r9 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-1oq5r9 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-kaz3nc -n kube-system --no-headers | grep Running\nkubectl get pod resource-kaz3nc -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get svc resource-8ajkoz -n dev -o jsonpath='{.spec.ports[0].port}' | grep 5436\nkubectl get svc resource-8ajkoz -n dev -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get svc resource-hps3h7 -n test -o jsonpath='{.spec.ports[0].port}' | grep 3048\nkubectl get svc resource-hps3h7 -n test -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pvc resource-6e5q0a -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-6e5q0a -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-0snnwz -n frontend --no-headers | grep Running\nkubectl get pod resource-0snnwz -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pod resource-ze939j -n staging --no-headers | grep Running\nkubectl get pod resource-ze939j -n staging -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get svc resource-b5dllz -n default -o jsonpath='{.spec.ports[0].port}' | grep 7655\nkubectl get svc resource-b5dllz -n default -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-hcptkk -n kube-system --no-headers | grep Running\nkubectl get pod resource-hcptkk -n kube-system -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
    "setupScript": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-nc47ez -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-xqahz3 -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-mykq3e -n default 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-32ii6d -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-xz8ilm -n dev 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-z2ea64-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-z2ea64 -n frontend 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-e1d8ml -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-c8bqq5 -n dev --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-kfpb2q-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-kfpb2q -n prod 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-1oq5r9 -n test 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-kaz3nc -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-8ajkoz-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-8ajkoz -n dev 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-hps3h7-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-hps3h7 -n test 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-6e5q0a -n test 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0snnwz -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ze939j -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-b5dllz-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-b5dllz -n default 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-hcptkk -n kube-system --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 20)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-nc47ez` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-nc47ez -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-nc47ez -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-nc47ez -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Create a Pod\nCreate a pod named `resource-xqahz3` in namespace `backend` using image `postgres`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-xqahz3 -n backend --no-headers | grep Running\nkubectl get pod resource-xqahz3 -n backend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-xqahz3 -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-mykq3e` in namespace `default`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-mykq3e -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-mykq3e -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-mykq3e -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Create a Pod\nCreate a pod named `resource-32ii6d` in namespace `frontend` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-32ii6d -n frontend --no-headers | grep Running\nkubectl get pod resource-32ii6d -n frontend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-32ii6d -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Scale Deployment\nCreate a deployment named `resource-xz8ilm` in namespace `dev` using image `busybox`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-xz8ilm -n dev -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-xz8ilm -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-xz8ilm -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Expose Service\nExpose the deployment `resource-z2ea64-dep` as a Service named `resource-z2ea64` in namespace `frontend`.\nThe service should listen on port `3990` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-z2ea64 -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 3990\nkubectl get svc resource-z2ea64 -n frontend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-z2ea64-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-z2ea64 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Create a Pod\nCreate a pod named `resource-e1d8ml` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-e1d8ml -n kube-system --no-headers | grep Running\nkubectl get pod resource-e1d8ml -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-e1d8ml -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Create a Pod\nCreate a pod named `resource-c8bqq5` in namespace `dev` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-c8bqq5 -n dev --no-headers | grep Running\nkubectl get pod resource-c8bqq5 -n dev -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-c8bqq5 -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Expose Service\nExpose the deployment `resource-kfpb2q-dep` as a Service named `resource-kfpb2q` in namespace `prod`.\nThe service should listen on port `7972` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-kfpb2q -n prod -o jsonpath='{.spec.ports[0].port}' | grep 7972\nkubectl get svc resource-kfpb2q -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-kfpb2q-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-kfpb2q -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-1oq5r9` in namespace `test`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-1oq5r9 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-1oq5r9 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-1oq5r9 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Create a Pod\nCreate a pod named `resource-kaz3nc` in namespace `kube-system` using image `redis`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-kaz3nc -n kube-system --no-headers | grep Running\nkubectl get pod resource-kaz3nc -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-kaz3nc -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Expose Service\nExpose the deployment `resource-8ajkoz-dep` as a Service named `resource-8ajkoz` in namespace `dev`.\nThe service should listen on port `5436` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-8ajkoz -n dev -o jsonpath='{.spec.ports[0].port}' | grep 5436\nkubectl get svc resource-8ajkoz -n dev -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-8ajkoz-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-8ajkoz -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Expose Service\nExpose the deployment `resource-hps3h7-dep` as a Service named `resource-hps3h7` in namespace `test`.\nThe service should listen on port `3048` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-hps3h7 -n test -o jsonpath='{.spec.ports[0].port}' | grep 3048\nkubectl get svc resource-hps3h7 -n test -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-hps3h7-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-hps3h7 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-6e5q0a` in namespace `test`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-6e5q0a -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-6e5q0a -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-6e5q0a -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Create a Pod\nCreate a pod named `resource-0snnwz` in namespace `frontend` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-0snnwz -n frontend --no-headers | grep Running\nkubectl get pod resource-0snnwz -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0snnwz -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Create a Pod\nCreate a pod named `resource-ze939j` in namespace `staging` using image `python:3.9`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-ze939j -n staging --no-headers | grep Running\nkubectl get pod resource-ze939j -n staging -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ze939j -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Expose Service\nExpose the deployment `resource-b5dllz-dep` as a Service named `resource-b5dllz` in namespace `default`.\nThe service should listen on port `7655` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-b5dllz -n default -o jsonpath='{.spec.ports[0].port}' | grep 7655\nkubectl get svc resource-b5dllz -n default -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-b5dllz-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-b5dllz -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Create a Pod\nCreate a pod named `resource-hcptkk` in namespace `kube-system` using image `python:3.9`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-hcptkk -n kube-system --no-headers | grep Running\nkubectl get pod resource-hcptkk -n kube-system -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-hcptkk -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-3": {
    "id": "auto-cka-3",
    "title": "Auto CKA Practice (Batch 3)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 3)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3k8x7f` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-bik7eh` in namespace `prod`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 4: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-kqmp61` in namespace `test`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 5: Scale Deployment\nCreate a deployment named `resource-vbvbxr` in namespace `kube-system` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 6: Expose Service\nExpose the deployment `resource-jfftxs-dep` as a Service named `resource-jfftxs` in namespace `backend`.\nThe service should listen on port `5768` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 7: Scale Deployment\nCreate a deployment named `resource-au2srh` in namespace `default` using image `redis`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 8: Create a Pod\nCreate a pod named `resource-4jec9c` in namespace `test` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 9: Expose Service\nExpose the deployment `resource-dv6pzm-dep` as a Service named `resource-dv6pzm` in namespace `backend`.\nThe service should listen on port `8264` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lnjk2l` in namespace `prod`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 11: Expose Service\nExpose the deployment `resource-1nzzsp-dep` as a Service named `resource-1nzzsp` in namespace `kube-system`.\nThe service should listen on port `7571` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 12: Expose Service\nExpose the deployment `resource-y19sn5-dep` as a Service named `resource-y19sn5` in namespace `test`.\nThe service should listen on port `6574` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-rjesh8` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 14: Create a Pod\nCreate a pod named `resource-5vbv89` in namespace `staging` using image `httpd`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-91a63z` in namespace `staging` using image `nginx`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-duk0od-dep` as a Service named `resource-duk0od` in namespace `kube-system`.\nThe service should listen on port `4767` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 17: Expose Service\nExpose the deployment `resource-3rvvmd-dep` as a Service named `resource-3rvvmd` in namespace `prod`.\nThe service should listen on port `6624` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Scale Deployment\nCreate a deployment named `resource-lv8c6g` in namespace `staging` using image `nginx`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-wqju5i` in namespace `dev`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n",
    "verifyScript": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-3k8x7f -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-3k8x7f -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-bik7eh -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-bik7eh -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-kqmp61 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-kqmp61 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get deploy resource-vbvbxr -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-vbvbxr -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\nkubectl get svc resource-jfftxs -n backend -o jsonpath='{.spec.ports[0].port}' | grep 5768\nkubectl get svc resource-jfftxs -n backend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-au2srh -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-au2srh -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get pod resource-4jec9c -n test --no-headers | grep Running\nkubectl get pod resource-4jec9c -n test -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get svc resource-dv6pzm -n backend -o jsonpath='{.spec.ports[0].port}' | grep 8264\nkubectl get svc resource-dv6pzm -n backend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-lnjk2l -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-lnjk2l -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get svc resource-1nzzsp -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7571\nkubectl get svc resource-1nzzsp -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get svc resource-y19sn5 -n test -o jsonpath='{.spec.ports[0].port}' | grep 6574\nkubectl get svc resource-y19sn5 -n test -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-rjesh8 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-rjesh8 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pod resource-5vbv89 -n staging --no-headers | grep Running\nkubectl get pod resource-5vbv89 -n staging -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get pod resource-91a63z -n staging --no-headers | grep Running\nkubectl get pod resource-91a63z -n staging -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get svc resource-duk0od -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4767\nkubectl get svc resource-duk0od -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get svc resource-3rvvmd -n prod -o jsonpath='{.spec.ports[0].port}' | grep 6624\nkubectl get svc resource-3rvvmd -n prod -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-lv8c6g -n staging -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-lv8c6g -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get pvc resource-wqju5i -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-wqju5i -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
    "setupScript": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3k8x7f -n test 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-bik7eh -n prod 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-kqmp61 -n test 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-vbvbxr -n kube-system 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-jfftxs-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-jfftxs -n backend 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-au2srh -n default 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4jec9c -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-dv6pzm-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-dv6pzm -n backend 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lnjk2l -n prod 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-1nzzsp-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-1nzzsp -n kube-system 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-y19sn5-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-y19sn5 -n test 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-rjesh8 -n kube-system 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-5vbv89 -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-91a63z -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-duk0od-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-duk0od -n kube-system 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-3rvvmd-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-3rvvmd -n prod 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-lv8c6g -n staging 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-wqju5i -n dev 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 3)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-3k8x7f` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-3k8x7f -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-3k8x7f -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-3k8x7f -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-bik7eh` in namespace `prod`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-bik7eh -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-bik7eh -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-bik7eh -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-kqmp61` in namespace `test`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-kqmp61 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-kqmp61 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-kqmp61 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Scale Deployment\nCreate a deployment named `resource-vbvbxr` in namespace `kube-system` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-vbvbxr -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-vbvbxr -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-vbvbxr -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Expose Service\nExpose the deployment `resource-jfftxs-dep` as a Service named `resource-jfftxs` in namespace `backend`.\nThe service should listen on port `5768` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-jfftxs -n backend -o jsonpath='{.spec.ports[0].port}' | grep 5768\nkubectl get svc resource-jfftxs -n backend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-jfftxs-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-jfftxs -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Scale Deployment\nCreate a deployment named `resource-au2srh` in namespace `default` using image `redis`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-au2srh -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-au2srh -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-au2srh -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Create a Pod\nCreate a pod named `resource-4jec9c` in namespace `test` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-4jec9c -n test --no-headers | grep Running\nkubectl get pod resource-4jec9c -n test -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4jec9c -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Expose Service\nExpose the deployment `resource-dv6pzm-dep` as a Service named `resource-dv6pzm` in namespace `backend`.\nThe service should listen on port `8264` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-dv6pzm -n backend -o jsonpath='{.spec.ports[0].port}' | grep 8264\nkubectl get svc resource-dv6pzm -n backend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-dv6pzm-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-dv6pzm -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lnjk2l` in namespace `prod`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-lnjk2l -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-lnjk2l -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lnjk2l -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Expose Service\nExpose the deployment `resource-1nzzsp-dep` as a Service named `resource-1nzzsp` in namespace `kube-system`.\nThe service should listen on port `7571` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-1nzzsp -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7571\nkubectl get svc resource-1nzzsp -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-1nzzsp-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-1nzzsp -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Expose Service\nExpose the deployment `resource-y19sn5-dep` as a Service named `resource-y19sn5` in namespace `test`.\nThe service should listen on port `6574` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-y19sn5 -n test -o jsonpath='{.spec.ports[0].port}' | grep 6574\nkubectl get svc resource-y19sn5 -n test -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-y19sn5-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-y19sn5 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-rjesh8` in namespace `kube-system`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-rjesh8 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-rjesh8 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-rjesh8 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Create a Pod\nCreate a pod named `resource-5vbv89` in namespace `staging` using image `httpd`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-5vbv89 -n staging --no-headers | grep Running\nkubectl get pod resource-5vbv89 -n staging -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-5vbv89 -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-91a63z` in namespace `staging` using image `nginx`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-91a63z -n staging --no-headers | grep Running\nkubectl get pod resource-91a63z -n staging -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-91a63z -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-duk0od-dep` as a Service named `resource-duk0od` in namespace `kube-system`.\nThe service should listen on port `4767` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-duk0od -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4767\nkubectl get svc resource-duk0od -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-duk0od-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-duk0od -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Expose Service\nExpose the deployment `resource-3rvvmd-dep` as a Service named `resource-3rvvmd` in namespace `prod`.\nThe service should listen on port `6624` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-3rvvmd -n prod -o jsonpath='{.spec.ports[0].port}' | grep 6624\nkubectl get svc resource-3rvvmd -n prod -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-3rvvmd-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-3rvvmd -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Scale Deployment\nCreate a deployment named `resource-lv8c6g` in namespace `staging` using image `nginx`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-lv8c6g -n staging -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-lv8c6g -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-lv8c6g -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-wqju5i` in namespace `dev`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-wqju5i -n dev -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-wqju5i -n dev -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-wqju5i -n dev 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-4": {
    "id": "auto-cka-4",
    "title": "Auto CKA Practice (Batch 4)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 4)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-1pxk55` in namespace `frontend` using image `postgres`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-membuf` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9itjti` in namespace `backend`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 4: Scale Deployment\nCreate a deployment named `resource-okzyqi` in namespace `backend` using image `python:3.9`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n# Task 5: Expose Service\nExpose the deployment `resource-vy7z93-dep` as a Service named `resource-vy7z93` in namespace `default`.\nThe service should listen on port `7074` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 6: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 7: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 8: Create a Pod\nCreate a pod named `resource-nqgm2x` in namespace `dev` using image `node:14`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 9: Scale Deployment\nCreate a deployment named `resource-72nw0p` in namespace `kube-system` using image `redis`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 10: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 11: Create a Pod\nCreate a pod named `resource-gtku45` in namespace `backend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lj0cb7` in namespace `frontend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-oew3ge` in namespace `backend`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 14: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ljdj5u` in namespace `kube-system`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-nebszf-dep` as a Service named `resource-nebszf` in namespace `dev`.\nThe service should listen on port `6392` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 17: Create a Pod\nCreate a pod named `resource-b3t235` in namespace `staging` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 18: Expose Service\nExpose the deployment `resource-do65wo-dep` as a Service named `resource-do65wo` in namespace `kube-system`.\nThe service should listen on port `6593` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 19: Create a Pod\nCreate a pod named `resource-1ik0nz` in namespace `dev` using image `postgres`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 20: Create a Pod\nCreate a pod named `resource-au2qec` in namespace `staging` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-1pxk55 -n frontend --no-headers | grep Running\nkubectl get pod resource-1pxk55 -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pvc resource-membuf -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-membuf -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pvc resource-9itjti -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-9itjti -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get deploy resource-okzyqi -n backend -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-okzyqi -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\nkubectl get svc resource-vy7z93 -n default -o jsonpath='{.spec.ports[0].port}' | grep 7074\nkubectl get svc resource-vy7z93 -n default -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-nqgm2x -n dev --no-headers | grep Running\nkubectl get pod resource-nqgm2x -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get deploy resource-72nw0p -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-72nw0p -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-gtku45 -n backend --no-headers | grep Running\nkubectl get pod resource-gtku45 -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get pvc resource-lj0cb7 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-lj0cb7 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-oew3ge -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-oew3ge -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-ljdj5u -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-ljdj5u -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get svc resource-nebszf -n dev -o jsonpath='{.spec.ports[0].port}' | grep 6392\nkubectl get svc resource-nebszf -n dev -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-b3t235 -n staging --no-headers | grep Running\nkubectl get pod resource-b3t235 -n staging -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get svc resource-do65wo -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 6593\nkubectl get svc resource-do65wo -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pod resource-1ik0nz -n dev --no-headers | grep Running\nkubectl get pod resource-1ik0nz -n dev -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pod resource-au2qec -n staging --no-headers | grep Running\nkubectl get pod resource-au2qec -n staging -o jsonpath='{.metadata.labels.release}' | grep stable\n",
    "setupScript": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-1pxk55 -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-membuf -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9itjti -n backend 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-okzyqi -n backend 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vy7z93-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vy7z93 -n default 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-nqgm2x -n dev --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-72nw0p -n kube-system 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-gtku45 -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lj0cb7 -n frontend 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-oew3ge -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ljdj5u -n kube-system 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-nebszf-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-nebszf -n dev 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-b3t235 -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-do65wo-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-do65wo -n kube-system 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-1ik0nz -n dev --force --grace-period=0 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-au2qec -n staging --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 4)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-1pxk55` in namespace `frontend` using image `postgres`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-1pxk55 -n frontend --no-headers | grep Running\nkubectl get pod resource-1pxk55 -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-1pxk55 -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-membuf` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-membuf -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-membuf -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-membuf -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9itjti` in namespace `backend`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-9itjti -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-9itjti -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9itjti -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Scale Deployment\nCreate a deployment named `resource-okzyqi` in namespace `backend` using image `python:3.9`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `python:3.9:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-okzyqi -n backend -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-okzyqi -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"python:3.9:latest\"\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-okzyqi -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Expose Service\nExpose the deployment `resource-vy7z93-dep` as a Service named `resource-vy7z93` in namespace `default`.\nThe service should listen on port `7074` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-vy7z93 -n default -o jsonpath='{.spec.ports[0].port}' | grep 7074\nkubectl get svc resource-vy7z93 -n default -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vy7z93-dep --image=nginx -n default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vy7z93 -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Create a Pod\nCreate a pod named `resource-nqgm2x` in namespace `dev` using image `node:14`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-nqgm2x -n dev --no-headers | grep Running\nkubectl get pod resource-nqgm2x -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-nqgm2x -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Scale Deployment\nCreate a deployment named `resource-72nw0p` in namespace `kube-system` using image `redis`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-72nw0p -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-72nw0p -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-72nw0p -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Create a Pod\nCreate a pod named `resource-gtku45` in namespace `backend` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-gtku45 -n backend --no-headers | grep Running\nkubectl get pod resource-gtku45 -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-gtku45 -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-lj0cb7` in namespace `frontend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-lj0cb7 -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-lj0cb7 -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-lj0cb7 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-oew3ge` in namespace `backend`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-oew3ge -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-oew3ge -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-oew3ge -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-ljdj5u` in namespace `kube-system`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-ljdj5u -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-ljdj5u -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-ljdj5u -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-nebszf-dep` as a Service named `resource-nebszf` in namespace `dev`.\nThe service should listen on port `6392` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-nebszf -n dev -o jsonpath='{.spec.ports[0].port}' | grep 6392\nkubectl get svc resource-nebszf -n dev -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-nebszf-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-nebszf -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Create a Pod\nCreate a pod named `resource-b3t235` in namespace `staging` using image `node:14`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-b3t235 -n staging --no-headers | grep Running\nkubectl get pod resource-b3t235 -n staging -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-b3t235 -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Expose Service\nExpose the deployment `resource-do65wo-dep` as a Service named `resource-do65wo` in namespace `kube-system`.\nThe service should listen on port `6593` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-do65wo -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 6593\nkubectl get svc resource-do65wo -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-do65wo-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-do65wo -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Create a Pod\nCreate a pod named `resource-1ik0nz` in namespace `dev` using image `postgres`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-1ik0nz -n dev --no-headers | grep Running\nkubectl get pod resource-1ik0nz -n dev -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-1ik0nz -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Create a Pod\nCreate a pod named `resource-au2qec` in namespace `staging` using image `busybox`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-au2qec -n staging --no-headers | grep Running\nkubectl get pod resource-au2qec -n staging -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-au2qec -n staging --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-5": {
    "id": "auto-cka-5",
    "title": "Auto CKA Practice (Batch 5)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 5)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Create a Pod\nCreate a pod named `resource-3hw102` in namespace `test` using image `node:14`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 2: Create a Pod\nCreate a pod named `resource-c5be3a` in namespace `prod` using image `mysql`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 3: Expose Service\nExpose the deployment `resource-wd7v93-dep` as a Service named `resource-wd7v93` in namespace `prod`.\nThe service should listen on port `7035` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 4: Scale Deployment\nCreate a deployment named `resource-k0q40k` in namespace `kube-system` using image `node:14`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n# Task 5: Create a Pod\nCreate a pod named `resource-b18g5p` in namespace `test` using image `alpine`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 6: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 7: Expose Service\nExpose the deployment `resource-2kdxrr-dep` as a Service named `resource-2kdxrr` in namespace `kube-system`.\nThe service should listen on port `4461` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 8: Expose Service\nExpose the deployment `resource-lwcs94-dep` as a Service named `resource-lwcs94` in namespace `staging`.\nThe service should listen on port `8454` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 9: Create a Pod\nCreate a pod named `resource-bqlrk3` in namespace `default` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-h4s1au` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 11: Expose Service\nExpose the deployment `resource-l4ntnd-dep` as a Service named `resource-l4ntnd` in namespace `backend`.\nThe service should listen on port `8081` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 12: Scale Deployment\nCreate a deployment named `resource-tczrwd` in namespace `test` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 13: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 14: Scale Deployment\nCreate a deployment named `resource-i0sv7f` in namespace `dev` using image `alpine`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-3lqpsf` in namespace `kube-system` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-3ueal4-dep` as a Service named `resource-3ueal4` in namespace `test`.\nThe service should listen on port `3998` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 17: Scale Deployment\nCreate a deployment named `resource-dcr5gx` in namespace `test` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Create a Pod\nCreate a pod named `resource-t28vg0` in namespace `staging` using image `nginx`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-75a1ty` in namespace `backend`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod resource-3hw102 -n test --no-headers | grep Running\nkubectl get pod resource-3hw102 -n test -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-c5be3a -n prod --no-headers | grep Running\nkubectl get pod resource-c5be3a -n prod -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get svc resource-wd7v93 -n prod -o jsonpath='{.spec.ports[0].port}' | grep 7035\nkubectl get svc resource-wd7v93 -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get deploy resource-k0q40k -n kube-system -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-k0q40k -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\nkubectl get pod resource-b18g5p -n test --no-headers | grep Running\nkubectl get pod resource-b18g5p -n test -o jsonpath='{.metadata.labels.team}' | grep blue\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-2kdxrr -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4461\nkubectl get svc resource-2kdxrr -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get svc resource-lwcs94 -n staging -o jsonpath='{.spec.ports[0].port}' | grep 8454\nkubectl get svc resource-lwcs94 -n staging -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get pod resource-bqlrk3 -n default --no-headers | grep Running\nkubectl get pod resource-bqlrk3 -n default -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pvc resource-h4s1au -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-h4s1au -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get svc resource-l4ntnd -n backend -o jsonpath='{.spec.ports[0].port}' | grep 8081\nkubectl get svc resource-l4ntnd -n backend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-tczrwd -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-tczrwd -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-i0sv7f -n dev -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-i0sv7f -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\nkubectl get pod resource-3lqpsf -n kube-system --no-headers | grep Running\nkubectl get pod resource-3lqpsf -n kube-system -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get svc resource-3ueal4 -n test -o jsonpath='{.spec.ports[0].port}' | grep 3998\nkubectl get svc resource-3ueal4 -n test -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-dcr5gx -n test -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-dcr5gx -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-t28vg0 -n staging --no-headers | grep Running\nkubectl get pod resource-t28vg0 -n staging -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pvc resource-75a1ty -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-75a1ty -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
    "setupScript": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3hw102 -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-c5be3a -n prod --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-wd7v93-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-wd7v93 -n prod 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-k0q40k -n kube-system 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-b18g5p -n test --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-2kdxrr-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-2kdxrr -n kube-system 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-lwcs94-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-lwcs94 -n staging 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-bqlrk3 -n default --force --grace-period=0 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-h4s1au -n prod 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-l4ntnd-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-l4ntnd -n backend 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-tczrwd -n test 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-i0sv7f -n dev 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3lqpsf -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-3ueal4-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-3ueal4 -n test 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-dcr5gx -n test 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-t28vg0 -n staging --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-75a1ty -n backend 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 5)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `resource-3hw102` in namespace `test` using image `node:14`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-3hw102 -n test --no-headers | grep Running\nkubectl get pod resource-3hw102 -n test -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3hw102 -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Create a Pod\nCreate a pod named `resource-c5be3a` in namespace `prod` using image `mysql`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-c5be3a -n prod --no-headers | grep Running\nkubectl get pod resource-c5be3a -n prod -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-c5be3a -n prod --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Expose Service\nExpose the deployment `resource-wd7v93-dep` as a Service named `resource-wd7v93` in namespace `prod`.\nThe service should listen on port `7035` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-wd7v93 -n prod -o jsonpath='{.spec.ports[0].port}' | grep 7035\nkubectl get svc resource-wd7v93 -n prod -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-wd7v93-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-wd7v93 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Scale Deployment\nCreate a deployment named `resource-k0q40k` in namespace `kube-system` using image `node:14`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-k0q40k -n kube-system -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-k0q40k -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-k0q40k -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Create a Pod\nCreate a pod named `resource-b18g5p` in namespace `test` using image `alpine`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-b18g5p -n test --no-headers | grep Running\nkubectl get pod resource-b18g5p -n test -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-b18g5p -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Expose Service\nExpose the deployment `resource-2kdxrr-dep` as a Service named `resource-2kdxrr` in namespace `kube-system`.\nThe service should listen on port `4461` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-2kdxrr -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 4461\nkubectl get svc resource-2kdxrr -n kube-system -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-2kdxrr-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-2kdxrr -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Expose Service\nExpose the deployment `resource-lwcs94-dep` as a Service named `resource-lwcs94` in namespace `staging`.\nThe service should listen on port `8454` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-lwcs94 -n staging -o jsonpath='{.spec.ports[0].port}' | grep 8454\nkubectl get svc resource-lwcs94 -n staging -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-lwcs94-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-lwcs94 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Create a Pod\nCreate a pod named `resource-bqlrk3` in namespace `default` using image `httpd`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-bqlrk3 -n default --no-headers | grep Running\nkubectl get pod resource-bqlrk3 -n default -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-bqlrk3 -n default --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-h4s1au` in namespace `prod`.\nRequest `100Mi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-h4s1au -n prod -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-h4s1au -n prod -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-h4s1au -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Expose Service\nExpose the deployment `resource-l4ntnd-dep` as a Service named `resource-l4ntnd` in namespace `backend`.\nThe service should listen on port `8081` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-l4ntnd -n backend -o jsonpath='{.spec.ports[0].port}' | grep 8081\nkubectl get svc resource-l4ntnd -n backend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-l4ntnd-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-l4ntnd -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Scale Deployment\nCreate a deployment named `resource-tczrwd` in namespace `test` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-tczrwd -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-tczrwd -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-tczrwd -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Scale Deployment\nCreate a deployment named `resource-i0sv7f` in namespace `dev` using image `alpine`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `alpine:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-i0sv7f -n dev -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-i0sv7f -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"alpine:latest\"\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-i0sv7f -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-3lqpsf` in namespace `kube-system` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-3lqpsf -n kube-system --no-headers | grep Running\nkubectl get pod resource-3lqpsf -n kube-system -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-3lqpsf -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-3ueal4-dep` as a Service named `resource-3ueal4` in namespace `test`.\nThe service should listen on port `3998` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-3ueal4 -n test -o jsonpath='{.spec.ports[0].port}' | grep 3998\nkubectl get svc resource-3ueal4 -n test -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-3ueal4-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-3ueal4 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Scale Deployment\nCreate a deployment named `resource-dcr5gx` in namespace `test` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-dcr5gx -n test -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-dcr5gx -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-dcr5gx -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Create a Pod\nCreate a pod named `resource-t28vg0` in namespace `staging` using image `nginx`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-t28vg0 -n staging --no-headers | grep Running\nkubectl get pod resource-t28vg0 -n staging -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-t28vg0 -n staging --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-75a1ty` in namespace `backend`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-75a1ty -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-75a1ty -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-75a1ty -n backend 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-6": {
    "id": "auto-cka-6",
    "title": "Auto CKA Practice (Batch 6)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 6)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Expose Service\nExpose the deployment `resource-c029o1-dep` as a Service named `resource-c029o1` in namespace `prod`.\nThe service should listen on port `7059` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 2: Expose Service\nExpose the deployment `resource-la0qh2-dep` as a Service named `resource-la0qh2` in namespace `frontend`.\nThe service should listen on port `8640` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9wb1q1` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 4: Scale Deployment\nCreate a deployment named `resource-o4pnf9` in namespace `staging` using image `memcached`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n# Task 5: Expose Service\nExpose the deployment `resource-p8t35u-dep` as a Service named `resource-p8t35u` in namespace `dev`.\nThe service should listen on port `7733` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 6: Scale Deployment\nCreate a deployment named `resource-wv998l` in namespace `default` using image `node:14`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n# Task 7: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-hezmww` in namespace `staging`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-vqzo3s` in namespace `backend`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 9: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 10: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 11: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cb6wxk` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-w5r1ed` in namespace `backend`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-1dxcca` in namespace `frontend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 14: Expose Service\nExpose the deployment `resource-9pamkd-dep` as a Service named `resource-9pamkd` in namespace `dev`.\nThe service should listen on port `7388` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 15: Scale Deployment\nCreate a deployment named `resource-hdpu1x` in namespace `default` using image `nginx`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 16: Scale Deployment\nCreate a deployment named `resource-a774y7` in namespace `frontend` using image `httpd`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n# Task 17: Scale Deployment\nCreate a deployment named `resource-c0fiso` in namespace `test` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 18: Scale Deployment\nCreate a deployment named `resource-gusota` in namespace `default` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-swuxj5` in namespace `test`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 20: Create a Pod\nCreate a pod named `resource-dask0p` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get svc resource-c029o1 -n prod -o jsonpath='{.spec.ports[0].port}' | grep 7059\nkubectl get svc resource-c029o1 -n prod -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get svc resource-la0qh2 -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 8640\nkubectl get svc resource-la0qh2 -n frontend -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pvc resource-9wb1q1 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-9wb1q1 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get deploy resource-o4pnf9 -n staging -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-o4pnf9 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\nkubectl get svc resource-p8t35u -n dev -o jsonpath='{.spec.ports[0].port}' | grep 7733\nkubectl get svc resource-p8t35u -n dev -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get deploy resource-wv998l -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-wv998l -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\nkubectl get pvc resource-hezmww -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-hezmww -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pvc resource-vqzo3s -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-vqzo3s -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-cb6wxk -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-cb6wxk -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get pvc resource-w5r1ed -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-w5r1ed -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\nkubectl get pvc resource-1dxcca -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-1dxcca -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get svc resource-9pamkd -n dev -o jsonpath='{.spec.ports[0].port}' | grep 7388\nkubectl get svc resource-9pamkd -n dev -o jsonpath='{.spec.type}' | grep ClusterIP\nkubectl get deploy resource-hdpu1x -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-hdpu1x -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get deploy resource-a774y7 -n frontend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-a774y7 -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\nkubectl get deploy resource-c0fiso -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-c0fiso -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get deploy resource-gusota -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-gusota -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\nkubectl get pvc resource-swuxj5 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-swuxj5 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pod resource-dask0p -n kube-system --no-headers | grep Running\nkubectl get pod resource-dask0p -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\n",
    "setupScript": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-c029o1-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-c029o1 -n prod 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-la0qh2-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-la0qh2 -n frontend 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9wb1q1 -n kube-system 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-o4pnf9 -n staging 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-p8t35u-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-p8t35u -n dev 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-wv998l -n default 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-hezmww -n staging 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-vqzo3s -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cb6wxk -n test 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-w5r1ed -n backend 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-1dxcca -n frontend 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-9pamkd-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-9pamkd -n dev 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-hdpu1x -n default 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-a774y7 -n frontend 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-c0fiso -n test 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-gusota -n default 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-swuxj5 -n test 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-dask0p -n kube-system --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 6)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Expose Service\nExpose the deployment `resource-c029o1-dep` as a Service named `resource-c029o1` in namespace `prod`.\nThe service should listen on port `7059` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-c029o1 -n prod -o jsonpath='{.spec.ports[0].port}' | grep 7059\nkubectl get svc resource-c029o1 -n prod -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-c029o1-dep --image=nginx -n prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-c029o1 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Expose Service\nExpose the deployment `resource-la0qh2-dep` as a Service named `resource-la0qh2` in namespace `frontend`.\nThe service should listen on port `8640` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-la0qh2 -n frontend -o jsonpath='{.spec.ports[0].port}' | grep 8640\nkubectl get svc resource-la0qh2 -n frontend -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-la0qh2-dep --image=nginx -n frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-la0qh2 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-9wb1q1` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-9wb1q1 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-9wb1q1 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-9wb1q1 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Scale Deployment\nCreate a deployment named `resource-o4pnf9` in namespace `staging` using image `memcached`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-o4pnf9 -n staging -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-o4pnf9 -n staging -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-o4pnf9 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Expose Service\nExpose the deployment `resource-p8t35u-dep` as a Service named `resource-p8t35u` in namespace `dev`.\nThe service should listen on port `7733` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-p8t35u -n dev -o jsonpath='{.spec.ports[0].port}' | grep 7733\nkubectl get svc resource-p8t35u -n dev -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-p8t35u-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-p8t35u -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Scale Deployment\nCreate a deployment named `resource-wv998l` in namespace `default` using image `node:14`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-wv998l -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-wv998l -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-wv998l -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-hezmww` in namespace `staging`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-hezmww -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-hezmww -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-hezmww -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-vqzo3s` in namespace `backend`.\nRequest `2Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-vqzo3s -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-vqzo3s -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-vqzo3s -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cb6wxk` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-cb6wxk -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-cb6wxk -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cb6wxk -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-w5r1ed` in namespace `backend`.\nRequest `1Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-w5r1ed -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-w5r1ed -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-w5r1ed -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-1dxcca` in namespace `frontend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-1dxcca -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-1dxcca -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-1dxcca -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Expose Service\nExpose the deployment `resource-9pamkd-dep` as a Service named `resource-9pamkd` in namespace `dev`.\nThe service should listen on port `7388` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-9pamkd -n dev -o jsonpath='{.spec.ports[0].port}' | grep 7388\nkubectl get svc resource-9pamkd -n dev -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-9pamkd-dep --image=nginx -n dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-9pamkd -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Scale Deployment\nCreate a deployment named `resource-hdpu1x` in namespace `default` using image `nginx`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-hdpu1x -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-hdpu1x -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-hdpu1x -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Scale Deployment\nCreate a deployment named `resource-a774y7` in namespace `frontend` using image `httpd`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-a774y7 -n frontend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-a774y7 -n frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-a774y7 -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Scale Deployment\nCreate a deployment named `resource-c0fiso` in namespace `test` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-c0fiso -n test -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-c0fiso -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-c0fiso -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Scale Deployment\nCreate a deployment named `resource-gusota` in namespace `default` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-gusota -n default -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-gusota -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-gusota -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-swuxj5` in namespace `test`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-swuxj5 -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-swuxj5 -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-swuxj5 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Create a Pod\nCreate a pod named `resource-dask0p` in namespace `kube-system` using image `nginx`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-dask0p -n kube-system --no-headers | grep Running\nkubectl get pod resource-dask0p -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-dask0p -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-7": {
    "id": "auto-cka-7",
    "title": "Auto CKA Practice (Batch 7)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 7)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-f44cos` in namespace `staging`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 2: Create a Pod\nCreate a pod named `resource-0u1gl0` in namespace `kube-system` using image `postgres`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 3: Create a Pod\nCreate a pod named `resource-ux3m1i` in namespace `default` using image `alpine`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 4: Scale Deployment\nCreate a deployment named `resource-lu9cfd` in namespace `default` using image `busybox`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n# Task 5: Create a Pod\nCreate a pod named `resource-cpgy5x` in namespace `frontend` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 6: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 7: Expose Service\nExpose the deployment `resource-vaot9i-dep` as a Service named `resource-vaot9i` in namespace `kube-system`.\nThe service should listen on port `6480` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 8: Create a Pod\nCreate a pod named `resource-9pplpw` in namespace `frontend` using image `redis`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 9: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-hhitcs` in namespace `default`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 11: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 12: Create a Pod\nCreate a pod named `resource-k9fn9n` in namespace `kube-system` using image `redis`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 13: Create a Pod\nCreate a pod named `resource-tjnqvr` in namespace `default` using image `httpd`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 14: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-806yt7` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-mv9str` in namespace `backend` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n# Task 16: Expose Service\nExpose the deployment `resource-cqas1z-dep` as a Service named `resource-cqas1z` in namespace `test`.\nThe service should listen on port `4942` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 17: Create a Pod\nCreate a pod named `resource-6w3l8c` in namespace `backend` using image `node:14`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 18: Create a Pod\nCreate a pod named `resource-eukkd7` in namespace `frontend` using image `redis`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n# Task 19: Create a Pod\nCreate a pod named `resource-m8bhra` in namespace `backend` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 20: Scale Deployment\nCreate a deployment named `resource-2s08q5` in namespace `default` using image `mysql`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get pvc resource-f44cos -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-f44cos -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pod resource-0u1gl0 -n kube-system --no-headers | grep Running\nkubectl get pod resource-0u1gl0 -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-ux3m1i -n default --no-headers | grep Running\nkubectl get pod resource-ux3m1i -n default -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get deploy resource-lu9cfd -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-lu9cfd -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\nkubectl get pod resource-cpgy5x -n frontend --no-headers | grep Running\nkubectl get pod resource-cpgy5x -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-vaot9i -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 6480\nkubectl get svc resource-vaot9i -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pod resource-9pplpw -n frontend --no-headers | grep Running\nkubectl get pod resource-9pplpw -n frontend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-hhitcs -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-hhitcs -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-k9fn9n -n kube-system --no-headers | grep Running\nkubectl get pod resource-k9fn9n -n kube-system -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pod resource-tjnqvr -n default --no-headers | grep Running\nkubectl get pod resource-tjnqvr -n default -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pvc resource-806yt7 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-806yt7 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pod resource-mv9str -n backend --no-headers | grep Running\nkubectl get pod resource-mv9str -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\nkubectl get svc resource-cqas1z -n test -o jsonpath='{.spec.ports[0].port}' | grep 4942\nkubectl get svc resource-cqas1z -n test -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pod resource-6w3l8c -n backend --no-headers | grep Running\nkubectl get pod resource-6w3l8c -n backend -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pod resource-eukkd7 -n frontend --no-headers | grep Running\nkubectl get pod resource-eukkd7 -n frontend -o jsonpath='{.metadata.labels.app}' | grep web\nkubectl get pod resource-m8bhra -n backend --no-headers | grep Running\nkubectl get pod resource-m8bhra -n backend -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get deploy resource-2s08q5 -n default -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-2s08q5 -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\n",
    "setupScript": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-f44cos -n staging 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0u1gl0 -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ux3m1i -n default --force --grace-period=0 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-lu9cfd -n default 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-cpgy5x -n frontend --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vaot9i-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vaot9i -n kube-system 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-9pplpw -n frontend --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-hhitcs -n default 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-k9fn9n -n kube-system --force --grace-period=0 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-tjnqvr -n default --force --grace-period=0 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-806yt7 -n kube-system 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-mv9str -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-cqas1z-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-cqas1z -n test 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-6w3l8c -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-eukkd7 -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-m8bhra -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-2s08q5 -n default 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 7)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-f44cos` in namespace `staging`.\nRequest `2Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-f44cos -n staging -o jsonpath='{.spec.resources.requests.storage}' | grep 2Gi\nkubectl get pvc resource-f44cos -n staging -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-f44cos -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Create a Pod\nCreate a pod named `resource-0u1gl0` in namespace `kube-system` using image `postgres`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-0u1gl0 -n kube-system --no-headers | grep Running\nkubectl get pod resource-0u1gl0 -n kube-system -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-0u1gl0 -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Create a Pod\nCreate a pod named `resource-ux3m1i` in namespace `default` using image `alpine`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-ux3m1i -n default --no-headers | grep Running\nkubectl get pod resource-ux3m1i -n default -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-ux3m1i -n default --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Scale Deployment\nCreate a deployment named `resource-lu9cfd` in namespace `default` using image `busybox`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-lu9cfd -n default -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-lu9cfd -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-lu9cfd -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Create a Pod\nCreate a pod named `resource-cpgy5x` in namespace `frontend` using image `memcached`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-cpgy5x -n frontend --no-headers | grep Running\nkubectl get pod resource-cpgy5x -n frontend -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-cpgy5x -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Expose Service\nExpose the deployment `resource-vaot9i-dep` as a Service named `resource-vaot9i` in namespace `kube-system`.\nThe service should listen on port `6480` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-vaot9i -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 6480\nkubectl get svc resource-vaot9i -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vaot9i-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vaot9i -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Create a Pod\nCreate a pod named `resource-9pplpw` in namespace `frontend` using image `redis`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-9pplpw -n frontend --no-headers | grep Running\nkubectl get pod resource-9pplpw -n frontend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-9pplpw -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-hhitcs` in namespace `default`.\nRequest `100Mi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-hhitcs -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 100Mi\nkubectl get pvc resource-hhitcs -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-hhitcs -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Create a Pod\nCreate a pod named `resource-k9fn9n` in namespace `kube-system` using image `redis`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-k9fn9n -n kube-system --no-headers | grep Running\nkubectl get pod resource-k9fn9n -n kube-system -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-k9fn9n -n kube-system --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Create a Pod\nCreate a pod named `resource-tjnqvr` in namespace `default` using image `httpd`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-tjnqvr -n default --no-headers | grep Running\nkubectl get pod resource-tjnqvr -n default -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-tjnqvr -n default --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-806yt7` in namespace `kube-system`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-806yt7 -n kube-system -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-806yt7 -n kube-system -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-806yt7 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-mv9str` in namespace `backend` using image `memcached`.\nEnsure it has a label `env=prod`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-mv9str -n backend --no-headers | grep Running\nkubectl get pod resource-mv9str -n backend -o jsonpath='{.metadata.labels.env}' | grep prod\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-mv9str -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Expose Service\nExpose the deployment `resource-cqas1z-dep` as a Service named `resource-cqas1z` in namespace `test`.\nThe service should listen on port `4942` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-cqas1z -n test -o jsonpath='{.spec.ports[0].port}' | grep 4942\nkubectl get svc resource-cqas1z -n test -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-cqas1z-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-cqas1z -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Create a Pod\nCreate a pod named `resource-6w3l8c` in namespace `backend` using image `node:14`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-6w3l8c -n backend --no-headers | grep Running\nkubectl get pod resource-6w3l8c -n backend -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-6w3l8c -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Create a Pod\nCreate a pod named `resource-eukkd7` in namespace `frontend` using image `redis`.\nEnsure it has a label `app=web`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-eukkd7 -n frontend --no-headers | grep Running\nkubectl get pod resource-eukkd7 -n frontend -o jsonpath='{.metadata.labels.app}' | grep web\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-eukkd7 -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Create a Pod\nCreate a pod named `resource-m8bhra` in namespace `backend` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-m8bhra -n backend --no-headers | grep Running\nkubectl get pod resource-m8bhra -n backend -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-m8bhra -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Scale Deployment\nCreate a deployment named `resource-2s08q5` in namespace `default` using image `mysql`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-2s08q5 -n default -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-2s08q5 -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-2s08q5 -n default 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-8": {
    "id": "auto-cka-8",
    "title": "Auto CKA Practice (Batch 8)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 8)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Expose Service\nExpose the deployment `resource-0fuqy0-dep` as a Service named `resource-0fuqy0` in namespace `kube-system`.\nThe service should listen on port `7325` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 2: Create a Pod\nCreate a pod named `resource-4i24o3` in namespace `backend` using image `nginx`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 3: Create a Pod\nCreate a pod named `resource-a2jppl` in namespace `frontend` using image `alpine`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 4: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 5: Expose Service\nExpose the deployment `resource-q7zeva-dep` as a Service named `resource-q7zeva` in namespace `test`.\nThe service should listen on port `6053` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 6: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 7: Scale Deployment\nCreate a deployment named `resource-q4t8v2` in namespace `backend` using image `busybox`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n# Task 8: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 9: Create a Pod\nCreate a pod named `resource-o4huvl` in namespace `dev` using image `nginx`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n# Task 10: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 11: Scale Deployment\nCreate a deployment named `resource-cmgeyr` in namespace `kube-system` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n# Task 12: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-sqrtw5` in namespace `default`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 14: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 15: Scale Deployment\nCreate a deployment named `resource-2be1iw` in namespace `dev` using image `redis`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n# Task 16: Scale Deployment\nCreate a deployment named `resource-oo21gp` in namespace `default` using image `busybox`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n# Task 17: Scale Deployment\nCreate a deployment named `resource-1g8pnf` in namespace `prod` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-j5xnwx` in namespace `frontend`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 20: Create a Pod\nCreate a pod named `resource-zbx9vj` in namespace `staging` using image `node:14`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n",
    "verifyScript": "kubectl get svc resource-0fuqy0 -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7325\nkubectl get svc resource-0fuqy0 -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\nkubectl get pod resource-4i24o3 -n backend --no-headers | grep Running\nkubectl get pod resource-4i24o3 -n backend -o jsonpath='{.metadata.labels.team}' | grep blue\nkubectl get pod resource-a2jppl -n frontend --no-headers | grep Running\nkubectl get pod resource-a2jppl -n frontend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-q7zeva -n test -o jsonpath='{.spec.ports[0].port}' | grep 6053\nkubectl get svc resource-q7zeva -n test -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-q4t8v2 -n backend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-q4t8v2 -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-o4huvl -n dev --no-headers | grep Running\nkubectl get pod resource-o4huvl -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-cmgeyr -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-cmgeyr -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-sqrtw5 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-sqrtw5 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-2be1iw -n dev -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-2be1iw -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\nkubectl get deploy resource-oo21gp -n default -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-oo21gp -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\nkubectl get deploy resource-1g8pnf -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-1g8pnf -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-j5xnwx -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-j5xnwx -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get pod resource-zbx9vj -n staging --no-headers | grep Running\nkubectl get pod resource-zbx9vj -n staging -o jsonpath='{.metadata.labels.release}' | grep stable\n",
    "setupScript": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-0fuqy0-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-0fuqy0 -n kube-system 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4i24o3 -n backend --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-a2jppl -n frontend --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-q7zeva-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-q7zeva -n test 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-q4t8v2 -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-o4huvl -n dev --force --grace-period=0 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-cmgeyr -n kube-system 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-sqrtw5 -n default 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-2be1iw -n dev 2>/dev/null || true\nkubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-oo21gp -n default 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-1g8pnf -n prod 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-j5xnwx -n frontend 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-zbx9vj -n staging --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 8)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Expose Service\nExpose the deployment `resource-0fuqy0-dep` as a Service named `resource-0fuqy0` in namespace `kube-system`.\nThe service should listen on port `7325` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-0fuqy0 -n kube-system -o jsonpath='{.spec.ports[0].port}' | grep 7325\nkubectl get svc resource-0fuqy0 -n kube-system -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-0fuqy0-dep --image=nginx -n kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-0fuqy0 -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Create a Pod\nCreate a pod named `resource-4i24o3` in namespace `backend` using image `nginx`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-4i24o3 -n backend --no-headers | grep Running\nkubectl get pod resource-4i24o3 -n backend -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-4i24o3 -n backend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Create a Pod\nCreate a pod named `resource-a2jppl` in namespace `frontend` using image `alpine`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-a2jppl -n frontend --no-headers | grep Running\nkubectl get pod resource-a2jppl -n frontend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-a2jppl -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Expose Service\nExpose the deployment `resource-q7zeva-dep` as a Service named `resource-q7zeva` in namespace `test`.\nThe service should listen on port `6053` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-q7zeva -n test -o jsonpath='{.spec.ports[0].port}' | grep 6053\nkubectl get svc resource-q7zeva -n test -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-q7zeva-dep --image=nginx -n test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-q7zeva -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Scale Deployment\nCreate a deployment named `resource-q4t8v2` in namespace `backend` using image `busybox`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-q4t8v2 -n backend -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-q4t8v2 -n backend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-q4t8v2 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Create a Pod\nCreate a pod named `resource-o4huvl` in namespace `dev` using image `nginx`.\nEnsure it has a label `team=blue`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-o4huvl -n dev --no-headers | grep Running\nkubectl get pod resource-o4huvl -n dev -o jsonpath='{.metadata.labels.team}' | grep blue\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-o4huvl -n dev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Scale Deployment\nCreate a deployment named `resource-cmgeyr` in namespace `kube-system` using image `postgres`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `postgres:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-cmgeyr -n kube-system -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-cmgeyr -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"postgres:latest\"\n",
        "setup": "kubectl create ns kube-system --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-cmgeyr -n kube-system 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-sqrtw5` in namespace `default`.\nRequest `5Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-sqrtw5 -n default -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-sqrtw5 -n default -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-sqrtw5 -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Scale Deployment\nCreate a deployment named `resource-2be1iw` in namespace `dev` using image `redis`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `redis:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-2be1iw -n dev -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-2be1iw -n dev -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"redis:latest\"\n",
        "setup": "kubectl create ns dev --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-2be1iw -n dev 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Scale Deployment\nCreate a deployment named `resource-oo21gp` in namespace `default` using image `busybox`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `busybox:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-oo21gp -n default -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-oo21gp -n default -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"busybox:latest\"\n",
        "setup": "kubectl create ns default --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-oo21gp -n default 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Scale Deployment\nCreate a deployment named `resource-1g8pnf` in namespace `prod` using image `nginx`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `nginx:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-1g8pnf -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-1g8pnf -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"nginx:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-1g8pnf -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-j5xnwx` in namespace `frontend`.\nRequest `10Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-j5xnwx -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-j5xnwx -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-j5xnwx -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Create a Pod\nCreate a pod named `resource-zbx9vj` in namespace `staging` using image `node:14`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-zbx9vj -n staging --no-headers | grep Running\nkubectl get pod resource-zbx9vj -n staging -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-zbx9vj -n staging --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cka-9": {
    "id": "auto-cka-9",
    "title": "Auto CKA Practice (Batch 9)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Auto CKA Practice (Batch 9)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-d26alj` in namespace `backend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 3: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 4: Scale Deployment\nCreate a deployment named `resource-7suds0` in namespace `test` using image `node:14`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n# Task 5: Scale Deployment\nCreate a deployment named `resource-jqliq6` in namespace `test` using image `mysql`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n# Task 6: Expose Service\nExpose the deployment `resource-my5wc5-dep` as a Service named `resource-my5wc5` in namespace `staging`.\nThe service should listen on port `7981` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 7: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 8: Expose Service\nExpose the deployment `resource-dnzvl8-dep` as a Service named `resource-dnzvl8` in namespace `backend`.\nThe service should listen on port `3310` and be of type `ClusterIP`.\n\n\n\n\n\n\n# Task 9: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 10: Create a Pod\nCreate a pod named `resource-25mrmk` in namespace `frontend` using image `python:3.9`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n# Task 11: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-nycvdu` in namespace `frontend`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n# Task 12: Scale Deployment\nCreate a deployment named `resource-pvbju5` in namespace `prod` using image `node:14`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n# Task 13: Expose Service\nExpose the deployment `resource-vzc30q-dep` as a Service named `resource-vzc30q` in namespace `staging`.\nThe service should listen on port `6032` and be of type `NodePort`.\n\n\n\n\n\n\n# Task 14: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 15: Create a Pod\nCreate a pod named `resource-xd0jmi` in namespace `test` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n# Task 16: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cgwstf` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-vc1bfu` in namespace `frontend`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n# Task 18: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n# Task 19: Scale Deployment\nCreate a deployment named `resource-geffq8` in namespace `prod` using image `memcached`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n# Task 20: Scale Deployment\nCreate a deployment named `resource-er52ib` in namespace `prod` using image `httpd`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n",
    "verifyScript": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pvc resource-d26alj -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-d26alj -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-7suds0 -n test -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-7suds0 -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\nkubectl get deploy resource-jqliq6 -n test -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-jqliq6 -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\nkubectl get svc resource-my5wc5 -n staging -o jsonpath='{.spec.ports[0].port}' | grep 7981\nkubectl get svc resource-my5wc5 -n staging -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get svc resource-dnzvl8 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 3310\nkubectl get svc resource-dnzvl8 -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-25mrmk -n frontend --no-headers | grep Running\nkubectl get pod resource-25mrmk -n frontend -o jsonpath='{.metadata.labels.tier}' | grep frontend\nkubectl get pvc resource-nycvdu -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-nycvdu -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\nkubectl get deploy resource-pvbju5 -n prod -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-pvbju5 -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\nkubectl get svc resource-vzc30q -n staging -o jsonpath='{.spec.ports[0].port}' | grep 6032\nkubectl get svc resource-vzc30q -n staging -o jsonpath='{.spec.type}' | grep NodePort\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get pod resource-xd0jmi -n test --no-headers | grep Running\nkubectl get pod resource-xd0jmi -n test -o jsonpath='{.metadata.labels.release}' | grep stable\nkubectl get pvc resource-cgwstf -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-cgwstf -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\nkubectl get pvc resource-vc1bfu -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-vc1bfu -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\nkubectl get deploy resource-geffq8 -n prod -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-geffq8 -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\nkubectl get deploy resource-er52ib -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-er52ib -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\n",
    "setupScript": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-d26alj -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-7suds0 -n test 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-jqliq6 -n test 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-my5wc5-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-my5wc5 -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\nkubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-dnzvl8-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-dnzvl8 -n backend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-25mrmk -n frontend --force --grace-period=0 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-nycvdu -n frontend 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-pvbju5 -n prod 2>/dev/null || true\nkubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vzc30q-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vzc30q -n staging 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-xd0jmi -n test --force --grace-period=0 2>/dev/null || true\nkubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cgwstf -n test 2>/dev/null || true\nkubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-vc1bfu -n frontend 2>/dev/null || true\n# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-geffq8 -n prod 2>/dev/null || true\nkubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-er52ib -n prod 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKA Practice (Batch 9)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-d26alj` in namespace `backend`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-d26alj -n backend -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-d26alj -n backend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-d26alj -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Node Maintenance\nMark node `node-1` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-1 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Scale Deployment\nCreate a deployment named `resource-7suds0` in namespace `test` using image `node:14`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-7suds0 -n test -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-7suds0 -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-7suds0 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Scale Deployment\nCreate a deployment named `resource-jqliq6` in namespace `test` using image `mysql`.\nScale it to `5` replicas.\nThen, perform a rolling update to image `mysql:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-jqliq6 -n test -o jsonpath='{.spec.replicas}' | grep 5\nkubectl get deploy resource-jqliq6 -n test -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"mysql:latest\"\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-jqliq6 -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Expose Service\nExpose the deployment `resource-my5wc5-dep` as a Service named `resource-my5wc5` in namespace `staging`.\nThe service should listen on port `7981` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-my5wc5 -n staging -o jsonpath='{.spec.ports[0].port}' | grep 7981\nkubectl get svc resource-my5wc5 -n staging -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-my5wc5-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-my5wc5 -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Node Maintenance\nMark node `node-5` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-5 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Expose Service\nExpose the deployment `resource-dnzvl8-dep` as a Service named `resource-dnzvl8` in namespace `backend`.\nThe service should listen on port `3310` and be of type `ClusterIP`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-dnzvl8 -n backend -o jsonpath='{.spec.ports[0].port}' | grep 3310\nkubectl get svc resource-dnzvl8 -n backend -o jsonpath='{.spec.type}' | grep ClusterIP\n",
        "setup": "kubectl create ns backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-dnzvl8-dep --image=nginx -n backend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-dnzvl8 -n backend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Node Maintenance\nMark node `node-3` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-3 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Create a Pod\nCreate a pod named `resource-25mrmk` in namespace `frontend` using image `python:3.9`.\nEnsure it has a label `tier=frontend`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-25mrmk -n frontend --no-headers | grep Running\nkubectl get pod resource-25mrmk -n frontend -o jsonpath='{.metadata.labels.tier}' | grep frontend\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-25mrmk -n frontend --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-nycvdu` in namespace `frontend`.\nRequest `5Gi` storage with access mode `ReadOnlyMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-nycvdu -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 5Gi\nkubectl get pvc resource-nycvdu -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadOnlyMany\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-nycvdu -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Scale Deployment\nCreate a deployment named `resource-pvbju5` in namespace `prod` using image `node:14`.\nScale it to `4` replicas.\nThen, perform a rolling update to image `node:14:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-pvbju5 -n prod -o jsonpath='{.spec.replicas}' | grep 4\nkubectl get deploy resource-pvbju5 -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"node:14:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-pvbju5 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Expose Service\nExpose the deployment `resource-vzc30q-dep` as a Service named `resource-vzc30q` in namespace `staging`.\nThe service should listen on port `6032` and be of type `NodePort`.\n\n\n\n\n\n\n",
        "verify": "kubectl get svc resource-vzc30q -n staging -o jsonpath='{.spec.ports[0].port}' | grep 6032\nkubectl get svc resource-vzc30q -n staging -o jsonpath='{.spec.type}' | grep NodePort\n",
        "setup": "kubectl create ns staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl create deployment resource-vzc30q-dep --image=nginx -n staging --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete svc resource-vzc30q -n staging 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Node Maintenance\nMark node `node-4` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-4 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Create a Pod\nCreate a pod named `resource-xd0jmi` in namespace `test` using image `postgres`.\nEnsure it has a label `release=stable`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pod resource-xd0jmi -n test --no-headers | grep Running\nkubectl get pod resource-xd0jmi -n test -o jsonpath='{.metadata.labels.release}' | grep stable\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\n# Clean up if exists\nkubectl delete pod resource-xd0jmi -n test --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-cgwstf` in namespace `test`.\nRequest `1Gi` storage with access mode `ReadWriteMany`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-cgwstf -n test -o jsonpath='{.spec.resources.requests.storage}' | grep 1Gi\nkubectl get pvc resource-cgwstf -n test -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteMany\n",
        "setup": "kubectl create ns test --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-cgwstf -n test 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Persistent Volume Claim\nCreate a PersistentVolumeClaim named `resource-vc1bfu` in namespace `frontend`.\nRequest `10Gi` storage with access mode `ReadWriteOnce`.\n\n\n\n\n\n\n",
        "verify": "kubectl get pvc resource-vc1bfu -n frontend -o jsonpath='{.spec.resources.requests.storage}' | grep 10Gi\nkubectl get pvc resource-vc1bfu -n frontend -o jsonpath='{.spec.accessModes[0]}' | grep ReadWriteOnce\n",
        "setup": "kubectl create ns frontend --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete pvc resource-vc1bfu -n frontend 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Node Maintenance\nMark node `node-2` as unschedulable (cordon).\nThen drain the node, ignoring daemonsets.\nFinally, uncordon the node.\n\n\n\n\n\n\n",
        "verify": "# Check if node exists and is ready (was uncordoned)\nkubectl get node node-2 --no-headers | grep Ready | grep -v SchedulingDisabled\n",
        "setup": "# Ensure node is uncordoned first\nkubectl uncordon node-2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Scale Deployment\nCreate a deployment named `resource-geffq8` in namespace `prod` using image `memcached`.\nScale it to `3` replicas.\nThen, perform a rolling update to image `memcached:latest`.\n\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-geffq8 -n prod -o jsonpath='{.spec.replicas}' | grep 3\nkubectl get deploy resource-geffq8 -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"memcached:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-geffq8 -n prod 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Scale Deployment\nCreate a deployment named `resource-er52ib` in namespace `prod` using image `httpd`.\nScale it to `2` replicas.\nThen, perform a rolling update to image `httpd:latest`.\n\n\n\n\n\n",
        "verify": "kubectl get deploy resource-er52ib -n prod -o jsonpath='{.spec.replicas}' | grep 2\nkubectl get deploy resource-er52ib -n prod -o jsonpath='{.spec.template.spec.containers[0].image}' | grep \"httpd:latest\"\n",
        "setup": "kubectl create ns prod --dry-run=client -o yaml | kubectl apply -f -\nkubectl delete deploy resource-er52ib -n prod 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-1": {
    "id": "auto-ckad-1",
    "title": "Auto CKAD Practice (Batch 1)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 1)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-kzsddk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-5lxxka` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-eto8nn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-2wlzze` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-axkm4t` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-nuedrg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-aw7nie` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-76gtdl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-8w6anr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-44eqqx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-b35iql` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-wr57ja` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-090iwo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-3odqp1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-801vvq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-x6fm3e` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-ffgs38` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-clkirr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-5ussr6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-y3dl6z` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-kzsddk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kzsddk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-5lxxka -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5lxxka -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-eto8nn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-eto8nn -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2wlzze -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2wlzze -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-axkm4t -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-axkm4t -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-nuedrg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nuedrg -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-aw7nie -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-aw7nie -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-76gtdl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-76gtdl -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8w6anr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8w6anr -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-44eqqx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-44eqqx -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-b35iql -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-b35iql -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-wr57ja -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wr57ja -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-090iwo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-090iwo -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3odqp1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3odqp1 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-801vvq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-801vvq -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-x6fm3e -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x6fm3e -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ffgs38 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ffgs38 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-clkirr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-clkirr -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-5ussr6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5ussr6 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-y3dl6z -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y3dl6z -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-kzsddk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-5lxxka --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-eto8nn --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2wlzze --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-axkm4t --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-nuedrg --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-aw7nie --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-76gtdl --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8w6anr --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-44eqqx --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-b35iql --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-wr57ja --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-090iwo --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3odqp1 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-801vvq --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-x6fm3e --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ffgs38 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-clkirr --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-5ussr6 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-y3dl6z --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 1)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-kzsddk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kzsddk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kzsddk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kzsddk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-5lxxka` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-5lxxka -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5lxxka -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-5lxxka --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-eto8nn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-eto8nn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-eto8nn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-eto8nn --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-2wlzze` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2wlzze -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2wlzze -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2wlzze --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-axkm4t` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-axkm4t -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-axkm4t -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-axkm4t --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-nuedrg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-nuedrg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nuedrg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-nuedrg --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-aw7nie` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-aw7nie -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-aw7nie -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-aw7nie --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-76gtdl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-76gtdl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-76gtdl -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-76gtdl --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-8w6anr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8w6anr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8w6anr -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8w6anr --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-44eqqx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-44eqqx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-44eqqx -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-44eqqx --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-b35iql` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-b35iql -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-b35iql -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-b35iql --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-wr57ja` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-wr57ja -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wr57ja -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-wr57ja --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-090iwo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-090iwo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-090iwo -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-090iwo --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-3odqp1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3odqp1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3odqp1 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3odqp1 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-801vvq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-801vvq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-801vvq -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-801vvq --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-x6fm3e` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x6fm3e -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x6fm3e -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x6fm3e --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-ffgs38` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ffgs38 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ffgs38 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ffgs38 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-clkirr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-clkirr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-clkirr -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-clkirr --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-5ussr6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-5ussr6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5ussr6 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-5ussr6 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-y3dl6z` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-y3dl6z -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y3dl6z -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-y3dl6z --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-10": {
    "id": "auto-ckad-10",
    "title": "Auto CKAD Practice (Batch 10)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 10)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-qdj9oz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-idwhah` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-sg58m2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-dsbcpm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-qpdevr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-pnx67d` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-hgu630` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-bnbu9i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-mwgtuu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-i1x63g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-xs2wy0` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-vufkab` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-qxn1vx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-95hh8o` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-2jd7xs` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-88r3ur` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-d49lf1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-dnt3y4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-x9s3xq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-fyifz8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-qdj9oz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qdj9oz -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-idwhah -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-idwhah -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-sg58m2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-sg58m2 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-dsbcpm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dsbcpm -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qpdevr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qpdevr -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-pnx67d -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pnx67d -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-hgu630 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hgu630 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-bnbu9i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bnbu9i -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-mwgtuu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-mwgtuu -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-i1x63g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-i1x63g -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-xs2wy0 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xs2wy0 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vufkab -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vufkab -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qxn1vx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qxn1vx -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-95hh8o -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-95hh8o -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2jd7xs -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2jd7xs -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-88r3ur -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-88r3ur -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-d49lf1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-d49lf1 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-dnt3y4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dnt3y4 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-x9s3xq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x9s3xq -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-fyifz8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-fyifz8 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-qdj9oz --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-idwhah --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-sg58m2 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-dsbcpm --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qpdevr --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-pnx67d --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-hgu630 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-bnbu9i --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-mwgtuu --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-i1x63g --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-xs2wy0 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vufkab --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qxn1vx --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-95hh8o --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2jd7xs --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-88r3ur --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-d49lf1 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-dnt3y4 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-x9s3xq --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-fyifz8 --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 10)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-qdj9oz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qdj9oz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qdj9oz -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qdj9oz --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-idwhah` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-idwhah -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-idwhah -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-idwhah --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-sg58m2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-sg58m2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-sg58m2 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-sg58m2 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-dsbcpm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-dsbcpm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dsbcpm -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-dsbcpm --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-qpdevr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qpdevr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qpdevr -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qpdevr --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-pnx67d` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-pnx67d -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pnx67d -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-pnx67d --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-hgu630` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-hgu630 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hgu630 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-hgu630 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-bnbu9i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-bnbu9i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bnbu9i -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-bnbu9i --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-mwgtuu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-mwgtuu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-mwgtuu -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-mwgtuu --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-i1x63g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-i1x63g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-i1x63g -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-i1x63g --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-xs2wy0` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-xs2wy0 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xs2wy0 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-xs2wy0 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-vufkab` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vufkab -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vufkab -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vufkab --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-qxn1vx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qxn1vx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qxn1vx -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qxn1vx --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-95hh8o` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-95hh8o -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-95hh8o -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-95hh8o --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-2jd7xs` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2jd7xs -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2jd7xs -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2jd7xs --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-88r3ur` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-88r3ur -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-88r3ur -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-88r3ur --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-d49lf1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-d49lf1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-d49lf1 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-d49lf1 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-dnt3y4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-dnt3y4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dnt3y4 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-dnt3y4 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-x9s3xq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x9s3xq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x9s3xq -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x9s3xq --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-fyifz8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-fyifz8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-fyifz8 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-fyifz8 --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-11": {
    "id": "auto-ckad-11",
    "title": "Auto CKAD Practice (Batch 11)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 11)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-1qcw9i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-4u6yap` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-z0yv5t` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-3bwj0y` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-xha16v` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-27509s` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-bxi65s` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-sem9wl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-1lp21x` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-chh7qh` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-vhybfv` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-414t9m` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-xyycnz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-byfyqh` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-44idmk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-griimg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-4nl7mn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-7nq0pg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-3axafo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-2pc3in` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-1qcw9i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1qcw9i -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4u6yap -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4u6yap -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-z0yv5t -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-z0yv5t -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3bwj0y -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3bwj0y -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-xha16v -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xha16v -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-27509s -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-27509s -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-bxi65s -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bxi65s -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-sem9wl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-sem9wl -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1lp21x -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1lp21x -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-chh7qh -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-chh7qh -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vhybfv -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vhybfv -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-414t9m -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-414t9m -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-xyycnz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xyycnz -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-byfyqh -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-byfyqh -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-44idmk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-44idmk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-griimg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-griimg -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4nl7mn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4nl7mn -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-7nq0pg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7nq0pg -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3axafo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3axafo -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2pc3in -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2pc3in -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-1qcw9i --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4u6yap --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-z0yv5t --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3bwj0y --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-xha16v --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-27509s --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-bxi65s --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-sem9wl --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1lp21x --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-chh7qh --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vhybfv --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-414t9m --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-xyycnz --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-byfyqh --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-44idmk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-griimg --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4nl7mn --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-7nq0pg --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3axafo --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2pc3in --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 11)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-1qcw9i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1qcw9i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1qcw9i -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1qcw9i --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-4u6yap` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4u6yap -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4u6yap -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4u6yap --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-z0yv5t` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-z0yv5t -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-z0yv5t -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-z0yv5t --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-3bwj0y` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3bwj0y -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3bwj0y -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3bwj0y --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-xha16v` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-xha16v -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xha16v -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-xha16v --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-27509s` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-27509s -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-27509s -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-27509s --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-bxi65s` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-bxi65s -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bxi65s -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-bxi65s --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-sem9wl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-sem9wl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-sem9wl -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-sem9wl --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-1lp21x` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1lp21x -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1lp21x -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1lp21x --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-chh7qh` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-chh7qh -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-chh7qh -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-chh7qh --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-vhybfv` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vhybfv -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vhybfv -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vhybfv --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-414t9m` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-414t9m -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-414t9m -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-414t9m --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-xyycnz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-xyycnz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xyycnz -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-xyycnz --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-byfyqh` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-byfyqh -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-byfyqh -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-byfyqh --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-44idmk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-44idmk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-44idmk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-44idmk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-griimg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-griimg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-griimg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-griimg --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-4nl7mn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4nl7mn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4nl7mn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4nl7mn --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-7nq0pg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-7nq0pg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7nq0pg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-7nq0pg --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-3axafo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3axafo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3axafo -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3axafo --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-2pc3in` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2pc3in -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2pc3in -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2pc3in --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-12": {
    "id": "auto-ckad-12",
    "title": "Auto CKAD Practice (Batch 12)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 12)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-x6nknc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-05f5io` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-h3nn1d` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-z9kxon` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-amnw3u` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-kz6vig` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-anh5nk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-qjlwbe` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-4xppqh` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-hrk9n6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-pame2g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-97ld0c` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-lf1ojl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-1oybnw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-p74ezx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-jh33n0` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-bz3f10` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-6kbtmx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-tz0vdk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-ilaz1h` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-x6nknc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x6nknc -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-05f5io -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-05f5io -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-h3nn1d -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h3nn1d -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-z9kxon -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-z9kxon -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-amnw3u -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-amnw3u -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kz6vig -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kz6vig -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-anh5nk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-anh5nk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qjlwbe -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qjlwbe -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4xppqh -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4xppqh -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-hrk9n6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hrk9n6 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-pame2g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pame2g -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-97ld0c -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-97ld0c -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-lf1ojl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lf1ojl -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1oybnw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1oybnw -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-p74ezx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-p74ezx -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-jh33n0 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-jh33n0 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-bz3f10 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bz3f10 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-6kbtmx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6kbtmx -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-tz0vdk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-tz0vdk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ilaz1h -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ilaz1h -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-x6nknc --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-05f5io --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-h3nn1d --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-z9kxon --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-amnw3u --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kz6vig --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-anh5nk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qjlwbe --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4xppqh --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-hrk9n6 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-pame2g --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-97ld0c --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-lf1ojl --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1oybnw --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-p74ezx --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-jh33n0 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-bz3f10 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-6kbtmx --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-tz0vdk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ilaz1h --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 12)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-x6nknc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x6nknc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x6nknc -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x6nknc --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-05f5io` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-05f5io -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-05f5io -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-05f5io --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-h3nn1d` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-h3nn1d -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h3nn1d -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-h3nn1d --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-z9kxon` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-z9kxon -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-z9kxon -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-z9kxon --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-amnw3u` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-amnw3u -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-amnw3u -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-amnw3u --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-kz6vig` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kz6vig -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kz6vig -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kz6vig --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-anh5nk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-anh5nk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-anh5nk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-anh5nk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-qjlwbe` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qjlwbe -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qjlwbe -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qjlwbe --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-4xppqh` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4xppqh -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4xppqh -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4xppqh --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-hrk9n6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-hrk9n6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hrk9n6 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-hrk9n6 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-pame2g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-pame2g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pame2g -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-pame2g --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-97ld0c` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-97ld0c -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-97ld0c -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-97ld0c --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-lf1ojl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-lf1ojl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lf1ojl -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-lf1ojl --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-1oybnw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1oybnw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1oybnw -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1oybnw --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-p74ezx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-p74ezx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-p74ezx -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-p74ezx --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-jh33n0` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-jh33n0 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-jh33n0 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-jh33n0 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-bz3f10` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-bz3f10 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bz3f10 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-bz3f10 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-6kbtmx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-6kbtmx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6kbtmx -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-6kbtmx --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-tz0vdk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-tz0vdk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-tz0vdk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-tz0vdk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-ilaz1h` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ilaz1h -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ilaz1h -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ilaz1h --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-13": {
    "id": "auto-ckad-13",
    "title": "Auto CKAD Practice (Batch 13)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 13)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-q7dt5k` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-gyqcdn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-8gdpir` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-3b9ym5` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-n4cue4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-bgiydm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-wt9wbp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-ehek4i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-ig8rr4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-yek4nf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-iawqc6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-rng37n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-gxsot1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-vo6rbg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-y6ps20` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-vk60cu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-36zn36` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-qtjct6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-npcek3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-0vkz6e` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-q7dt5k -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q7dt5k -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-gyqcdn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gyqcdn -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8gdpir -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8gdpir -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3b9ym5 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3b9ym5 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-n4cue4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-n4cue4 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-bgiydm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bgiydm -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-wt9wbp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wt9wbp -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ehek4i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ehek4i -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ig8rr4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ig8rr4 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-yek4nf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-yek4nf -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-iawqc6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-iawqc6 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-rng37n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-rng37n -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-gxsot1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gxsot1 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vo6rbg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vo6rbg -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-y6ps20 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y6ps20 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vk60cu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vk60cu -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-36zn36 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-36zn36 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qtjct6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qtjct6 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-npcek3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-npcek3 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-0vkz6e -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0vkz6e -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-q7dt5k --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-gyqcdn --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8gdpir --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3b9ym5 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-n4cue4 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-bgiydm --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-wt9wbp --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ehek4i --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ig8rr4 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-yek4nf --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-iawqc6 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-rng37n --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-gxsot1 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vo6rbg --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-y6ps20 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vk60cu --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-36zn36 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qtjct6 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-npcek3 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-0vkz6e --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 13)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-q7dt5k` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-q7dt5k -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q7dt5k -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-q7dt5k --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-gyqcdn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-gyqcdn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gyqcdn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-gyqcdn --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-8gdpir` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8gdpir -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8gdpir -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8gdpir --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-3b9ym5` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3b9ym5 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3b9ym5 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3b9ym5 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-n4cue4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-n4cue4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-n4cue4 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-n4cue4 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-bgiydm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-bgiydm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bgiydm -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-bgiydm --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-wt9wbp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-wt9wbp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wt9wbp -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-wt9wbp --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-ehek4i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ehek4i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ehek4i -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ehek4i --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-ig8rr4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ig8rr4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ig8rr4 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ig8rr4 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-yek4nf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-yek4nf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-yek4nf -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-yek4nf --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-iawqc6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-iawqc6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-iawqc6 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-iawqc6 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-rng37n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-rng37n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-rng37n -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-rng37n --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-gxsot1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-gxsot1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gxsot1 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-gxsot1 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-vo6rbg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vo6rbg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vo6rbg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vo6rbg --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-y6ps20` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-y6ps20 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y6ps20 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-y6ps20 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-vk60cu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vk60cu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vk60cu -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vk60cu --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-36zn36` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-36zn36 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-36zn36 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-36zn36 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-qtjct6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qtjct6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qtjct6 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qtjct6 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-npcek3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-npcek3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-npcek3 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-npcek3 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-0vkz6e` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-0vkz6e -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0vkz6e -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-0vkz6e --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-14": {
    "id": "auto-ckad-14",
    "title": "Auto CKAD Practice (Batch 14)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 14)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-kshv59` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-vdss6p` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-vof1h3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-tgzbnj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-14n3g8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-3kr5r6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-ey949y` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-n853dn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-pxlkk7` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-kx3pxu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-nnbcam` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-zl8qkw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-ij73ff` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-ut00dx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-i5b1iz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-jp913k` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-2jygua` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-stju2y` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-kx16zk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-9fx2w2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-kshv59 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kshv59 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vdss6p -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vdss6p -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vof1h3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vof1h3 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-tgzbnj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-tgzbnj -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-14n3g8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-14n3g8 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3kr5r6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3kr5r6 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ey949y -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ey949y -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-n853dn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-n853dn -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-pxlkk7 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pxlkk7 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kx3pxu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kx3pxu -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-nnbcam -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nnbcam -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-zl8qkw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zl8qkw -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ij73ff -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ij73ff -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ut00dx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ut00dx -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-i5b1iz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-i5b1iz -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-jp913k -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-jp913k -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2jygua -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2jygua -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-stju2y -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-stju2y -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kx16zk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kx16zk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-9fx2w2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9fx2w2 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-kshv59 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vdss6p --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vof1h3 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-tgzbnj --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-14n3g8 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3kr5r6 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ey949y --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-n853dn --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-pxlkk7 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kx3pxu --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-nnbcam --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-zl8qkw --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ij73ff --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ut00dx --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-i5b1iz --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-jp913k --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2jygua --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-stju2y --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kx16zk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-9fx2w2 --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 14)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-kshv59` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kshv59 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kshv59 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kshv59 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-vdss6p` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vdss6p -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vdss6p -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vdss6p --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-vof1h3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vof1h3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vof1h3 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vof1h3 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-tgzbnj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-tgzbnj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-tgzbnj -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-tgzbnj --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-14n3g8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-14n3g8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-14n3g8 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-14n3g8 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-3kr5r6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3kr5r6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3kr5r6 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3kr5r6 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-ey949y` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ey949y -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ey949y -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ey949y --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-n853dn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-n853dn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-n853dn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-n853dn --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-pxlkk7` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-pxlkk7 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pxlkk7 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-pxlkk7 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-kx3pxu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kx3pxu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kx3pxu -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kx3pxu --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-nnbcam` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-nnbcam -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nnbcam -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-nnbcam --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-zl8qkw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-zl8qkw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zl8qkw -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-zl8qkw --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-ij73ff` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ij73ff -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ij73ff -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ij73ff --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-ut00dx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ut00dx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ut00dx -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ut00dx --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-i5b1iz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-i5b1iz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-i5b1iz -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-i5b1iz --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-jp913k` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-jp913k -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-jp913k -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-jp913k --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-2jygua` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2jygua -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2jygua -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2jygua --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-stju2y` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-stju2y -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-stju2y -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-stju2y --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-kx16zk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kx16zk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kx16zk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kx16zk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-9fx2w2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-9fx2w2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9fx2w2 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-9fx2w2 --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-15": {
    "id": "auto-ckad-15",
    "title": "Auto CKAD Practice (Batch 15)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 15)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-wegyi3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-x05bet` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-qx8fap` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-lfkosk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-65bobq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-3lr9lp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-s4foap` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-ya669b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-fi5dxm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-qj8nj0` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-kex8wt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-q40458` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-cq49xn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-3ze3ed` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-9uz02m` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-kbsj8n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-gomuaf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-vduze6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-p3e8c9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-5onieg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-wegyi3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wegyi3 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-x05bet -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x05bet -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qx8fap -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qx8fap -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-lfkosk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lfkosk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-65bobq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-65bobq -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3lr9lp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3lr9lp -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-s4foap -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-s4foap -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ya669b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ya669b -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-fi5dxm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-fi5dxm -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qj8nj0 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qj8nj0 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kex8wt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kex8wt -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-q40458 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q40458 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-cq49xn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cq49xn -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3ze3ed -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3ze3ed -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-9uz02m -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9uz02m -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kbsj8n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kbsj8n -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-gomuaf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gomuaf -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vduze6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vduze6 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-p3e8c9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-p3e8c9 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-5onieg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5onieg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-wegyi3 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-x05bet --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qx8fap --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-lfkosk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-65bobq --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3lr9lp --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-s4foap --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ya669b --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-fi5dxm --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qj8nj0 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kex8wt --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-q40458 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-cq49xn --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3ze3ed --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-9uz02m --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kbsj8n --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-gomuaf --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vduze6 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-p3e8c9 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-5onieg --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 15)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-wegyi3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-wegyi3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wegyi3 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-wegyi3 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-x05bet` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x05bet -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x05bet -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x05bet --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-qx8fap` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qx8fap -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qx8fap -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qx8fap --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-lfkosk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-lfkosk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lfkosk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-lfkosk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-65bobq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-65bobq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-65bobq -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-65bobq --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-3lr9lp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3lr9lp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3lr9lp -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3lr9lp --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-s4foap` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-s4foap -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-s4foap -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-s4foap --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-ya669b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ya669b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ya669b -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ya669b --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-fi5dxm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-fi5dxm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-fi5dxm -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-fi5dxm --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-qj8nj0` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qj8nj0 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qj8nj0 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qj8nj0 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-kex8wt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kex8wt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kex8wt -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kex8wt --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-q40458` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-q40458 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q40458 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-q40458 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-cq49xn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-cq49xn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cq49xn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-cq49xn --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-3ze3ed` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3ze3ed -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3ze3ed -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3ze3ed --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-9uz02m` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-9uz02m -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9uz02m -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-9uz02m --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-kbsj8n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kbsj8n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kbsj8n -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kbsj8n --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-gomuaf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-gomuaf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gomuaf -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-gomuaf --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-vduze6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vduze6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vduze6 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vduze6 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-p3e8c9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-p3e8c9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-p3e8c9 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-p3e8c9 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-5onieg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-5onieg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5onieg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-5onieg --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-2": {
    "id": "auto-ckad-2",
    "title": "Auto CKAD Practice (Batch 2)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 2)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-nprn2e` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-xcko99` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-8f1no3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-yhdart` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-v4bzmy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-jyvyy6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-19ej1f` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-bz2ton` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-1iqka2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-aixqpx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-9y79zz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-rprpfb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-cd794n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-wz0hl3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-8e751b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-onskt8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-m712kk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-g3hoic` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-q6pouw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-x3qv1b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-nprn2e -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nprn2e -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-xcko99 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xcko99 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8f1no3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8f1no3 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-yhdart -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-yhdart -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-v4bzmy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-v4bzmy -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-jyvyy6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-jyvyy6 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-19ej1f -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-19ej1f -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-bz2ton -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bz2ton -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1iqka2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1iqka2 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-aixqpx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-aixqpx -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-9y79zz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9y79zz -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-rprpfb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-rprpfb -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-cd794n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cd794n -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-wz0hl3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wz0hl3 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8e751b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8e751b -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-onskt8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-onskt8 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-m712kk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-m712kk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-g3hoic -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-g3hoic -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-q6pouw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q6pouw -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-x3qv1b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x3qv1b -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-nprn2e --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-xcko99 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8f1no3 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-yhdart --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-v4bzmy --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-jyvyy6 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-19ej1f --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-bz2ton --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1iqka2 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-aixqpx --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-9y79zz --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-rprpfb --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-cd794n --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-wz0hl3 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8e751b --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-onskt8 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-m712kk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-g3hoic --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-q6pouw --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-x3qv1b --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 2)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-nprn2e` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-nprn2e -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nprn2e -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-nprn2e --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-xcko99` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-xcko99 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xcko99 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-xcko99 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-8f1no3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8f1no3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8f1no3 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8f1no3 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-yhdart` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-yhdart -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-yhdart -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-yhdart --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-v4bzmy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-v4bzmy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-v4bzmy -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-v4bzmy --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-jyvyy6` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-jyvyy6 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-jyvyy6 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-jyvyy6 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-19ej1f` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-19ej1f -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-19ej1f -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-19ej1f --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-bz2ton` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-bz2ton -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bz2ton -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-bz2ton --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-1iqka2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1iqka2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1iqka2 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1iqka2 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-aixqpx` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-aixqpx -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-aixqpx -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-aixqpx --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-9y79zz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-9y79zz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9y79zz -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-9y79zz --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-rprpfb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-rprpfb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-rprpfb -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-rprpfb --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-cd794n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-cd794n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cd794n -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-cd794n --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-wz0hl3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-wz0hl3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wz0hl3 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-wz0hl3 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-8e751b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8e751b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8e751b -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8e751b --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-onskt8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-onskt8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-onskt8 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-onskt8 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-m712kk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-m712kk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-m712kk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-m712kk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-g3hoic` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-g3hoic -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-g3hoic -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-g3hoic --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-q6pouw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-q6pouw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q6pouw -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-q6pouw --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-x3qv1b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x3qv1b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x3qv1b -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x3qv1b --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-3": {
    "id": "auto-ckad-3",
    "title": "Auto CKAD Practice (Batch 3)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 3)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-0r6n02` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-alen1i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-g73b64` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-brik5d` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-5zbiqw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-dj6kci` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-wdj888` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-2uovf4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-2zgjno` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-1zxqrj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-cod1q8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-pjx7gb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-hvxsry` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-q1kik9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-v30jbe` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-bh4kir` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-vyelgt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-zu2wt2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-x9a5w1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-ipz6hn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-0r6n02 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0r6n02 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-alen1i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-alen1i -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-g73b64 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-g73b64 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-brik5d -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-brik5d -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-5zbiqw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5zbiqw -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-dj6kci -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dj6kci -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-wdj888 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wdj888 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2uovf4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2uovf4 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2zgjno -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2zgjno -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1zxqrj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1zxqrj -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-cod1q8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cod1q8 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-pjx7gb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pjx7gb -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-hvxsry -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hvxsry -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-q1kik9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q1kik9 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-v30jbe -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-v30jbe -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-bh4kir -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bh4kir -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vyelgt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vyelgt -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-zu2wt2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zu2wt2 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-x9a5w1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x9a5w1 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ipz6hn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ipz6hn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-0r6n02 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-alen1i --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-g73b64 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-brik5d --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-5zbiqw --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-dj6kci --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-wdj888 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2uovf4 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2zgjno --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1zxqrj --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-cod1q8 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-pjx7gb --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-hvxsry --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-q1kik9 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-v30jbe --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-bh4kir --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vyelgt --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-zu2wt2 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-x9a5w1 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ipz6hn --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 3)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-0r6n02` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-0r6n02 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0r6n02 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-0r6n02 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-alen1i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-alen1i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-alen1i -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-alen1i --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-g73b64` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-g73b64 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-g73b64 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-g73b64 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-brik5d` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-brik5d -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-brik5d -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-brik5d --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-5zbiqw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-5zbiqw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5zbiqw -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-5zbiqw --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-dj6kci` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-dj6kci -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dj6kci -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-dj6kci --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-wdj888` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-wdj888 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wdj888 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-wdj888 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-2uovf4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2uovf4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2uovf4 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2uovf4 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-2zgjno` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2zgjno -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2zgjno -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2zgjno --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-1zxqrj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1zxqrj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1zxqrj -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1zxqrj --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-cod1q8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-cod1q8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cod1q8 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-cod1q8 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-pjx7gb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-pjx7gb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pjx7gb -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-pjx7gb --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-hvxsry` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-hvxsry -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hvxsry -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-hvxsry --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-q1kik9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-q1kik9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q1kik9 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-q1kik9 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-v30jbe` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-v30jbe -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-v30jbe -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-v30jbe --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-bh4kir` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-bh4kir -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-bh4kir -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-bh4kir --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-vyelgt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vyelgt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vyelgt -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vyelgt --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-zu2wt2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-zu2wt2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zu2wt2 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-zu2wt2 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-x9a5w1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x9a5w1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x9a5w1 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x9a5w1 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-ipz6hn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ipz6hn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ipz6hn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ipz6hn --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-4": {
    "id": "auto-ckad-4",
    "title": "Auto CKAD Practice (Batch 4)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 4)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-0l16fg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-dgmyze` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-y70ojp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-snwnnc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-btuzdl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-86f2hm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-2umf3o` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-8bkp43` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-h24ncv` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-vp2684` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-iigsuc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-qstegz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-kolsjo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-xuecjg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-8fggfv` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-qmorfl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-4jl6uo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-letzco` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-tkweh7` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-4zx6og` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-0l16fg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0l16fg -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-dgmyze -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dgmyze -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-y70ojp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y70ojp -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-snwnnc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-snwnnc -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-btuzdl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-btuzdl -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-86f2hm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-86f2hm -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2umf3o -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2umf3o -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8bkp43 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8bkp43 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-h24ncv -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h24ncv -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vp2684 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vp2684 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-iigsuc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-iigsuc -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qstegz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qstegz -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kolsjo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kolsjo -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-xuecjg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xuecjg -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8fggfv -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8fggfv -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qmorfl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qmorfl -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4jl6uo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4jl6uo -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-letzco -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-letzco -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-tkweh7 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-tkweh7 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4zx6og -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4zx6og -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-0l16fg --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-dgmyze --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-y70ojp --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-snwnnc --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-btuzdl --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-86f2hm --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2umf3o --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8bkp43 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-h24ncv --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vp2684 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-iigsuc --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qstegz --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kolsjo --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-xuecjg --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8fggfv --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qmorfl --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4jl6uo --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-letzco --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-tkweh7 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4zx6og --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 4)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-0l16fg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-0l16fg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0l16fg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-0l16fg --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-dgmyze` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-dgmyze -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dgmyze -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-dgmyze --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-y70ojp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-y70ojp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y70ojp -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-y70ojp --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-snwnnc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-snwnnc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-snwnnc -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-snwnnc --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-btuzdl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-btuzdl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-btuzdl -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-btuzdl --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-86f2hm` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-86f2hm -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-86f2hm -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-86f2hm --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-2umf3o` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2umf3o -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2umf3o -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2umf3o --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-8bkp43` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8bkp43 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8bkp43 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8bkp43 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-h24ncv` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-h24ncv -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h24ncv -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-h24ncv --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-vp2684` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vp2684 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vp2684 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vp2684 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-iigsuc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-iigsuc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-iigsuc -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-iigsuc --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-qstegz` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qstegz -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qstegz -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qstegz --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-kolsjo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kolsjo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kolsjo -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kolsjo --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-xuecjg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-xuecjg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xuecjg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-xuecjg --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-8fggfv` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8fggfv -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8fggfv -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8fggfv --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-qmorfl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qmorfl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qmorfl -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qmorfl --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-4jl6uo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4jl6uo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4jl6uo -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4jl6uo --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-letzco` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-letzco -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-letzco -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-letzco --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-tkweh7` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-tkweh7 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-tkweh7 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-tkweh7 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-4zx6og` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4zx6og -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4zx6og -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4zx6og --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-5": {
    "id": "auto-ckad-5",
    "title": "Auto CKAD Practice (Batch 5)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 5)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-dbroff` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-fixy1c` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-djwg1i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-qj1y63` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-ya4x0b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-chqeem` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-6ckmez` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-lzu2tw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-b2oerb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-6p9pgu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-0s986j` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-3kkqzf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-8k8ksa` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-4hf6st` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-urjoxe` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-gi7wq4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-q6kx0i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-294w68` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-alurqs` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-q1w3ip` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-dbroff -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dbroff -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-fixy1c -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-fixy1c -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-djwg1i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-djwg1i -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qj1y63 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qj1y63 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ya4x0b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ya4x0b -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-chqeem -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-chqeem -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-6ckmez -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6ckmez -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-lzu2tw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lzu2tw -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-b2oerb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-b2oerb -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-6p9pgu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6p9pgu -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-0s986j -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0s986j -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3kkqzf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3kkqzf -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8k8ksa -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8k8ksa -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4hf6st -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4hf6st -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-urjoxe -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-urjoxe -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-gi7wq4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gi7wq4 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-q6kx0i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q6kx0i -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-294w68 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-294w68 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-alurqs -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-alurqs -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-q1w3ip -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q1w3ip -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-dbroff --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-fixy1c --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-djwg1i --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qj1y63 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ya4x0b --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-chqeem --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-6ckmez --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-lzu2tw --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-b2oerb --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-6p9pgu --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-0s986j --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3kkqzf --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8k8ksa --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4hf6st --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-urjoxe --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-gi7wq4 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-q6kx0i --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-294w68 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-alurqs --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-q1w3ip --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 5)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-dbroff` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-dbroff -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dbroff -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-dbroff --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-fixy1c` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-fixy1c -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-fixy1c -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-fixy1c --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-djwg1i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-djwg1i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-djwg1i -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-djwg1i --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-qj1y63` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qj1y63 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qj1y63 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qj1y63 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-ya4x0b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ya4x0b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ya4x0b -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ya4x0b --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-chqeem` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-chqeem -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-chqeem -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-chqeem --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-6ckmez` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-6ckmez -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6ckmez -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-6ckmez --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-lzu2tw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-lzu2tw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lzu2tw -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-lzu2tw --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-b2oerb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-b2oerb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-b2oerb -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-b2oerb --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-6p9pgu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-6p9pgu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6p9pgu -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-6p9pgu --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-0s986j` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-0s986j -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0s986j -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-0s986j --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-3kkqzf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3kkqzf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3kkqzf -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3kkqzf --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-8k8ksa` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8k8ksa -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8k8ksa -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8k8ksa --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-4hf6st` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4hf6st -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4hf6st -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4hf6st --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-urjoxe` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-urjoxe -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-urjoxe -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-urjoxe --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-gi7wq4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-gi7wq4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gi7wq4 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-gi7wq4 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-q6kx0i` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-q6kx0i -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q6kx0i -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-q6kx0i --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-294w68` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-294w68 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-294w68 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-294w68 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-alurqs` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-alurqs -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-alurqs -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-alurqs --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-q1w3ip` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-q1w3ip -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-q1w3ip -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-q1w3ip --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-6": {
    "id": "auto-ckad-6",
    "title": "Auto CKAD Practice (Batch 6)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 6)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-dosng5` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-r0ozmt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-8icxjc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-x6bj6u` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-hcq2ie` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-ao931l` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-4454s2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-1ljssn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-zcil23` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-21ydat` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-lnuasy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-pux7zf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-kpu3yj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-h5b6ev` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-9ms51b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-5joahn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-h37ghi` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-dahpfb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-7zhv9l` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-8n2xsd` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-dosng5 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dosng5 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-r0ozmt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-r0ozmt -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8icxjc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8icxjc -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-x6bj6u -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x6bj6u -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-hcq2ie -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hcq2ie -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ao931l -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ao931l -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4454s2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4454s2 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1ljssn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1ljssn -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-zcil23 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zcil23 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-21ydat -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-21ydat -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-lnuasy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lnuasy -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-pux7zf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pux7zf -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kpu3yj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kpu3yj -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-h5b6ev -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h5b6ev -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-9ms51b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9ms51b -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-5joahn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5joahn -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-h37ghi -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h37ghi -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-dahpfb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dahpfb -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-7zhv9l -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7zhv9l -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8n2xsd -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8n2xsd -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-dosng5 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-r0ozmt --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8icxjc --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-x6bj6u --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-hcq2ie --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ao931l --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4454s2 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1ljssn --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-zcil23 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-21ydat --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-lnuasy --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-pux7zf --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kpu3yj --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-h5b6ev --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-9ms51b --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-5joahn --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-h37ghi --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-dahpfb --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-7zhv9l --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8n2xsd --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 6)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-dosng5` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-dosng5 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dosng5 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-dosng5 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-r0ozmt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-r0ozmt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-r0ozmt -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-r0ozmt --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-8icxjc` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8icxjc -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8icxjc -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8icxjc --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-x6bj6u` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x6bj6u -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x6bj6u -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x6bj6u --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-hcq2ie` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-hcq2ie -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-hcq2ie -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-hcq2ie --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-ao931l` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ao931l -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ao931l -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ao931l --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-4454s2` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4454s2 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4454s2 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4454s2 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-1ljssn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1ljssn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1ljssn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1ljssn --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-zcil23` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-zcil23 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zcil23 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-zcil23 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-21ydat` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-21ydat -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-21ydat -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-21ydat --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-lnuasy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-lnuasy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-lnuasy -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-lnuasy --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-pux7zf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-pux7zf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pux7zf -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-pux7zf --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-kpu3yj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kpu3yj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kpu3yj -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kpu3yj --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-h5b6ev` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-h5b6ev -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h5b6ev -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-h5b6ev --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-9ms51b` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-9ms51b -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9ms51b -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-9ms51b --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-5joahn` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-5joahn -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5joahn -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-5joahn --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-h37ghi` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-h37ghi -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-h37ghi -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-h37ghi --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-dahpfb` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-dahpfb -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-dahpfb -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-dahpfb --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-7zhv9l` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-7zhv9l -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7zhv9l -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-7zhv9l --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-8n2xsd` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8n2xsd -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8n2xsd -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8n2xsd --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-7": {
    "id": "auto-ckad-7",
    "title": "Auto CKAD Practice (Batch 7)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 7)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-i65fhs` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-6s7zp1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-msqkwt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-8phiki` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-glpl9u` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-otw3f8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-y1b03s` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-2zcuzu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-1ndg68` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-9mpg9g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-7aur49` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-cnoqiy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-wkc25a` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-80eoib` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-j833o4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-3oppq4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-8np7ra` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-4148ro` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-6ep6mo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-l4t485` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-i65fhs -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-i65fhs -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-6s7zp1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6s7zp1 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-msqkwt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-msqkwt -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8phiki -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8phiki -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-glpl9u -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-glpl9u -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-otw3f8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-otw3f8 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-y1b03s -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y1b03s -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-2zcuzu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2zcuzu -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1ndg68 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1ndg68 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-9mpg9g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9mpg9g -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-7aur49 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7aur49 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-cnoqiy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cnoqiy -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-wkc25a -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wkc25a -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-80eoib -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-80eoib -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-j833o4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-j833o4 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3oppq4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3oppq4 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-8np7ra -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8np7ra -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-4148ro -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4148ro -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-6ep6mo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6ep6mo -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-l4t485 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-l4t485 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-i65fhs --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-6s7zp1 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-msqkwt --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8phiki --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-glpl9u --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-otw3f8 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-y1b03s --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-2zcuzu --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1ndg68 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-9mpg9g --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-7aur49 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-cnoqiy --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-wkc25a --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-80eoib --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-j833o4 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3oppq4 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-8np7ra --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-4148ro --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-6ep6mo --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-l4t485 --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 7)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-i65fhs` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-i65fhs -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-i65fhs -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-i65fhs --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-6s7zp1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-6s7zp1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6s7zp1 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-6s7zp1 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-msqkwt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-msqkwt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-msqkwt -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-msqkwt --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-8phiki` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8phiki -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8phiki -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8phiki --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-glpl9u` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-glpl9u -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-glpl9u -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-glpl9u --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-otw3f8` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-otw3f8 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-otw3f8 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-otw3f8 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-y1b03s` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-y1b03s -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-y1b03s -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-y1b03s --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-2zcuzu` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-2zcuzu -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-2zcuzu -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-2zcuzu --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-1ndg68` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1ndg68 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1ndg68 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1ndg68 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-9mpg9g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-9mpg9g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-9mpg9g -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-9mpg9g --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-7aur49` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-7aur49 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7aur49 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-7aur49 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-cnoqiy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-cnoqiy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-cnoqiy -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-cnoqiy --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-wkc25a` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-wkc25a -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wkc25a -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-wkc25a --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-80eoib` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-80eoib -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-80eoib -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-80eoib --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-j833o4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-j833o4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-j833o4 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-j833o4 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-3oppq4` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3oppq4 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3oppq4 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3oppq4 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-8np7ra` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-8np7ra -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-8np7ra -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-8np7ra --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-4148ro` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-4148ro -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-4148ro -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-4148ro --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-6ep6mo` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-6ep6mo -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-6ep6mo -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-6ep6mo --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-l4t485` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-l4t485 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-l4t485 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-l4t485 --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-8": {
    "id": "auto-ckad-8",
    "title": "Auto CKAD Practice (Batch 8)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 8)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-7w0kgg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-mc8ufd` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-x3yozk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-zdc1s3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-oihg3n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-kdbxa9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-oaizu3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-78huzi` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-gt0cca` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-qepwhr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-td5h3a` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-vgh1ic` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-ykwlic` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-ehrlca` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-wgqgnw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-5j5xgd` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-monu0h` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-7wd2er` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-uy9v7x` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-1b807m` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-7w0kgg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7w0kgg -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-mc8ufd -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-mc8ufd -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-x3yozk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x3yozk -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-zdc1s3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zdc1s3 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-oihg3n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-oihg3n -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-kdbxa9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kdbxa9 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-oaizu3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-oaizu3 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-78huzi -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-78huzi -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-gt0cca -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gt0cca -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-qepwhr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qepwhr -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-td5h3a -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-td5h3a -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vgh1ic -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vgh1ic -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ykwlic -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ykwlic -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ehrlca -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ehrlca -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-wgqgnw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wgqgnw -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-5j5xgd -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5j5xgd -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-monu0h -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-monu0h -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-7wd2er -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7wd2er -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-uy9v7x -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-uy9v7x -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1b807m -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1b807m -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-7w0kgg --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-mc8ufd --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-x3yozk --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-zdc1s3 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-oihg3n --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-kdbxa9 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-oaizu3 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-78huzi --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-gt0cca --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-qepwhr --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-td5h3a --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vgh1ic --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ykwlic --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ehrlca --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-wgqgnw --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-5j5xgd --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-monu0h --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-7wd2er --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-uy9v7x --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1b807m --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 8)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-7w0kgg` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-7w0kgg -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7w0kgg -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-7w0kgg --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-mc8ufd` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-mc8ufd -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-mc8ufd -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-mc8ufd --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-x3yozk` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-x3yozk -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-x3yozk -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-x3yozk --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-zdc1s3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-zdc1s3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-zdc1s3 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-zdc1s3 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-oihg3n` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-oihg3n -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-oihg3n -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-oihg3n --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-kdbxa9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-kdbxa9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-kdbxa9 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-kdbxa9 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-oaizu3` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-oaizu3 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-oaizu3 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-oaizu3 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-78huzi` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-78huzi -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-78huzi -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-78huzi --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-gt0cca` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-gt0cca -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-gt0cca -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-gt0cca --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-qepwhr` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-qepwhr -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-qepwhr -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-qepwhr --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-td5h3a` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-td5h3a -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-td5h3a -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-td5h3a --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-vgh1ic` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vgh1ic -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vgh1ic -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vgh1ic --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-ykwlic` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ykwlic -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ykwlic -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ykwlic --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-ehrlca` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ehrlca -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ehrlca -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ehrlca --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-wgqgnw` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-wgqgnw -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-wgqgnw -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-wgqgnw --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-5j5xgd` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-5j5xgd -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5j5xgd -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-5j5xgd --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-monu0h` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-monu0h -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-monu0h -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-monu0h --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-7wd2er` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-7wd2er -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-7wd2er -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-7wd2er --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-uy9v7x` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-uy9v7x -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-uy9v7x -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-uy9v7x --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-1b807m` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1b807m -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1b807m -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1b807m --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-ckad-9": {
    "id": "auto-ckad-9",
    "title": "Auto CKAD Practice (Batch 9)",
    "category": "CKAD",
    "duration": "120 mins",
    "markdown": "\n# Auto CKAD Practice (Batch 9)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Multi-Container Pod\nCreate a pod named `ckad-sfg713` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 2: Multi-Container Pod\nCreate a pod named `ckad-0epdn9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 3: Multi-Container Pod\nCreate a pod named `ckad-vos9jt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 4: Multi-Container Pod\nCreate a pod named `ckad-1hyenf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 5: Multi-Container Pod\nCreate a pod named `ckad-pw44yp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 6: Multi-Container Pod\nCreate a pod named `ckad-nn2bg1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 7: Multi-Container Pod\nCreate a pod named `ckad-p79zpy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 8: Multi-Container Pod\nCreate a pod named `ckad-5pz249` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 9: Multi-Container Pod\nCreate a pod named `ckad-08gphq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 10: Multi-Container Pod\nCreate a pod named `ckad-36av4l` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 11: Multi-Container Pod\nCreate a pod named `ckad-3o3e3g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 12: Multi-Container Pod\nCreate a pod named `ckad-aoeece` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 13: Multi-Container Pod\nCreate a pod named `ckad-rmq74z` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 14: Multi-Container Pod\nCreate a pod named `ckad-92zr4x` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 15: Multi-Container Pod\nCreate a pod named `ckad-46q7ud` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 16: Multi-Container Pod\nCreate a pod named `ckad-1qpnby` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 17: Multi-Container Pod\nCreate a pod named `ckad-xdr3fl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 18: Multi-Container Pod\nCreate a pod named `ckad-mofxo1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 19: Multi-Container Pod\nCreate a pod named `ckad-ziwi5a` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n# Task 20: Multi-Container Pod\nCreate a pod named `ckad-vmqmvj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
    "verifyScript": "kubectl get pod ckad-sfg713 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-sfg713 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-0epdn9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0epdn9 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vos9jt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vos9jt -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1hyenf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1hyenf -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-pw44yp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pw44yp -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-nn2bg1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nn2bg1 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-p79zpy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-p79zpy -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-5pz249 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5pz249 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-08gphq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-08gphq -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-36av4l -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-36av4l -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-3o3e3g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3o3e3g -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-aoeece -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-aoeece -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-rmq74z -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-rmq74z -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-92zr4x -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-92zr4x -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-46q7ud -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-46q7ud -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-1qpnby -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1qpnby -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-xdr3fl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xdr3fl -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-mofxo1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-mofxo1 -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-ziwi5a -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ziwi5a -o jsonpath='{.spec.containers[*].name}' | grep c2\nkubectl get pod ckad-vmqmvj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vmqmvj -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
    "setupScript": "kubectl delete pod ckad-sfg713 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-0epdn9 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vos9jt --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1hyenf --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-pw44yp --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-nn2bg1 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-p79zpy --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-5pz249 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-08gphq --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-36av4l --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-3o3e3g --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-aoeece --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-rmq74z --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-92zr4x --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-46q7ud --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-1qpnby --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-xdr3fl --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-mofxo1 --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-ziwi5a --force --grace-period=0 2>/dev/null || true\nkubectl delete pod ckad-vmqmvj --force --grace-period=0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKAD Practice (Batch 9)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Multi-Container Pod\nCreate a pod named `ckad-sfg713` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-sfg713 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-sfg713 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-sfg713 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Multi-Container Pod\nCreate a pod named `ckad-0epdn9` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-0epdn9 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-0epdn9 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-0epdn9 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Multi-Container Pod\nCreate a pod named `ckad-vos9jt` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vos9jt -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vos9jt -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vos9jt --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Multi-Container Pod\nCreate a pod named `ckad-1hyenf` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1hyenf -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1hyenf -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1hyenf --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Multi-Container Pod\nCreate a pod named `ckad-pw44yp` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-pw44yp -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-pw44yp -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-pw44yp --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Multi-Container Pod\nCreate a pod named `ckad-nn2bg1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-nn2bg1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-nn2bg1 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-nn2bg1 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Multi-Container Pod\nCreate a pod named `ckad-p79zpy` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-p79zpy -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-p79zpy -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-p79zpy --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Multi-Container Pod\nCreate a pod named `ckad-5pz249` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-5pz249 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-5pz249 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-5pz249 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Multi-Container Pod\nCreate a pod named `ckad-08gphq` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-08gphq -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-08gphq -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-08gphq --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Multi-Container Pod\nCreate a pod named `ckad-36av4l` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-36av4l -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-36av4l -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-36av4l --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Multi-Container Pod\nCreate a pod named `ckad-3o3e3g` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-3o3e3g -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-3o3e3g -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-3o3e3g --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Multi-Container Pod\nCreate a pod named `ckad-aoeece` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-aoeece -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-aoeece -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-aoeece --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Multi-Container Pod\nCreate a pod named `ckad-rmq74z` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-rmq74z -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-rmq74z -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-rmq74z --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Multi-Container Pod\nCreate a pod named `ckad-92zr4x` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-92zr4x -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-92zr4x -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-92zr4x --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Multi-Container Pod\nCreate a pod named `ckad-46q7ud` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-46q7ud -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-46q7ud -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-46q7ud --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Multi-Container Pod\nCreate a pod named `ckad-1qpnby` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-1qpnby -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-1qpnby -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-1qpnby --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Multi-Container Pod\nCreate a pod named `ckad-xdr3fl` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-xdr3fl -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-xdr3fl -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-xdr3fl --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Multi-Container Pod\nCreate a pod named `ckad-mofxo1` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-mofxo1 -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-mofxo1 -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-mofxo1 --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Multi-Container Pod\nCreate a pod named `ckad-ziwi5a` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-ziwi5a -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-ziwi5a -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-ziwi5a --force --grace-period=0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Multi-Container Pod\nCreate a pod named `ckad-vmqmvj` with two containers.\nContainer 1: image `nginx`, name `c1`.\nContainer 2: image `busybox`, name `c2`, command \"sleep 3600\".\n\n\n\n\n\n",
        "verify": "kubectl get pod ckad-vmqmvj -o jsonpath='{.spec.containers[*].name}' | grep c1\nkubectl get pod ckad-vmqmvj -o jsonpath='{.spec.containers[*].name}' | grep c2\n",
        "setup": "kubectl delete pod ckad-vmqmvj --force --grace-period=0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-1": {
    "id": "auto-cks-1",
    "title": "Auto CKS Practice (Batch 1)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 1)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-ine0m2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-tpmd71` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-oiha52` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-68yi3a` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-5wy505` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-11a93n` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-7cxu38` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-07t21u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-tnwtrz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-45ob59` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-uasbup` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-1n43cx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-r2teqm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-u12hgb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-oazp58` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-tkzmnq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-etkxrq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-e9quqa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-lhyq1i` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-roalk1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-ine0m2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ine0m2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ine0m2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-tpmd71 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tpmd71 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tpmd71 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-oiha52 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oiha52 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oiha52 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-68yi3a -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-68yi3a -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-68yi3a -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-5wy505 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5wy505 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5wy505 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-11a93n -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-11a93n -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-11a93n -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-7cxu38 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7cxu38 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7cxu38 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-07t21u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-07t21u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-07t21u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-tnwtrz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tnwtrz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tnwtrz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-45ob59 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-45ob59 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-45ob59 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-uasbup -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-uasbup -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-uasbup -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-1n43cx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1n43cx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1n43cx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-r2teqm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r2teqm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r2teqm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-u12hgb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-u12hgb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-u12hgb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-oazp58 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oazp58 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oazp58 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-tkzmnq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tkzmnq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tkzmnq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-etkxrq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-etkxrq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-etkxrq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-e9quqa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e9quqa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e9quqa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-lhyq1i -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lhyq1i -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lhyq1i -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-roalk1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-roalk1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-roalk1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-ine0m2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ine0m2 2>/dev/null || true\nkubectl run db-pod-cks-tpmd71 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tpmd71 2>/dev/null || true\nkubectl run db-pod-cks-oiha52 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oiha52 2>/dev/null || true\nkubectl run db-pod-cks-68yi3a --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-68yi3a 2>/dev/null || true\nkubectl run db-pod-cks-5wy505 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5wy505 2>/dev/null || true\nkubectl run db-pod-cks-11a93n --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-11a93n 2>/dev/null || true\nkubectl run db-pod-cks-7cxu38 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7cxu38 2>/dev/null || true\nkubectl run db-pod-cks-07t21u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-07t21u 2>/dev/null || true\nkubectl run db-pod-cks-tnwtrz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tnwtrz 2>/dev/null || true\nkubectl run db-pod-cks-45ob59 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-45ob59 2>/dev/null || true\nkubectl run db-pod-cks-uasbup --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-uasbup 2>/dev/null || true\nkubectl run db-pod-cks-1n43cx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1n43cx 2>/dev/null || true\nkubectl run db-pod-cks-r2teqm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r2teqm 2>/dev/null || true\nkubectl run db-pod-cks-u12hgb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-u12hgb 2>/dev/null || true\nkubectl run db-pod-cks-oazp58 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oazp58 2>/dev/null || true\nkubectl run db-pod-cks-tkzmnq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tkzmnq 2>/dev/null || true\nkubectl run db-pod-cks-etkxrq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-etkxrq 2>/dev/null || true\nkubectl run db-pod-cks-e9quqa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e9quqa 2>/dev/null || true\nkubectl run db-pod-cks-lhyq1i --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lhyq1i 2>/dev/null || true\nkubectl run db-pod-cks-roalk1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-roalk1 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 1)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-ine0m2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ine0m2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ine0m2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ine0m2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ine0m2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ine0m2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-tpmd71` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-tpmd71 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tpmd71 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tpmd71 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-tpmd71 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tpmd71 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-oiha52` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-oiha52 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oiha52 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oiha52 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-oiha52 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oiha52 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-68yi3a` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-68yi3a -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-68yi3a -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-68yi3a -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-68yi3a --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-68yi3a 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-5wy505` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-5wy505 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5wy505 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5wy505 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-5wy505 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5wy505 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-11a93n` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-11a93n -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-11a93n -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-11a93n -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-11a93n --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-11a93n 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-7cxu38` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-7cxu38 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7cxu38 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7cxu38 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-7cxu38 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7cxu38 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-07t21u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-07t21u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-07t21u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-07t21u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-07t21u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-07t21u 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-tnwtrz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-tnwtrz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tnwtrz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tnwtrz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-tnwtrz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tnwtrz 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-45ob59` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-45ob59 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-45ob59 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-45ob59 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-45ob59 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-45ob59 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-uasbup` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-uasbup -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-uasbup -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-uasbup -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-uasbup --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-uasbup 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-1n43cx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-1n43cx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1n43cx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1n43cx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-1n43cx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1n43cx 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-r2teqm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-r2teqm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r2teqm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r2teqm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-r2teqm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r2teqm 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-u12hgb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-u12hgb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-u12hgb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-u12hgb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-u12hgb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-u12hgb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-oazp58` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-oazp58 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oazp58 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oazp58 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-oazp58 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oazp58 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-tkzmnq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-tkzmnq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tkzmnq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tkzmnq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-tkzmnq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tkzmnq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-etkxrq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-etkxrq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-etkxrq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-etkxrq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-etkxrq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-etkxrq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-e9quqa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-e9quqa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e9quqa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e9quqa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-e9quqa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e9quqa 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-lhyq1i` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-lhyq1i -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lhyq1i -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lhyq1i -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-lhyq1i --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lhyq1i 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-roalk1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-roalk1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-roalk1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-roalk1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-roalk1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-roalk1 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-10": {
    "id": "auto-cks-10",
    "title": "Auto CKS Practice (Batch 10)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 10)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-wszopj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-p04qpi` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-j0jado` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-3dx8us` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-2hssz3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-vrvl7w` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-xkwaaw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-8q8jrn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-ea0qwe` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-5cl60c` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-q2q0cp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-w008tg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-6y391q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-hxo8xn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-asavsl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-81ab9k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-e85cu2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-m66u36` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-j8rpj1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-sijoi9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-wszopj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-wszopj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-wszopj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-p04qpi -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-p04qpi -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-p04qpi -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-j0jado -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-j0jado -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-j0jado -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3dx8us -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3dx8us -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3dx8us -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-2hssz3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2hssz3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2hssz3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-vrvl7w -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-vrvl7w -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-vrvl7w -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-xkwaaw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-xkwaaw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-xkwaaw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-8q8jrn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-8q8jrn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-8q8jrn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ea0qwe -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ea0qwe -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ea0qwe -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-5cl60c -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5cl60c -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5cl60c -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-q2q0cp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-q2q0cp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-q2q0cp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-w008tg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-w008tg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-w008tg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-6y391q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6y391q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6y391q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-hxo8xn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hxo8xn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hxo8xn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-asavsl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-asavsl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-asavsl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-81ab9k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-81ab9k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-81ab9k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-e85cu2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e85cu2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e85cu2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-m66u36 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-m66u36 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-m66u36 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-j8rpj1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-j8rpj1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-j8rpj1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-sijoi9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sijoi9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sijoi9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-wszopj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-wszopj 2>/dev/null || true\nkubectl run db-pod-cks-p04qpi --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-p04qpi 2>/dev/null || true\nkubectl run db-pod-cks-j0jado --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-j0jado 2>/dev/null || true\nkubectl run db-pod-cks-3dx8us --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3dx8us 2>/dev/null || true\nkubectl run db-pod-cks-2hssz3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2hssz3 2>/dev/null || true\nkubectl run db-pod-cks-vrvl7w --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-vrvl7w 2>/dev/null || true\nkubectl run db-pod-cks-xkwaaw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-xkwaaw 2>/dev/null || true\nkubectl run db-pod-cks-8q8jrn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-8q8jrn 2>/dev/null || true\nkubectl run db-pod-cks-ea0qwe --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ea0qwe 2>/dev/null || true\nkubectl run db-pod-cks-5cl60c --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5cl60c 2>/dev/null || true\nkubectl run db-pod-cks-q2q0cp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-q2q0cp 2>/dev/null || true\nkubectl run db-pod-cks-w008tg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-w008tg 2>/dev/null || true\nkubectl run db-pod-cks-6y391q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6y391q 2>/dev/null || true\nkubectl run db-pod-cks-hxo8xn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hxo8xn 2>/dev/null || true\nkubectl run db-pod-cks-asavsl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-asavsl 2>/dev/null || true\nkubectl run db-pod-cks-81ab9k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-81ab9k 2>/dev/null || true\nkubectl run db-pod-cks-e85cu2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e85cu2 2>/dev/null || true\nkubectl run db-pod-cks-m66u36 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-m66u36 2>/dev/null || true\nkubectl run db-pod-cks-j8rpj1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-j8rpj1 2>/dev/null || true\nkubectl run db-pod-cks-sijoi9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sijoi9 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 10)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-wszopj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-wszopj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-wszopj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-wszopj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-wszopj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-wszopj 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-p04qpi` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-p04qpi -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-p04qpi -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-p04qpi -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-p04qpi --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-p04qpi 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-j0jado` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-j0jado -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-j0jado -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-j0jado -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-j0jado --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-j0jado 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-3dx8us` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3dx8us -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3dx8us -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3dx8us -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3dx8us --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3dx8us 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-2hssz3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-2hssz3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2hssz3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2hssz3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-2hssz3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2hssz3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-vrvl7w` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-vrvl7w -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-vrvl7w -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-vrvl7w -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-vrvl7w --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-vrvl7w 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-xkwaaw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-xkwaaw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-xkwaaw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-xkwaaw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-xkwaaw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-xkwaaw 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-8q8jrn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-8q8jrn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-8q8jrn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-8q8jrn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-8q8jrn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-8q8jrn 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-ea0qwe` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ea0qwe -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ea0qwe -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ea0qwe -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ea0qwe --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ea0qwe 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-5cl60c` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-5cl60c -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5cl60c -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5cl60c -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-5cl60c --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5cl60c 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-q2q0cp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-q2q0cp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-q2q0cp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-q2q0cp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-q2q0cp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-q2q0cp 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-w008tg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-w008tg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-w008tg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-w008tg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-w008tg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-w008tg 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-6y391q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-6y391q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6y391q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6y391q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-6y391q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6y391q 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-hxo8xn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-hxo8xn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hxo8xn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hxo8xn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-hxo8xn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hxo8xn 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-asavsl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-asavsl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-asavsl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-asavsl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-asavsl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-asavsl 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-81ab9k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-81ab9k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-81ab9k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-81ab9k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-81ab9k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-81ab9k 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-e85cu2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-e85cu2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e85cu2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e85cu2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-e85cu2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e85cu2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-m66u36` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-m66u36 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-m66u36 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-m66u36 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-m66u36 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-m66u36 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-j8rpj1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-j8rpj1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-j8rpj1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-j8rpj1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-j8rpj1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-j8rpj1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-sijoi9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-sijoi9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sijoi9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sijoi9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-sijoi9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sijoi9 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-11": {
    "id": "auto-cks-11",
    "title": "Auto CKS Practice (Batch 11)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 11)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-js2clv` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-drccor` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-rzwrxa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-88iny9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-kgufco` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-3bw8eg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-zz0l99` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-3enh2l` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-gfdz5w` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-32cy7k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-gvnugg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-217qaq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-exjk5x` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-ok0ojy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-3vd8ai` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-2je9q6` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-h0m65z` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-2m0fhz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-fsdk1v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-aern1g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-js2clv -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-js2clv -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-js2clv -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-drccor -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-drccor -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-drccor -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-rzwrxa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rzwrxa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rzwrxa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-88iny9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-88iny9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-88iny9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-kgufco -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kgufco -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kgufco -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3bw8eg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3bw8eg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3bw8eg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-zz0l99 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-zz0l99 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-zz0l99 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3enh2l -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3enh2l -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3enh2l -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-gfdz5w -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gfdz5w -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gfdz5w -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-32cy7k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-32cy7k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-32cy7k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-gvnugg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gvnugg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gvnugg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-217qaq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-217qaq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-217qaq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-exjk5x -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-exjk5x -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-exjk5x -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ok0ojy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ok0ojy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ok0ojy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3vd8ai -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3vd8ai -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3vd8ai -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-2je9q6 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2je9q6 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2je9q6 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-h0m65z -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-h0m65z -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-h0m65z -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-2m0fhz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2m0fhz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2m0fhz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-fsdk1v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-fsdk1v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-fsdk1v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-aern1g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-aern1g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-aern1g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-js2clv --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-js2clv 2>/dev/null || true\nkubectl run db-pod-cks-drccor --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-drccor 2>/dev/null || true\nkubectl run db-pod-cks-rzwrxa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rzwrxa 2>/dev/null || true\nkubectl run db-pod-cks-88iny9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-88iny9 2>/dev/null || true\nkubectl run db-pod-cks-kgufco --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kgufco 2>/dev/null || true\nkubectl run db-pod-cks-3bw8eg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3bw8eg 2>/dev/null || true\nkubectl run db-pod-cks-zz0l99 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-zz0l99 2>/dev/null || true\nkubectl run db-pod-cks-3enh2l --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3enh2l 2>/dev/null || true\nkubectl run db-pod-cks-gfdz5w --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gfdz5w 2>/dev/null || true\nkubectl run db-pod-cks-32cy7k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-32cy7k 2>/dev/null || true\nkubectl run db-pod-cks-gvnugg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gvnugg 2>/dev/null || true\nkubectl run db-pod-cks-217qaq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-217qaq 2>/dev/null || true\nkubectl run db-pod-cks-exjk5x --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-exjk5x 2>/dev/null || true\nkubectl run db-pod-cks-ok0ojy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ok0ojy 2>/dev/null || true\nkubectl run db-pod-cks-3vd8ai --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3vd8ai 2>/dev/null || true\nkubectl run db-pod-cks-2je9q6 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2je9q6 2>/dev/null || true\nkubectl run db-pod-cks-h0m65z --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-h0m65z 2>/dev/null || true\nkubectl run db-pod-cks-2m0fhz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2m0fhz 2>/dev/null || true\nkubectl run db-pod-cks-fsdk1v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-fsdk1v 2>/dev/null || true\nkubectl run db-pod-cks-aern1g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-aern1g 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 11)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-js2clv` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-js2clv -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-js2clv -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-js2clv -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-js2clv --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-js2clv 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-drccor` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-drccor -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-drccor -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-drccor -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-drccor --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-drccor 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-rzwrxa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-rzwrxa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rzwrxa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rzwrxa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-rzwrxa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rzwrxa 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-88iny9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-88iny9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-88iny9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-88iny9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-88iny9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-88iny9 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-kgufco` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-kgufco -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kgufco -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kgufco -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-kgufco --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kgufco 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-3bw8eg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3bw8eg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3bw8eg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3bw8eg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3bw8eg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3bw8eg 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-zz0l99` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-zz0l99 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-zz0l99 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-zz0l99 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-zz0l99 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-zz0l99 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-3enh2l` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3enh2l -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3enh2l -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3enh2l -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3enh2l --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3enh2l 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-gfdz5w` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-gfdz5w -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gfdz5w -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gfdz5w -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-gfdz5w --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gfdz5w 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-32cy7k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-32cy7k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-32cy7k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-32cy7k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-32cy7k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-32cy7k 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-gvnugg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-gvnugg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gvnugg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gvnugg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-gvnugg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gvnugg 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-217qaq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-217qaq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-217qaq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-217qaq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-217qaq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-217qaq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-exjk5x` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-exjk5x -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-exjk5x -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-exjk5x -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-exjk5x --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-exjk5x 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-ok0ojy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ok0ojy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ok0ojy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ok0ojy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ok0ojy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ok0ojy 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-3vd8ai` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3vd8ai -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3vd8ai -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3vd8ai -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3vd8ai --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3vd8ai 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-2je9q6` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-2je9q6 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2je9q6 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2je9q6 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-2je9q6 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2je9q6 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-h0m65z` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-h0m65z -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-h0m65z -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-h0m65z -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-h0m65z --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-h0m65z 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-2m0fhz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-2m0fhz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2m0fhz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2m0fhz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-2m0fhz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2m0fhz 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-fsdk1v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-fsdk1v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-fsdk1v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-fsdk1v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-fsdk1v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-fsdk1v 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-aern1g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-aern1g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-aern1g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-aern1g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-aern1g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-aern1g 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-12": {
    "id": "auto-cks-12",
    "title": "Auto CKS Practice (Batch 12)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 12)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-bhsn52` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-0l4ziu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-srxerj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-3j9fdw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-35ftyy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-9592qk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-gz4h4v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-xmujwd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-p6y0xp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-uty5ck` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-dkxt3p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-d3tp25` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-i4y9l3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-a5b7pb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-jti0lh` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-rpg6eg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-7tys98` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-l04x5b` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-78x2i7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-lwr2l7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-bhsn52 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bhsn52 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bhsn52 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-0l4ziu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0l4ziu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0l4ziu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-srxerj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-srxerj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-srxerj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3j9fdw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3j9fdw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3j9fdw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-35ftyy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-35ftyy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-35ftyy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-9592qk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9592qk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9592qk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-gz4h4v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gz4h4v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gz4h4v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-xmujwd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-xmujwd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-xmujwd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-p6y0xp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-p6y0xp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-p6y0xp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-uty5ck -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-uty5ck -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-uty5ck -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-dkxt3p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dkxt3p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dkxt3p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-d3tp25 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-d3tp25 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-d3tp25 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-i4y9l3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-i4y9l3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-i4y9l3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-a5b7pb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-a5b7pb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-a5b7pb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-jti0lh -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jti0lh -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jti0lh -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-rpg6eg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rpg6eg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rpg6eg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-7tys98 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7tys98 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7tys98 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-l04x5b -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-l04x5b -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-l04x5b -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-78x2i7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-78x2i7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-78x2i7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-lwr2l7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lwr2l7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lwr2l7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-bhsn52 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bhsn52 2>/dev/null || true\nkubectl run db-pod-cks-0l4ziu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0l4ziu 2>/dev/null || true\nkubectl run db-pod-cks-srxerj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-srxerj 2>/dev/null || true\nkubectl run db-pod-cks-3j9fdw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3j9fdw 2>/dev/null || true\nkubectl run db-pod-cks-35ftyy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-35ftyy 2>/dev/null || true\nkubectl run db-pod-cks-9592qk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9592qk 2>/dev/null || true\nkubectl run db-pod-cks-gz4h4v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gz4h4v 2>/dev/null || true\nkubectl run db-pod-cks-xmujwd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-xmujwd 2>/dev/null || true\nkubectl run db-pod-cks-p6y0xp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-p6y0xp 2>/dev/null || true\nkubectl run db-pod-cks-uty5ck --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-uty5ck 2>/dev/null || true\nkubectl run db-pod-cks-dkxt3p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dkxt3p 2>/dev/null || true\nkubectl run db-pod-cks-d3tp25 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-d3tp25 2>/dev/null || true\nkubectl run db-pod-cks-i4y9l3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-i4y9l3 2>/dev/null || true\nkubectl run db-pod-cks-a5b7pb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-a5b7pb 2>/dev/null || true\nkubectl run db-pod-cks-jti0lh --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jti0lh 2>/dev/null || true\nkubectl run db-pod-cks-rpg6eg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rpg6eg 2>/dev/null || true\nkubectl run db-pod-cks-7tys98 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7tys98 2>/dev/null || true\nkubectl run db-pod-cks-l04x5b --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-l04x5b 2>/dev/null || true\nkubectl run db-pod-cks-78x2i7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-78x2i7 2>/dev/null || true\nkubectl run db-pod-cks-lwr2l7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lwr2l7 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 12)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-bhsn52` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-bhsn52 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bhsn52 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bhsn52 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-bhsn52 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bhsn52 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-0l4ziu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-0l4ziu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0l4ziu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0l4ziu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-0l4ziu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0l4ziu 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-srxerj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-srxerj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-srxerj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-srxerj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-srxerj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-srxerj 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-3j9fdw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3j9fdw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3j9fdw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3j9fdw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3j9fdw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3j9fdw 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-35ftyy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-35ftyy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-35ftyy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-35ftyy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-35ftyy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-35ftyy 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-9592qk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-9592qk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9592qk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9592qk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-9592qk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9592qk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-gz4h4v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-gz4h4v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gz4h4v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gz4h4v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-gz4h4v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gz4h4v 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-xmujwd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-xmujwd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-xmujwd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-xmujwd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-xmujwd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-xmujwd 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-p6y0xp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-p6y0xp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-p6y0xp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-p6y0xp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-p6y0xp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-p6y0xp 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-uty5ck` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-uty5ck -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-uty5ck -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-uty5ck -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-uty5ck --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-uty5ck 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-dkxt3p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-dkxt3p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dkxt3p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dkxt3p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-dkxt3p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dkxt3p 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-d3tp25` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-d3tp25 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-d3tp25 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-d3tp25 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-d3tp25 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-d3tp25 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-i4y9l3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-i4y9l3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-i4y9l3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-i4y9l3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-i4y9l3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-i4y9l3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-a5b7pb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-a5b7pb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-a5b7pb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-a5b7pb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-a5b7pb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-a5b7pb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-jti0lh` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-jti0lh -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jti0lh -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jti0lh -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-jti0lh --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jti0lh 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-rpg6eg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-rpg6eg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rpg6eg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rpg6eg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-rpg6eg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rpg6eg 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-7tys98` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-7tys98 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7tys98 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7tys98 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-7tys98 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7tys98 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-l04x5b` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-l04x5b -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-l04x5b -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-l04x5b -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-l04x5b --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-l04x5b 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-78x2i7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-78x2i7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-78x2i7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-78x2i7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-78x2i7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-78x2i7 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-lwr2l7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-lwr2l7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lwr2l7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lwr2l7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-lwr2l7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lwr2l7 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-13": {
    "id": "auto-cks-13",
    "title": "Auto CKS Practice (Batch 13)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 13)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-e9g1a0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-ay7qkp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-om39qa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-iwho66` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-dsj08x` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-sxvndk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-tv1vqb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-quhqha` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-im8uwj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-mjv6uk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-3pac2t` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-7lswc1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-ccz1br` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-0o97j8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-t3cbni` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-iqdj0k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-imrq8v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-n19avl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-vixoq1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-iszibp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-e9g1a0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e9g1a0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e9g1a0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ay7qkp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ay7qkp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ay7qkp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-om39qa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-om39qa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-om39qa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-iwho66 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iwho66 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iwho66 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-dsj08x -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dsj08x -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dsj08x -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-sxvndk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sxvndk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sxvndk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-tv1vqb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tv1vqb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tv1vqb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-quhqha -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-quhqha -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-quhqha -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-im8uwj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-im8uwj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-im8uwj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-mjv6uk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-mjv6uk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-mjv6uk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3pac2t -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3pac2t -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3pac2t -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-7lswc1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7lswc1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7lswc1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ccz1br -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ccz1br -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ccz1br -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-0o97j8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0o97j8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0o97j8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-t3cbni -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t3cbni -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t3cbni -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-iqdj0k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iqdj0k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iqdj0k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-imrq8v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-imrq8v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-imrq8v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-n19avl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n19avl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n19avl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-vixoq1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-vixoq1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-vixoq1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-iszibp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iszibp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iszibp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-e9g1a0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e9g1a0 2>/dev/null || true\nkubectl run db-pod-cks-ay7qkp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ay7qkp 2>/dev/null || true\nkubectl run db-pod-cks-om39qa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-om39qa 2>/dev/null || true\nkubectl run db-pod-cks-iwho66 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iwho66 2>/dev/null || true\nkubectl run db-pod-cks-dsj08x --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dsj08x 2>/dev/null || true\nkubectl run db-pod-cks-sxvndk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sxvndk 2>/dev/null || true\nkubectl run db-pod-cks-tv1vqb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tv1vqb 2>/dev/null || true\nkubectl run db-pod-cks-quhqha --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-quhqha 2>/dev/null || true\nkubectl run db-pod-cks-im8uwj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-im8uwj 2>/dev/null || true\nkubectl run db-pod-cks-mjv6uk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-mjv6uk 2>/dev/null || true\nkubectl run db-pod-cks-3pac2t --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3pac2t 2>/dev/null || true\nkubectl run db-pod-cks-7lswc1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7lswc1 2>/dev/null || true\nkubectl run db-pod-cks-ccz1br --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ccz1br 2>/dev/null || true\nkubectl run db-pod-cks-0o97j8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0o97j8 2>/dev/null || true\nkubectl run db-pod-cks-t3cbni --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t3cbni 2>/dev/null || true\nkubectl run db-pod-cks-iqdj0k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iqdj0k 2>/dev/null || true\nkubectl run db-pod-cks-imrq8v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-imrq8v 2>/dev/null || true\nkubectl run db-pod-cks-n19avl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n19avl 2>/dev/null || true\nkubectl run db-pod-cks-vixoq1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-vixoq1 2>/dev/null || true\nkubectl run db-pod-cks-iszibp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iszibp 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 13)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-e9g1a0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-e9g1a0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e9g1a0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e9g1a0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-e9g1a0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e9g1a0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-ay7qkp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ay7qkp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ay7qkp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ay7qkp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ay7qkp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ay7qkp 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-om39qa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-om39qa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-om39qa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-om39qa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-om39qa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-om39qa 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-iwho66` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-iwho66 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iwho66 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iwho66 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-iwho66 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iwho66 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-dsj08x` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-dsj08x -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dsj08x -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dsj08x -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-dsj08x --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dsj08x 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-sxvndk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-sxvndk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sxvndk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sxvndk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-sxvndk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sxvndk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-tv1vqb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-tv1vqb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tv1vqb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tv1vqb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-tv1vqb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tv1vqb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-quhqha` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-quhqha -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-quhqha -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-quhqha -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-quhqha --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-quhqha 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-im8uwj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-im8uwj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-im8uwj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-im8uwj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-im8uwj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-im8uwj 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-mjv6uk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-mjv6uk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-mjv6uk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-mjv6uk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-mjv6uk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-mjv6uk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-3pac2t` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3pac2t -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3pac2t -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3pac2t -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3pac2t --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3pac2t 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-7lswc1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-7lswc1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7lswc1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7lswc1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-7lswc1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7lswc1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-ccz1br` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ccz1br -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ccz1br -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ccz1br -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ccz1br --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ccz1br 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-0o97j8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-0o97j8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0o97j8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0o97j8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-0o97j8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0o97j8 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-t3cbni` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-t3cbni -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t3cbni -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t3cbni -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-t3cbni --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t3cbni 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-iqdj0k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-iqdj0k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iqdj0k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iqdj0k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-iqdj0k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iqdj0k 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-imrq8v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-imrq8v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-imrq8v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-imrq8v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-imrq8v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-imrq8v 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-n19avl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-n19avl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n19avl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n19avl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-n19avl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n19avl 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-vixoq1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-vixoq1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-vixoq1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-vixoq1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-vixoq1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-vixoq1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-iszibp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-iszibp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iszibp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iszibp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-iszibp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iszibp 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-14": {
    "id": "auto-cks-14",
    "title": "Auto CKS Practice (Batch 14)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 14)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-afhx42` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-iah9ku` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-1eon1e` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-h2w3zr` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-lpthl7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-3dp635` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-4y7rff` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-guc6d1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-so66a1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-t2zspl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-ie0azd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-55lve1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-rmp7np` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-hhnaht` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-896fwa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-r4ddal` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-6t2qzq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-tj1y4e` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-s7uaps` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-2mao1m` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-afhx42 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-afhx42 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-afhx42 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-iah9ku -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iah9ku -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iah9ku -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-1eon1e -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1eon1e -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1eon1e -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-h2w3zr -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-h2w3zr -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-h2w3zr -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-lpthl7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lpthl7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lpthl7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3dp635 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3dp635 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3dp635 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-4y7rff -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4y7rff -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4y7rff -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-guc6d1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-guc6d1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-guc6d1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-so66a1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-so66a1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-so66a1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-t2zspl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t2zspl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t2zspl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ie0azd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ie0azd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ie0azd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-55lve1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-55lve1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-55lve1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-rmp7np -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rmp7np -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rmp7np -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-hhnaht -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hhnaht -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hhnaht -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-896fwa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-896fwa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-896fwa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-r4ddal -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r4ddal -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r4ddal -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-6t2qzq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6t2qzq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6t2qzq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-tj1y4e -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tj1y4e -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tj1y4e -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-s7uaps -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-s7uaps -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-s7uaps -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-2mao1m -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2mao1m -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2mao1m -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-afhx42 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-afhx42 2>/dev/null || true\nkubectl run db-pod-cks-iah9ku --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iah9ku 2>/dev/null || true\nkubectl run db-pod-cks-1eon1e --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1eon1e 2>/dev/null || true\nkubectl run db-pod-cks-h2w3zr --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-h2w3zr 2>/dev/null || true\nkubectl run db-pod-cks-lpthl7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lpthl7 2>/dev/null || true\nkubectl run db-pod-cks-3dp635 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3dp635 2>/dev/null || true\nkubectl run db-pod-cks-4y7rff --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4y7rff 2>/dev/null || true\nkubectl run db-pod-cks-guc6d1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-guc6d1 2>/dev/null || true\nkubectl run db-pod-cks-so66a1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-so66a1 2>/dev/null || true\nkubectl run db-pod-cks-t2zspl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t2zspl 2>/dev/null || true\nkubectl run db-pod-cks-ie0azd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ie0azd 2>/dev/null || true\nkubectl run db-pod-cks-55lve1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-55lve1 2>/dev/null || true\nkubectl run db-pod-cks-rmp7np --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rmp7np 2>/dev/null || true\nkubectl run db-pod-cks-hhnaht --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hhnaht 2>/dev/null || true\nkubectl run db-pod-cks-896fwa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-896fwa 2>/dev/null || true\nkubectl run db-pod-cks-r4ddal --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r4ddal 2>/dev/null || true\nkubectl run db-pod-cks-6t2qzq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6t2qzq 2>/dev/null || true\nkubectl run db-pod-cks-tj1y4e --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tj1y4e 2>/dev/null || true\nkubectl run db-pod-cks-s7uaps --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-s7uaps 2>/dev/null || true\nkubectl run db-pod-cks-2mao1m --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2mao1m 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 14)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-afhx42` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-afhx42 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-afhx42 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-afhx42 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-afhx42 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-afhx42 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-iah9ku` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-iah9ku -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iah9ku -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iah9ku -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-iah9ku --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iah9ku 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-1eon1e` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-1eon1e -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1eon1e -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1eon1e -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-1eon1e --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1eon1e 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-h2w3zr` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-h2w3zr -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-h2w3zr -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-h2w3zr -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-h2w3zr --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-h2w3zr 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-lpthl7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-lpthl7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lpthl7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lpthl7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-lpthl7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lpthl7 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-3dp635` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3dp635 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3dp635 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3dp635 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3dp635 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3dp635 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-4y7rff` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-4y7rff -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4y7rff -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4y7rff -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-4y7rff --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4y7rff 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-guc6d1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-guc6d1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-guc6d1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-guc6d1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-guc6d1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-guc6d1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-so66a1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-so66a1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-so66a1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-so66a1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-so66a1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-so66a1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-t2zspl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-t2zspl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t2zspl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t2zspl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-t2zspl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t2zspl 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-ie0azd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ie0azd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ie0azd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ie0azd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ie0azd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ie0azd 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-55lve1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-55lve1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-55lve1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-55lve1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-55lve1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-55lve1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-rmp7np` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-rmp7np -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rmp7np -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rmp7np -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-rmp7np --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rmp7np 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-hhnaht` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-hhnaht -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hhnaht -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hhnaht -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-hhnaht --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hhnaht 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-896fwa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-896fwa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-896fwa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-896fwa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-896fwa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-896fwa 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-r4ddal` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-r4ddal -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r4ddal -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r4ddal -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-r4ddal --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r4ddal 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-6t2qzq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-6t2qzq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6t2qzq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6t2qzq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-6t2qzq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6t2qzq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-tj1y4e` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-tj1y4e -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tj1y4e -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tj1y4e -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-tj1y4e --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tj1y4e 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-s7uaps` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-s7uaps -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-s7uaps -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-s7uaps -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-s7uaps --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-s7uaps 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-2mao1m` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-2mao1m -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2mao1m -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2mao1m -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-2mao1m --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2mao1m 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-15": {
    "id": "auto-cks-15",
    "title": "Auto CKS Practice (Batch 15)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 15)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-r3s0jk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-2ltmjg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-pnrbz1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-cw3h6k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-nu3oys` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-czhih3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-qyq06u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-icupv3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-6opfnu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-52hsnw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-u74u9n` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-6a2r1a` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-glaq77` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-97ut3o` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-hu6fqz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-dkryxm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-9h7p8g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-dfijv5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-31pwdx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-nk6c6s` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-r3s0jk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r3s0jk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r3s0jk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-2ltmjg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2ltmjg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2ltmjg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-pnrbz1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-pnrbz1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-pnrbz1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-cw3h6k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cw3h6k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cw3h6k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-nu3oys -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nu3oys -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nu3oys -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-czhih3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-czhih3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-czhih3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-qyq06u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-qyq06u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-qyq06u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-icupv3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-icupv3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-icupv3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-6opfnu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6opfnu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6opfnu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-52hsnw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-52hsnw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-52hsnw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-u74u9n -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-u74u9n -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-u74u9n -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-6a2r1a -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6a2r1a -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6a2r1a -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-glaq77 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-glaq77 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-glaq77 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-97ut3o -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-97ut3o -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-97ut3o -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-hu6fqz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hu6fqz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hu6fqz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-dkryxm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dkryxm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dkryxm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-9h7p8g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9h7p8g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9h7p8g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-dfijv5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dfijv5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dfijv5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-31pwdx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-31pwdx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-31pwdx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-nk6c6s -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nk6c6s -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nk6c6s -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-r3s0jk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r3s0jk 2>/dev/null || true\nkubectl run db-pod-cks-2ltmjg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2ltmjg 2>/dev/null || true\nkubectl run db-pod-cks-pnrbz1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-pnrbz1 2>/dev/null || true\nkubectl run db-pod-cks-cw3h6k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cw3h6k 2>/dev/null || true\nkubectl run db-pod-cks-nu3oys --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nu3oys 2>/dev/null || true\nkubectl run db-pod-cks-czhih3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-czhih3 2>/dev/null || true\nkubectl run db-pod-cks-qyq06u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-qyq06u 2>/dev/null || true\nkubectl run db-pod-cks-icupv3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-icupv3 2>/dev/null || true\nkubectl run db-pod-cks-6opfnu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6opfnu 2>/dev/null || true\nkubectl run db-pod-cks-52hsnw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-52hsnw 2>/dev/null || true\nkubectl run db-pod-cks-u74u9n --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-u74u9n 2>/dev/null || true\nkubectl run db-pod-cks-6a2r1a --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6a2r1a 2>/dev/null || true\nkubectl run db-pod-cks-glaq77 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-glaq77 2>/dev/null || true\nkubectl run db-pod-cks-97ut3o --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-97ut3o 2>/dev/null || true\nkubectl run db-pod-cks-hu6fqz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hu6fqz 2>/dev/null || true\nkubectl run db-pod-cks-dkryxm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dkryxm 2>/dev/null || true\nkubectl run db-pod-cks-9h7p8g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9h7p8g 2>/dev/null || true\nkubectl run db-pod-cks-dfijv5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dfijv5 2>/dev/null || true\nkubectl run db-pod-cks-31pwdx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-31pwdx 2>/dev/null || true\nkubectl run db-pod-cks-nk6c6s --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nk6c6s 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 15)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-r3s0jk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-r3s0jk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r3s0jk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r3s0jk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-r3s0jk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r3s0jk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-2ltmjg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-2ltmjg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2ltmjg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2ltmjg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-2ltmjg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2ltmjg 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-pnrbz1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-pnrbz1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-pnrbz1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-pnrbz1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-pnrbz1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-pnrbz1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-cw3h6k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-cw3h6k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cw3h6k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cw3h6k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-cw3h6k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cw3h6k 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-nu3oys` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-nu3oys -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nu3oys -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nu3oys -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-nu3oys --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nu3oys 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-czhih3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-czhih3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-czhih3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-czhih3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-czhih3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-czhih3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-qyq06u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-qyq06u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-qyq06u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-qyq06u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-qyq06u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-qyq06u 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-icupv3` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-icupv3 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-icupv3 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-icupv3 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-icupv3 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-icupv3 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-6opfnu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-6opfnu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6opfnu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6opfnu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-6opfnu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6opfnu 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-52hsnw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-52hsnw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-52hsnw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-52hsnw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-52hsnw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-52hsnw 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-u74u9n` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-u74u9n -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-u74u9n -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-u74u9n -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-u74u9n --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-u74u9n 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-6a2r1a` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-6a2r1a -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6a2r1a -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6a2r1a -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-6a2r1a --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6a2r1a 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-glaq77` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-glaq77 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-glaq77 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-glaq77 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-glaq77 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-glaq77 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-97ut3o` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-97ut3o -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-97ut3o -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-97ut3o -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-97ut3o --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-97ut3o 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-hu6fqz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-hu6fqz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hu6fqz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hu6fqz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-hu6fqz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hu6fqz 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-dkryxm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-dkryxm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dkryxm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dkryxm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-dkryxm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dkryxm 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-9h7p8g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-9h7p8g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9h7p8g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9h7p8g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-9h7p8g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9h7p8g 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-dfijv5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-dfijv5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dfijv5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dfijv5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-dfijv5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dfijv5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-31pwdx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-31pwdx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-31pwdx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-31pwdx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-31pwdx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-31pwdx 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-nk6c6s` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-nk6c6s -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nk6c6s -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nk6c6s -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-nk6c6s --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nk6c6s 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-2": {
    "id": "auto-cks-2",
    "title": "Auto CKS Practice (Batch 2)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 2)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-fpvyex` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-ig3dr8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-kvynnq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-6ek222` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-0rg0v4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-hyjvsz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-rgcou5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-jcwixa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-jnku8m` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-bgo5a0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-14jifb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-ib0thf` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-8e9erg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-ydc1b2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-pffe03` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-7vb1es` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-4u7ps2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-n8gm7u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-x2jvqy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-eqe4x4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-fpvyex -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-fpvyex -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-fpvyex -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ig3dr8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ig3dr8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ig3dr8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-kvynnq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kvynnq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kvynnq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-6ek222 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6ek222 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6ek222 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-0rg0v4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0rg0v4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0rg0v4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-hyjvsz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hyjvsz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hyjvsz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-rgcou5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rgcou5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rgcou5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-jcwixa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jcwixa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jcwixa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-jnku8m -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jnku8m -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jnku8m -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-bgo5a0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bgo5a0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bgo5a0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-14jifb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-14jifb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-14jifb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ib0thf -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ib0thf -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ib0thf -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-8e9erg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-8e9erg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-8e9erg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ydc1b2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ydc1b2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ydc1b2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-pffe03 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-pffe03 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-pffe03 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-7vb1es -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7vb1es -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7vb1es -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-4u7ps2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4u7ps2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4u7ps2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-n8gm7u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n8gm7u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n8gm7u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-x2jvqy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-x2jvqy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-x2jvqy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-eqe4x4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-eqe4x4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-eqe4x4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-fpvyex --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-fpvyex 2>/dev/null || true\nkubectl run db-pod-cks-ig3dr8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ig3dr8 2>/dev/null || true\nkubectl run db-pod-cks-kvynnq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kvynnq 2>/dev/null || true\nkubectl run db-pod-cks-6ek222 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6ek222 2>/dev/null || true\nkubectl run db-pod-cks-0rg0v4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0rg0v4 2>/dev/null || true\nkubectl run db-pod-cks-hyjvsz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hyjvsz 2>/dev/null || true\nkubectl run db-pod-cks-rgcou5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rgcou5 2>/dev/null || true\nkubectl run db-pod-cks-jcwixa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jcwixa 2>/dev/null || true\nkubectl run db-pod-cks-jnku8m --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jnku8m 2>/dev/null || true\nkubectl run db-pod-cks-bgo5a0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bgo5a0 2>/dev/null || true\nkubectl run db-pod-cks-14jifb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-14jifb 2>/dev/null || true\nkubectl run db-pod-cks-ib0thf --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ib0thf 2>/dev/null || true\nkubectl run db-pod-cks-8e9erg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-8e9erg 2>/dev/null || true\nkubectl run db-pod-cks-ydc1b2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ydc1b2 2>/dev/null || true\nkubectl run db-pod-cks-pffe03 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-pffe03 2>/dev/null || true\nkubectl run db-pod-cks-7vb1es --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7vb1es 2>/dev/null || true\nkubectl run db-pod-cks-4u7ps2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4u7ps2 2>/dev/null || true\nkubectl run db-pod-cks-n8gm7u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n8gm7u 2>/dev/null || true\nkubectl run db-pod-cks-x2jvqy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-x2jvqy 2>/dev/null || true\nkubectl run db-pod-cks-eqe4x4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-eqe4x4 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 2)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-fpvyex` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-fpvyex -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-fpvyex -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-fpvyex -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-fpvyex --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-fpvyex 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-ig3dr8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ig3dr8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ig3dr8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ig3dr8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ig3dr8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ig3dr8 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-kvynnq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-kvynnq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kvynnq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kvynnq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-kvynnq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kvynnq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-6ek222` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-6ek222 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6ek222 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6ek222 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-6ek222 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6ek222 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-0rg0v4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-0rg0v4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0rg0v4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0rg0v4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-0rg0v4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0rg0v4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-hyjvsz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-hyjvsz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hyjvsz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hyjvsz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-hyjvsz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hyjvsz 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-rgcou5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-rgcou5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rgcou5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rgcou5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-rgcou5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rgcou5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-jcwixa` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-jcwixa -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jcwixa -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jcwixa -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-jcwixa --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jcwixa 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-jnku8m` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-jnku8m -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jnku8m -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jnku8m -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-jnku8m --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jnku8m 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-bgo5a0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-bgo5a0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bgo5a0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bgo5a0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-bgo5a0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bgo5a0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-14jifb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-14jifb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-14jifb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-14jifb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-14jifb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-14jifb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-ib0thf` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ib0thf -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ib0thf -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ib0thf -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ib0thf --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ib0thf 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-8e9erg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-8e9erg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-8e9erg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-8e9erg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-8e9erg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-8e9erg 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-ydc1b2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ydc1b2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ydc1b2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ydc1b2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ydc1b2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ydc1b2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-pffe03` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-pffe03 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-pffe03 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-pffe03 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-pffe03 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-pffe03 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-7vb1es` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-7vb1es -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7vb1es -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7vb1es -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-7vb1es --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7vb1es 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-4u7ps2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-4u7ps2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4u7ps2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4u7ps2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-4u7ps2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4u7ps2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-n8gm7u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-n8gm7u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n8gm7u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n8gm7u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-n8gm7u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n8gm7u 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-x2jvqy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-x2jvqy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-x2jvqy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-x2jvqy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-x2jvqy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-x2jvqy 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-eqe4x4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-eqe4x4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-eqe4x4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-eqe4x4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-eqe4x4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-eqe4x4 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-3": {
    "id": "auto-cks-3",
    "title": "Auto CKS Practice (Batch 3)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 3)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-h9y848` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-gdjg87` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-ol8lwk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-zoqqz5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-diwhyj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-255m3g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-9ru34d` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-glmo0p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-o8dxx1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-bk229p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-9gk1jr` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-va51o8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-kp36bs` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-moeh54` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-nvvzg0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-y4zatx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-q15911` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-k779xb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-yhcmvc` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-ubuibi` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-h9y848 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-h9y848 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-h9y848 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-gdjg87 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gdjg87 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gdjg87 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ol8lwk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ol8lwk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ol8lwk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-zoqqz5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-zoqqz5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-zoqqz5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-diwhyj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-diwhyj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-diwhyj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-255m3g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-255m3g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-255m3g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-9ru34d -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9ru34d -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9ru34d -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-glmo0p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-glmo0p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-glmo0p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-o8dxx1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-o8dxx1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-o8dxx1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-bk229p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bk229p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bk229p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-9gk1jr -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9gk1jr -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9gk1jr -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-va51o8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-va51o8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-va51o8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-kp36bs -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kp36bs -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kp36bs -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-moeh54 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-moeh54 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-moeh54 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-nvvzg0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nvvzg0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nvvzg0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-y4zatx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-y4zatx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-y4zatx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-q15911 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-q15911 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-q15911 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-k779xb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-k779xb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-k779xb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-yhcmvc -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yhcmvc -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yhcmvc -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ubuibi -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ubuibi -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ubuibi -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-h9y848 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-h9y848 2>/dev/null || true\nkubectl run db-pod-cks-gdjg87 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gdjg87 2>/dev/null || true\nkubectl run db-pod-cks-ol8lwk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ol8lwk 2>/dev/null || true\nkubectl run db-pod-cks-zoqqz5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-zoqqz5 2>/dev/null || true\nkubectl run db-pod-cks-diwhyj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-diwhyj 2>/dev/null || true\nkubectl run db-pod-cks-255m3g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-255m3g 2>/dev/null || true\nkubectl run db-pod-cks-9ru34d --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9ru34d 2>/dev/null || true\nkubectl run db-pod-cks-glmo0p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-glmo0p 2>/dev/null || true\nkubectl run db-pod-cks-o8dxx1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-o8dxx1 2>/dev/null || true\nkubectl run db-pod-cks-bk229p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bk229p 2>/dev/null || true\nkubectl run db-pod-cks-9gk1jr --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9gk1jr 2>/dev/null || true\nkubectl run db-pod-cks-va51o8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-va51o8 2>/dev/null || true\nkubectl run db-pod-cks-kp36bs --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kp36bs 2>/dev/null || true\nkubectl run db-pod-cks-moeh54 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-moeh54 2>/dev/null || true\nkubectl run db-pod-cks-nvvzg0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nvvzg0 2>/dev/null || true\nkubectl run db-pod-cks-y4zatx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-y4zatx 2>/dev/null || true\nkubectl run db-pod-cks-q15911 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-q15911 2>/dev/null || true\nkubectl run db-pod-cks-k779xb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-k779xb 2>/dev/null || true\nkubectl run db-pod-cks-yhcmvc --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yhcmvc 2>/dev/null || true\nkubectl run db-pod-cks-ubuibi --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ubuibi 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 3)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-h9y848` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-h9y848 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-h9y848 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-h9y848 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-h9y848 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-h9y848 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-gdjg87` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-gdjg87 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gdjg87 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gdjg87 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-gdjg87 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gdjg87 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-ol8lwk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ol8lwk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ol8lwk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ol8lwk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ol8lwk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ol8lwk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-zoqqz5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-zoqqz5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-zoqqz5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-zoqqz5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-zoqqz5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-zoqqz5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-diwhyj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-diwhyj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-diwhyj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-diwhyj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-diwhyj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-diwhyj 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-255m3g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-255m3g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-255m3g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-255m3g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-255m3g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-255m3g 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-9ru34d` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-9ru34d -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9ru34d -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9ru34d -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-9ru34d --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9ru34d 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-glmo0p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-glmo0p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-glmo0p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-glmo0p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-glmo0p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-glmo0p 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-o8dxx1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-o8dxx1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-o8dxx1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-o8dxx1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-o8dxx1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-o8dxx1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-bk229p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-bk229p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bk229p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bk229p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-bk229p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bk229p 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-9gk1jr` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-9gk1jr -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9gk1jr -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9gk1jr -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-9gk1jr --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9gk1jr 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-va51o8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-va51o8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-va51o8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-va51o8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-va51o8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-va51o8 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-kp36bs` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-kp36bs -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kp36bs -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kp36bs -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-kp36bs --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kp36bs 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-moeh54` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-moeh54 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-moeh54 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-moeh54 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-moeh54 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-moeh54 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-nvvzg0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-nvvzg0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nvvzg0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nvvzg0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-nvvzg0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nvvzg0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-y4zatx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-y4zatx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-y4zatx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-y4zatx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-y4zatx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-y4zatx 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-q15911` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-q15911 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-q15911 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-q15911 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-q15911 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-q15911 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-k779xb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-k779xb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-k779xb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-k779xb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-k779xb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-k779xb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-yhcmvc` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-yhcmvc -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yhcmvc -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yhcmvc -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-yhcmvc --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yhcmvc 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-ubuibi` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ubuibi -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ubuibi -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ubuibi -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ubuibi --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ubuibi 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-4": {
    "id": "auto-cks-4",
    "title": "Auto CKS Practice (Batch 4)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 4)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-thgtyt` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-0uyzvf` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-tbl5sd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-yuyb85` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-iqfq64` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-2iap2f` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-gbtk0d` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-sn3zxu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-clixwt` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-uua8zp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-ajspin` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-menyqk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-ervgei` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-p3fif9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-5rjjym` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-f3u7j8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-nq6ar7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-53zjx5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-kjb1w1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-lu6tmp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-thgtyt -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-thgtyt -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-thgtyt -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-0uyzvf -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0uyzvf -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0uyzvf -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-tbl5sd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tbl5sd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tbl5sd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-yuyb85 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yuyb85 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yuyb85 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-iqfq64 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iqfq64 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iqfq64 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-2iap2f -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2iap2f -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2iap2f -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-gbtk0d -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gbtk0d -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gbtk0d -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-sn3zxu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sn3zxu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sn3zxu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-clixwt -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-clixwt -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-clixwt -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-uua8zp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-uua8zp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-uua8zp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ajspin -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ajspin -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ajspin -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-menyqk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-menyqk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-menyqk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ervgei -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ervgei -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ervgei -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-p3fif9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-p3fif9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-p3fif9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-5rjjym -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5rjjym -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5rjjym -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-f3u7j8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-f3u7j8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-f3u7j8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-nq6ar7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nq6ar7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nq6ar7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-53zjx5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-53zjx5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-53zjx5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-kjb1w1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kjb1w1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kjb1w1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-lu6tmp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lu6tmp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lu6tmp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-thgtyt --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-thgtyt 2>/dev/null || true\nkubectl run db-pod-cks-0uyzvf --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0uyzvf 2>/dev/null || true\nkubectl run db-pod-cks-tbl5sd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tbl5sd 2>/dev/null || true\nkubectl run db-pod-cks-yuyb85 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yuyb85 2>/dev/null || true\nkubectl run db-pod-cks-iqfq64 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iqfq64 2>/dev/null || true\nkubectl run db-pod-cks-2iap2f --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2iap2f 2>/dev/null || true\nkubectl run db-pod-cks-gbtk0d --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gbtk0d 2>/dev/null || true\nkubectl run db-pod-cks-sn3zxu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sn3zxu 2>/dev/null || true\nkubectl run db-pod-cks-clixwt --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-clixwt 2>/dev/null || true\nkubectl run db-pod-cks-uua8zp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-uua8zp 2>/dev/null || true\nkubectl run db-pod-cks-ajspin --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ajspin 2>/dev/null || true\nkubectl run db-pod-cks-menyqk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-menyqk 2>/dev/null || true\nkubectl run db-pod-cks-ervgei --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ervgei 2>/dev/null || true\nkubectl run db-pod-cks-p3fif9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-p3fif9 2>/dev/null || true\nkubectl run db-pod-cks-5rjjym --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5rjjym 2>/dev/null || true\nkubectl run db-pod-cks-f3u7j8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-f3u7j8 2>/dev/null || true\nkubectl run db-pod-cks-nq6ar7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nq6ar7 2>/dev/null || true\nkubectl run db-pod-cks-53zjx5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-53zjx5 2>/dev/null || true\nkubectl run db-pod-cks-kjb1w1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kjb1w1 2>/dev/null || true\nkubectl run db-pod-cks-lu6tmp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lu6tmp 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 4)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-thgtyt` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-thgtyt -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-thgtyt -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-thgtyt -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-thgtyt --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-thgtyt 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-0uyzvf` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-0uyzvf -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0uyzvf -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0uyzvf -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-0uyzvf --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0uyzvf 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-tbl5sd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-tbl5sd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tbl5sd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tbl5sd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-tbl5sd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tbl5sd 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-yuyb85` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-yuyb85 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yuyb85 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yuyb85 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-yuyb85 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yuyb85 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-iqfq64` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-iqfq64 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-iqfq64 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-iqfq64 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-iqfq64 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-iqfq64 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-2iap2f` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-2iap2f -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2iap2f -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2iap2f -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-2iap2f --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2iap2f 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-gbtk0d` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-gbtk0d -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-gbtk0d -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-gbtk0d -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-gbtk0d --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-gbtk0d 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-sn3zxu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-sn3zxu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sn3zxu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sn3zxu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-sn3zxu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sn3zxu 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-clixwt` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-clixwt -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-clixwt -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-clixwt -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-clixwt --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-clixwt 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-uua8zp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-uua8zp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-uua8zp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-uua8zp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-uua8zp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-uua8zp 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-ajspin` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ajspin -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ajspin -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ajspin -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ajspin --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ajspin 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-menyqk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-menyqk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-menyqk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-menyqk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-menyqk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-menyqk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-ervgei` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ervgei -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ervgei -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ervgei -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ervgei --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ervgei 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-p3fif9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-p3fif9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-p3fif9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-p3fif9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-p3fif9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-p3fif9 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-5rjjym` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-5rjjym -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5rjjym -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5rjjym -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-5rjjym --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5rjjym 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-f3u7j8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-f3u7j8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-f3u7j8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-f3u7j8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-f3u7j8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-f3u7j8 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-nq6ar7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-nq6ar7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nq6ar7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nq6ar7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-nq6ar7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nq6ar7 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-53zjx5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-53zjx5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-53zjx5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-53zjx5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-53zjx5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-53zjx5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-kjb1w1` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-kjb1w1 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kjb1w1 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kjb1w1 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-kjb1w1 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kjb1w1 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-lu6tmp` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-lu6tmp -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lu6tmp -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lu6tmp -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-lu6tmp --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lu6tmp 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-5": {
    "id": "auto-cks-5",
    "title": "Auto CKS Practice (Batch 5)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 5)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-ahrv98` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-w9wgtq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-12xqnb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-dp4wj7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-s2ami4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-oer3i8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-oll1my` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-7ezs46` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-ac6ezl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-43kz7u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-9rv7k6` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-3r88pw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-a6tn7p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-ya4rw4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-u6vp7j` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-bgik2q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-0xjyjq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-3mbr5v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-psce49` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-erqb02` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-ahrv98 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ahrv98 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ahrv98 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-w9wgtq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-w9wgtq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-w9wgtq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-12xqnb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-12xqnb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-12xqnb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-dp4wj7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dp4wj7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dp4wj7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-s2ami4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-s2ami4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-s2ami4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-oer3i8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oer3i8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oer3i8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-oll1my -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oll1my -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oll1my -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-7ezs46 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7ezs46 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7ezs46 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ac6ezl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ac6ezl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ac6ezl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-43kz7u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-43kz7u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-43kz7u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-9rv7k6 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9rv7k6 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9rv7k6 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3r88pw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3r88pw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3r88pw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-a6tn7p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-a6tn7p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-a6tn7p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ya4rw4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ya4rw4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ya4rw4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-u6vp7j -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-u6vp7j -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-u6vp7j -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-bgik2q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bgik2q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bgik2q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-0xjyjq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0xjyjq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0xjyjq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3mbr5v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3mbr5v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3mbr5v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-psce49 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-psce49 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-psce49 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-erqb02 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-erqb02 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-erqb02 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-ahrv98 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ahrv98 2>/dev/null || true\nkubectl run db-pod-cks-w9wgtq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-w9wgtq 2>/dev/null || true\nkubectl run db-pod-cks-12xqnb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-12xqnb 2>/dev/null || true\nkubectl run db-pod-cks-dp4wj7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dp4wj7 2>/dev/null || true\nkubectl run db-pod-cks-s2ami4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-s2ami4 2>/dev/null || true\nkubectl run db-pod-cks-oer3i8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oer3i8 2>/dev/null || true\nkubectl run db-pod-cks-oll1my --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oll1my 2>/dev/null || true\nkubectl run db-pod-cks-7ezs46 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7ezs46 2>/dev/null || true\nkubectl run db-pod-cks-ac6ezl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ac6ezl 2>/dev/null || true\nkubectl run db-pod-cks-43kz7u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-43kz7u 2>/dev/null || true\nkubectl run db-pod-cks-9rv7k6 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9rv7k6 2>/dev/null || true\nkubectl run db-pod-cks-3r88pw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3r88pw 2>/dev/null || true\nkubectl run db-pod-cks-a6tn7p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-a6tn7p 2>/dev/null || true\nkubectl run db-pod-cks-ya4rw4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ya4rw4 2>/dev/null || true\nkubectl run db-pod-cks-u6vp7j --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-u6vp7j 2>/dev/null || true\nkubectl run db-pod-cks-bgik2q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bgik2q 2>/dev/null || true\nkubectl run db-pod-cks-0xjyjq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0xjyjq 2>/dev/null || true\nkubectl run db-pod-cks-3mbr5v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3mbr5v 2>/dev/null || true\nkubectl run db-pod-cks-psce49 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-psce49 2>/dev/null || true\nkubectl run db-pod-cks-erqb02 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-erqb02 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 5)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-ahrv98` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ahrv98 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ahrv98 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ahrv98 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ahrv98 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ahrv98 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-w9wgtq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-w9wgtq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-w9wgtq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-w9wgtq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-w9wgtq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-w9wgtq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-12xqnb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-12xqnb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-12xqnb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-12xqnb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-12xqnb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-12xqnb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-dp4wj7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-dp4wj7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dp4wj7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dp4wj7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-dp4wj7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dp4wj7 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-s2ami4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-s2ami4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-s2ami4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-s2ami4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-s2ami4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-s2ami4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-oer3i8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-oer3i8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oer3i8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oer3i8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-oer3i8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oer3i8 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-oll1my` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-oll1my -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oll1my -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oll1my -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-oll1my --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oll1my 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-7ezs46` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-7ezs46 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7ezs46 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7ezs46 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-7ezs46 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7ezs46 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-ac6ezl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ac6ezl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ac6ezl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ac6ezl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ac6ezl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ac6ezl 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-43kz7u` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-43kz7u -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-43kz7u -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-43kz7u -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-43kz7u --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-43kz7u 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-9rv7k6` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-9rv7k6 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9rv7k6 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9rv7k6 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-9rv7k6 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9rv7k6 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-3r88pw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3r88pw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3r88pw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3r88pw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3r88pw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3r88pw 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-a6tn7p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-a6tn7p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-a6tn7p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-a6tn7p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-a6tn7p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-a6tn7p 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-ya4rw4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ya4rw4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ya4rw4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ya4rw4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ya4rw4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ya4rw4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-u6vp7j` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-u6vp7j -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-u6vp7j -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-u6vp7j -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-u6vp7j --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-u6vp7j 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-bgik2q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-bgik2q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-bgik2q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-bgik2q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-bgik2q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-bgik2q 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-0xjyjq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-0xjyjq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0xjyjq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0xjyjq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-0xjyjq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0xjyjq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-3mbr5v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3mbr5v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3mbr5v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3mbr5v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3mbr5v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3mbr5v 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-psce49` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-psce49 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-psce49 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-psce49 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-psce49 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-psce49 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-erqb02` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-erqb02 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-erqb02 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-erqb02 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-erqb02 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-erqb02 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-6": {
    "id": "auto-cks-6",
    "title": "Auto CKS Practice (Batch 6)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 6)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-7svncn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-6ymetv` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-64rkxo` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-n07bjx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-ecrqyq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-sv5b5p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-df3ijq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-l8wral` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-i6m54k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-nhg95q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-05cinb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-74ihvb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-cnr39o` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-fay2qk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-cuu080` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-pod0v7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-dwxlc8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-4vn9qn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-ndomaj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-5gh2io` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-7svncn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7svncn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7svncn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-6ymetv -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6ymetv -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6ymetv -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-64rkxo -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-64rkxo -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-64rkxo -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-n07bjx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n07bjx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n07bjx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ecrqyq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ecrqyq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ecrqyq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-sv5b5p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sv5b5p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sv5b5p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-df3ijq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-df3ijq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-df3ijq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-l8wral -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-l8wral -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-l8wral -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-i6m54k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-i6m54k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-i6m54k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-nhg95q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nhg95q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nhg95q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-05cinb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-05cinb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-05cinb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-74ihvb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-74ihvb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-74ihvb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-cnr39o -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cnr39o -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cnr39o -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-fay2qk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-fay2qk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-fay2qk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-cuu080 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cuu080 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cuu080 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-pod0v7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-pod0v7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-pod0v7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-dwxlc8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dwxlc8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dwxlc8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-4vn9qn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4vn9qn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4vn9qn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-ndomaj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ndomaj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ndomaj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-5gh2io -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5gh2io -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5gh2io -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-7svncn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7svncn 2>/dev/null || true\nkubectl run db-pod-cks-6ymetv --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6ymetv 2>/dev/null || true\nkubectl run db-pod-cks-64rkxo --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-64rkxo 2>/dev/null || true\nkubectl run db-pod-cks-n07bjx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n07bjx 2>/dev/null || true\nkubectl run db-pod-cks-ecrqyq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ecrqyq 2>/dev/null || true\nkubectl run db-pod-cks-sv5b5p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sv5b5p 2>/dev/null || true\nkubectl run db-pod-cks-df3ijq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-df3ijq 2>/dev/null || true\nkubectl run db-pod-cks-l8wral --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-l8wral 2>/dev/null || true\nkubectl run db-pod-cks-i6m54k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-i6m54k 2>/dev/null || true\nkubectl run db-pod-cks-nhg95q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nhg95q 2>/dev/null || true\nkubectl run db-pod-cks-05cinb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-05cinb 2>/dev/null || true\nkubectl run db-pod-cks-74ihvb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-74ihvb 2>/dev/null || true\nkubectl run db-pod-cks-cnr39o --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cnr39o 2>/dev/null || true\nkubectl run db-pod-cks-fay2qk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-fay2qk 2>/dev/null || true\nkubectl run db-pod-cks-cuu080 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cuu080 2>/dev/null || true\nkubectl run db-pod-cks-pod0v7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-pod0v7 2>/dev/null || true\nkubectl run db-pod-cks-dwxlc8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dwxlc8 2>/dev/null || true\nkubectl run db-pod-cks-4vn9qn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4vn9qn 2>/dev/null || true\nkubectl run db-pod-cks-ndomaj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ndomaj 2>/dev/null || true\nkubectl run db-pod-cks-5gh2io --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5gh2io 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 6)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-7svncn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-7svncn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-7svncn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-7svncn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-7svncn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-7svncn 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-6ymetv` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-6ymetv -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-6ymetv -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-6ymetv -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-6ymetv --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-6ymetv 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-64rkxo` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-64rkxo -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-64rkxo -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-64rkxo -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-64rkxo --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-64rkxo 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-n07bjx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-n07bjx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n07bjx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n07bjx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-n07bjx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n07bjx 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-ecrqyq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ecrqyq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ecrqyq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ecrqyq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ecrqyq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ecrqyq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-sv5b5p` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-sv5b5p -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sv5b5p -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sv5b5p -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-sv5b5p --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sv5b5p 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-df3ijq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-df3ijq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-df3ijq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-df3ijq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-df3ijq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-df3ijq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-l8wral` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-l8wral -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-l8wral -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-l8wral -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-l8wral --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-l8wral 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-i6m54k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-i6m54k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-i6m54k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-i6m54k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-i6m54k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-i6m54k 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-nhg95q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-nhg95q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nhg95q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nhg95q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-nhg95q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nhg95q 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-05cinb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-05cinb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-05cinb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-05cinb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-05cinb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-05cinb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-74ihvb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-74ihvb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-74ihvb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-74ihvb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-74ihvb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-74ihvb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-cnr39o` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-cnr39o -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cnr39o -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cnr39o -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-cnr39o --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cnr39o 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-fay2qk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-fay2qk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-fay2qk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-fay2qk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-fay2qk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-fay2qk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-cuu080` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-cuu080 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cuu080 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cuu080 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-cuu080 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cuu080 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-pod0v7` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-pod0v7 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-pod0v7 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-pod0v7 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-pod0v7 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-pod0v7 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-dwxlc8` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-dwxlc8 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-dwxlc8 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-dwxlc8 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-dwxlc8 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-dwxlc8 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-4vn9qn` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-4vn9qn -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4vn9qn -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4vn9qn -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-4vn9qn --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4vn9qn 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-ndomaj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-ndomaj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-ndomaj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-ndomaj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-ndomaj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-ndomaj 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-5gh2io` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-5gh2io -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5gh2io -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5gh2io -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-5gh2io --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5gh2io 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-7": {
    "id": "auto-cks-7",
    "title": "Auto CKS Practice (Batch 7)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 7)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-jwd24y` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-1ubovm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-yn50gz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-5imybr` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-687o4w` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-wco2tc` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-naasnj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-r2jm0s` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-f2ptbd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-tn7cno` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-k2yo7v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-hlw77g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-rkq9y0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-f3kyqf` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-jbfzza` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-y6gqth` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-9a5ia9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-70j0mm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-shsf64` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-e7bry0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-jwd24y -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jwd24y -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jwd24y -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-1ubovm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1ubovm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1ubovm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-yn50gz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yn50gz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yn50gz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-5imybr -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5imybr -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5imybr -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-687o4w -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-687o4w -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-687o4w -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-wco2tc -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-wco2tc -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-wco2tc -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-naasnj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-naasnj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-naasnj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-r2jm0s -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r2jm0s -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r2jm0s -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-f2ptbd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-f2ptbd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-f2ptbd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-tn7cno -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tn7cno -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tn7cno -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-k2yo7v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-k2yo7v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-k2yo7v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-hlw77g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hlw77g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hlw77g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-rkq9y0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rkq9y0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rkq9y0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-f3kyqf -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-f3kyqf -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-f3kyqf -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-jbfzza -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jbfzza -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jbfzza -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-y6gqth -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-y6gqth -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-y6gqth -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-9a5ia9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9a5ia9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9a5ia9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-70j0mm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-70j0mm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-70j0mm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-shsf64 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-shsf64 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-shsf64 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-e7bry0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e7bry0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e7bry0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-jwd24y --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jwd24y 2>/dev/null || true\nkubectl run db-pod-cks-1ubovm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1ubovm 2>/dev/null || true\nkubectl run db-pod-cks-yn50gz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yn50gz 2>/dev/null || true\nkubectl run db-pod-cks-5imybr --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5imybr 2>/dev/null || true\nkubectl run db-pod-cks-687o4w --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-687o4w 2>/dev/null || true\nkubectl run db-pod-cks-wco2tc --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-wco2tc 2>/dev/null || true\nkubectl run db-pod-cks-naasnj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-naasnj 2>/dev/null || true\nkubectl run db-pod-cks-r2jm0s --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r2jm0s 2>/dev/null || true\nkubectl run db-pod-cks-f2ptbd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-f2ptbd 2>/dev/null || true\nkubectl run db-pod-cks-tn7cno --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tn7cno 2>/dev/null || true\nkubectl run db-pod-cks-k2yo7v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-k2yo7v 2>/dev/null || true\nkubectl run db-pod-cks-hlw77g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hlw77g 2>/dev/null || true\nkubectl run db-pod-cks-rkq9y0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rkq9y0 2>/dev/null || true\nkubectl run db-pod-cks-f3kyqf --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-f3kyqf 2>/dev/null || true\nkubectl run db-pod-cks-jbfzza --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jbfzza 2>/dev/null || true\nkubectl run db-pod-cks-y6gqth --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-y6gqth 2>/dev/null || true\nkubectl run db-pod-cks-9a5ia9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9a5ia9 2>/dev/null || true\nkubectl run db-pod-cks-70j0mm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-70j0mm 2>/dev/null || true\nkubectl run db-pod-cks-shsf64 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-shsf64 2>/dev/null || true\nkubectl run db-pod-cks-e7bry0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e7bry0 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 7)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-jwd24y` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-jwd24y -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jwd24y -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jwd24y -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-jwd24y --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jwd24y 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-1ubovm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-1ubovm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1ubovm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1ubovm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-1ubovm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1ubovm 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-yn50gz` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-yn50gz -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yn50gz -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yn50gz -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-yn50gz --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yn50gz 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-5imybr` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-5imybr -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5imybr -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5imybr -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-5imybr --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5imybr 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-687o4w` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-687o4w -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-687o4w -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-687o4w -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-687o4w --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-687o4w 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-wco2tc` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-wco2tc -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-wco2tc -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-wco2tc -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-wco2tc --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-wco2tc 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-naasnj` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-naasnj -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-naasnj -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-naasnj -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-naasnj --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-naasnj 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-r2jm0s` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-r2jm0s -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r2jm0s -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r2jm0s -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-r2jm0s --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r2jm0s 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-f2ptbd` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-f2ptbd -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-f2ptbd -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-f2ptbd -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-f2ptbd --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-f2ptbd 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-tn7cno` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-tn7cno -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-tn7cno -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-tn7cno -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-tn7cno --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-tn7cno 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-k2yo7v` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-k2yo7v -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-k2yo7v -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-k2yo7v -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-k2yo7v --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-k2yo7v 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-hlw77g` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-hlw77g -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-hlw77g -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-hlw77g -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-hlw77g --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-hlw77g 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-rkq9y0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-rkq9y0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rkq9y0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rkq9y0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-rkq9y0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rkq9y0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-f3kyqf` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-f3kyqf -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-f3kyqf -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-f3kyqf -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-f3kyqf --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-f3kyqf 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-jbfzza` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-jbfzza -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-jbfzza -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-jbfzza -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-jbfzza --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-jbfzza 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-y6gqth` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-y6gqth -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-y6gqth -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-y6gqth -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-y6gqth --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-y6gqth 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-9a5ia9` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-9a5ia9 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-9a5ia9 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-9a5ia9 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-9a5ia9 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-9a5ia9 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-70j0mm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-70j0mm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-70j0mm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-70j0mm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-70j0mm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-70j0mm 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-shsf64` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-shsf64 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-shsf64 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-shsf64 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-shsf64 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-shsf64 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-e7bry0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-e7bry0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-e7bry0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-e7bry0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-e7bry0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-e7bry0 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-8": {
    "id": "auto-cks-8",
    "title": "Auto CKS Practice (Batch 8)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 8)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-s68uu0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-utrdgl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-yd2s92` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-nfq80c` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-lqt5et` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-8tock4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-skhbui` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-vs2fxk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-2e1s85` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-rk2cyx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-r0mm4s` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-y8dkse` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-3av96i` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-xl0wba` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-5j7ut5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-qwkmvt` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-m9rxds` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-brfomy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-3yfhrb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-o0yfws` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-s68uu0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-s68uu0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-s68uu0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-utrdgl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-utrdgl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-utrdgl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-yd2s92 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yd2s92 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yd2s92 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-nfq80c -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nfq80c -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nfq80c -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-lqt5et -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lqt5et -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lqt5et -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-8tock4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-8tock4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-8tock4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-skhbui -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-skhbui -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-skhbui -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-vs2fxk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-vs2fxk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-vs2fxk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-2e1s85 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2e1s85 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2e1s85 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-rk2cyx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rk2cyx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rk2cyx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-r0mm4s -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r0mm4s -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r0mm4s -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-y8dkse -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-y8dkse -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-y8dkse -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3av96i -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3av96i -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3av96i -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-xl0wba -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-xl0wba -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-xl0wba -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-5j7ut5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5j7ut5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5j7ut5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-qwkmvt -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-qwkmvt -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-qwkmvt -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-m9rxds -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-m9rxds -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-m9rxds -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-brfomy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-brfomy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-brfomy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-3yfhrb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3yfhrb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3yfhrb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-o0yfws -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-o0yfws -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-o0yfws -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-s68uu0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-s68uu0 2>/dev/null || true\nkubectl run db-pod-cks-utrdgl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-utrdgl 2>/dev/null || true\nkubectl run db-pod-cks-yd2s92 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yd2s92 2>/dev/null || true\nkubectl run db-pod-cks-nfq80c --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nfq80c 2>/dev/null || true\nkubectl run db-pod-cks-lqt5et --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lqt5et 2>/dev/null || true\nkubectl run db-pod-cks-8tock4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-8tock4 2>/dev/null || true\nkubectl run db-pod-cks-skhbui --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-skhbui 2>/dev/null || true\nkubectl run db-pod-cks-vs2fxk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-vs2fxk 2>/dev/null || true\nkubectl run db-pod-cks-2e1s85 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2e1s85 2>/dev/null || true\nkubectl run db-pod-cks-rk2cyx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rk2cyx 2>/dev/null || true\nkubectl run db-pod-cks-r0mm4s --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r0mm4s 2>/dev/null || true\nkubectl run db-pod-cks-y8dkse --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-y8dkse 2>/dev/null || true\nkubectl run db-pod-cks-3av96i --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3av96i 2>/dev/null || true\nkubectl run db-pod-cks-xl0wba --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-xl0wba 2>/dev/null || true\nkubectl run db-pod-cks-5j7ut5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5j7ut5 2>/dev/null || true\nkubectl run db-pod-cks-qwkmvt --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-qwkmvt 2>/dev/null || true\nkubectl run db-pod-cks-m9rxds --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-m9rxds 2>/dev/null || true\nkubectl run db-pod-cks-brfomy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-brfomy 2>/dev/null || true\nkubectl run db-pod-cks-3yfhrb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3yfhrb 2>/dev/null || true\nkubectl run db-pod-cks-o0yfws --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-o0yfws 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 8)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-s68uu0` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-s68uu0 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-s68uu0 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-s68uu0 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-s68uu0 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-s68uu0 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-utrdgl` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-utrdgl -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-utrdgl -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-utrdgl -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-utrdgl --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-utrdgl 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-yd2s92` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-yd2s92 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-yd2s92 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-yd2s92 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-yd2s92 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-yd2s92 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-nfq80c` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-nfq80c -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-nfq80c -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-nfq80c -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-nfq80c --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-nfq80c 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-lqt5et` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-lqt5et -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-lqt5et -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-lqt5et -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-lqt5et --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-lqt5et 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-8tock4` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-8tock4 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-8tock4 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-8tock4 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-8tock4 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-8tock4 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-skhbui` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-skhbui -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-skhbui -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-skhbui -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-skhbui --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-skhbui 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-vs2fxk` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-vs2fxk -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-vs2fxk -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-vs2fxk -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-vs2fxk --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-vs2fxk 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-2e1s85` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-2e1s85 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-2e1s85 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-2e1s85 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-2e1s85 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-2e1s85 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-rk2cyx` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-rk2cyx -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rk2cyx -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rk2cyx -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-rk2cyx --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rk2cyx 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-r0mm4s` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-r0mm4s -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-r0mm4s -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-r0mm4s -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-r0mm4s --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-r0mm4s 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-y8dkse` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-y8dkse -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-y8dkse -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-y8dkse -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-y8dkse --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-y8dkse 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-3av96i` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3av96i -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3av96i -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3av96i -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3av96i --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3av96i 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-xl0wba` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-xl0wba -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-xl0wba -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-xl0wba -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-xl0wba --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-xl0wba 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-5j7ut5` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-5j7ut5 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5j7ut5 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5j7ut5 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-5j7ut5 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5j7ut5 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-qwkmvt` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-qwkmvt -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-qwkmvt -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-qwkmvt -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-qwkmvt --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-qwkmvt 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-m9rxds` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-m9rxds -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-m9rxds -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-m9rxds -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-m9rxds --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-m9rxds 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-brfomy` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-brfomy -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-brfomy -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-brfomy -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-brfomy --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-brfomy 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-3yfhrb` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-3yfhrb -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-3yfhrb -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-3yfhrb -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-3yfhrb --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-3yfhrb 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-o0yfws` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-o0yfws -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-o0yfws -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-o0yfws -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-o0yfws --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-o0yfws 2>/dev/null || true\n"
      }
    ]
  },
  "auto-cks-9": {
    "id": "auto-cks-9",
    "title": "Auto CKS Practice (Batch 9)",
    "category": "CKS",
    "duration": "120 mins",
    "markdown": "\n# Auto CKS Practice (Batch 9)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-0o9dln` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-rwz5vu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-0w04xm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-n8q051` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-1m6bkg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-btid6b` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-zun9at` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-4rfn3k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-cw2dfw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-sdlefm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-t3w306` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-oakvby` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-mds10z` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-07p8p2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-5ttamu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-t9yz7q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-89ecgv` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-1wwiaq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-eimele` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-kyz4kh` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
    "verifyScript": "kubectl get netpol cks-0o9dln -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0o9dln -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0o9dln -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-rwz5vu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rwz5vu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rwz5vu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-0w04xm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0w04xm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0w04xm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-n8q051 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n8q051 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n8q051 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-1m6bkg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1m6bkg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1m6bkg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-btid6b -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-btid6b -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-btid6b -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-zun9at -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-zun9at -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-zun9at -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-4rfn3k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4rfn3k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4rfn3k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-cw2dfw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cw2dfw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cw2dfw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-sdlefm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sdlefm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sdlefm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-t3w306 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t3w306 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t3w306 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-oakvby -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oakvby -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oakvby -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-mds10z -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-mds10z -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-mds10z -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-07p8p2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-07p8p2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-07p8p2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-5ttamu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5ttamu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5ttamu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-t9yz7q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t9yz7q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t9yz7q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-89ecgv -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-89ecgv -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-89ecgv -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-1wwiaq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1wwiaq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1wwiaq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-eimele -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-eimele -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-eimele -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\nkubectl get netpol cks-kyz4kh -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kyz4kh -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kyz4kh -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
    "setupScript": "kubectl run db-pod-cks-0o9dln --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0o9dln 2>/dev/null || true\nkubectl run db-pod-cks-rwz5vu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rwz5vu 2>/dev/null || true\nkubectl run db-pod-cks-0w04xm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0w04xm 2>/dev/null || true\nkubectl run db-pod-cks-n8q051 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n8q051 2>/dev/null || true\nkubectl run db-pod-cks-1m6bkg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1m6bkg 2>/dev/null || true\nkubectl run db-pod-cks-btid6b --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-btid6b 2>/dev/null || true\nkubectl run db-pod-cks-zun9at --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-zun9at 2>/dev/null || true\nkubectl run db-pod-cks-4rfn3k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4rfn3k 2>/dev/null || true\nkubectl run db-pod-cks-cw2dfw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cw2dfw 2>/dev/null || true\nkubectl run db-pod-cks-sdlefm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sdlefm 2>/dev/null || true\nkubectl run db-pod-cks-t3w306 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t3w306 2>/dev/null || true\nkubectl run db-pod-cks-oakvby --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oakvby 2>/dev/null || true\nkubectl run db-pod-cks-mds10z --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-mds10z 2>/dev/null || true\nkubectl run db-pod-cks-07p8p2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-07p8p2 2>/dev/null || true\nkubectl run db-pod-cks-5ttamu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5ttamu 2>/dev/null || true\nkubectl run db-pod-cks-t9yz7q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t9yz7q 2>/dev/null || true\nkubectl run db-pod-cks-89ecgv --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-89ecgv 2>/dev/null || true\nkubectl run db-pod-cks-1wwiaq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1wwiaq 2>/dev/null || true\nkubectl run db-pod-cks-eimele --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-eimele 2>/dev/null || true\nkubectl run db-pod-cks-kyz4kh --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kyz4kh 2>/dev/null || true\n",
    "tasks": [
      {
        "markdown": "# Task \n# Auto CKS Practice (Batch 9)\n\nThis is an auto-generated exam to provide infinite practice questions.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 1: Network Policy\nCreate a NetworkPolicy named `cks-0o9dln` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-0o9dln -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0o9dln -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0o9dln -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-0o9dln --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0o9dln 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 2: Network Policy\nCreate a NetworkPolicy named `cks-rwz5vu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-rwz5vu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-rwz5vu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-rwz5vu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-rwz5vu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-rwz5vu 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 3: Network Policy\nCreate a NetworkPolicy named `cks-0w04xm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-0w04xm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-0w04xm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-0w04xm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-0w04xm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-0w04xm 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 4: Network Policy\nCreate a NetworkPolicy named `cks-n8q051` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-n8q051 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-n8q051 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-n8q051 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-n8q051 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-n8q051 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 5: Network Policy\nCreate a NetworkPolicy named `cks-1m6bkg` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-1m6bkg -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1m6bkg -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1m6bkg -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-1m6bkg --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1m6bkg 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 6: Network Policy\nCreate a NetworkPolicy named `cks-btid6b` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-btid6b -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-btid6b -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-btid6b -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-btid6b --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-btid6b 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 7: Network Policy\nCreate a NetworkPolicy named `cks-zun9at` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-zun9at -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-zun9at -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-zun9at -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-zun9at --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-zun9at 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 8: Network Policy\nCreate a NetworkPolicy named `cks-4rfn3k` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-4rfn3k -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-4rfn3k -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-4rfn3k -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-4rfn3k --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-4rfn3k 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 9: Network Policy\nCreate a NetworkPolicy named `cks-cw2dfw` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-cw2dfw -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-cw2dfw -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-cw2dfw -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-cw2dfw --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-cw2dfw 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 10: Network Policy\nCreate a NetworkPolicy named `cks-sdlefm` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-sdlefm -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-sdlefm -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-sdlefm -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-sdlefm --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-sdlefm 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 11: Network Policy\nCreate a NetworkPolicy named `cks-t3w306` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-t3w306 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t3w306 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t3w306 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-t3w306 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t3w306 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 12: Network Policy\nCreate a NetworkPolicy named `cks-oakvby` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-oakvby -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-oakvby -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-oakvby -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-oakvby --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-oakvby 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 13: Network Policy\nCreate a NetworkPolicy named `cks-mds10z` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-mds10z -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-mds10z -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-mds10z -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-mds10z --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-mds10z 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 14: Network Policy\nCreate a NetworkPolicy named `cks-07p8p2` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-07p8p2 -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-07p8p2 -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-07p8p2 -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-07p8p2 --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-07p8p2 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 15: Network Policy\nCreate a NetworkPolicy named `cks-5ttamu` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-5ttamu -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-5ttamu -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-5ttamu -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-5ttamu --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-5ttamu 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 16: Network Policy\nCreate a NetworkPolicy named `cks-t9yz7q` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-t9yz7q -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-t9yz7q -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-t9yz7q -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-t9yz7q --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-t9yz7q 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 17: Network Policy\nCreate a NetworkPolicy named `cks-89ecgv` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-89ecgv -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-89ecgv -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-89ecgv -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-89ecgv --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-89ecgv 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 18: Network Policy\nCreate a NetworkPolicy named `cks-1wwiaq` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-1wwiaq -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-1wwiaq -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-1wwiaq -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-1wwiaq --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-1wwiaq 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 19: Network Policy\nCreate a NetworkPolicy named `cks-eimele` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-eimele -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-eimele -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-eimele -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-eimele --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-eimele 2>/dev/null || true\n"
      },
      {
        "markdown": "# Task 20: Network Policy\nCreate a NetworkPolicy named `cks-kyz4kh` in namespace `default`.\nDeny all ingress traffic to pods with label `role=db`.\nAllow egress only to port 53 (DNS).\n\n\n\n\n\n",
        "verify": "kubectl get netpol cks-kyz4kh -n default -o jsonpath='{.spec.podSelector.matchLabels.role}' | grep db\nkubectl get netpol cks-kyz4kh -n default -o jsonpath='{.spec.policyTypes}' | grep Ingress\nkubectl get netpol cks-kyz4kh -n default -o jsonpath='{.spec.egress[0].ports[0].port}' | grep 53\n",
        "setup": "kubectl run db-pod-cks-kyz4kh --image=nginx --labels=role=db --restart=Never\nkubectl delete netpol cks-kyz4kh 2>/dev/null || true\n"
      }
    ]
  },
  "cka-mock-auto-01": {
    "id": "cka-mock-auto-01",
    "title": "Automated CKA Mock Exam 1",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Task 1: Create a Pod\nCreate a pod named `nginx` with image `nginx`.\n\n```bash\nkubectl run nginx --image=nginx\n```\n\n# Task 2: Service Limit\nCreate a service named `my-service` of type `NodePort`.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": [
      {
        "markdown": "# Task 1: Create a Pod\nCreate a pod named `nginx` with image `nginx`.\n\n```bash\nkubectl run nginx --image=nginx\n```\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 2: Service Limit\nCreate a service named `my-service` of type `NodePort`.\n",
        "verify": "",
        "setup": ""
      }
    ]
  },
  "cka-mock-02": {
    "id": "cka-mock-02",
    "title": "CKA Mock Exam 2 (Pool)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Task 1: ETCD Snapshot\nCreate a snapshot of the etcd instance running on the controlplane node.\nSave it to /opt/etcd-backup.db.\n\n```bash\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  snapshot save /opt/etcd-backup.db\n```\n\n# Task 2: Fix a broken node\nThe node `worker-1` is in `NotReady` state. Investigate why and fix it.\nCheck `systemctl status kubelet`.\n\n# Task 3: Ingress Resource\nCreate an ingress named `my-ingress` that routes path `/hello` to service `hello-service` on port 80.\n\n# Task 4: Private Registry Secret\nCreate a secret named `my-registry-key` of type `docker-registry` with username `user` and password `pass`.\nThen patch the default service account to use it.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": [
      {
        "markdown": "# Task 1: ETCD Snapshot\nCreate a snapshot of the etcd instance running on the controlplane node.\nSave it to /opt/etcd-backup.db.\n\n```bash\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  snapshot save /opt/etcd-backup.db\n```\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 2: Fix a broken node\nThe node `worker-1` is in `NotReady` state. Investigate why and fix it.\nCheck `systemctl status kubelet`.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 3: Ingress Resource\nCreate an ingress named `my-ingress` that routes path `/hello` to service `hello-service` on port 80.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 4: Private Registry Secret\nCreate a secret named `my-registry-key` of type `docker-registry` with username `user` and password `pass`.\nThen patch the default service account to use it.\n",
        "verify": "",
        "setup": ""
      }
    ]
  },
  "cka-mock-03": {
    "id": "cka-mock-03",
    "title": "CKA Mock Exam 3 (Pool)",
    "category": "CKA",
    "duration": "120 mins",
    "markdown": "\n# Task 1: Node Affinity\nCreate a deployment `nginx` with 2 replicas. Ensure they land on nodes with label `disk=ssd`.\n\n# Task 2: Sidecar Container\nEdit the pod `logging-pod`. Add a sidecar container image `busybox` that runs `tail -f /var/log/app.log`.\nThe volume `log-volume` is already mounted at `/var/log`.\n\n# Task 3: ClusterUpgrade\nUpgrade the control plane to version 1.29.0.\nRemember to drain the node first!\n\n# Task 4: Network Policy Access\nAllow traffic from pods with label `role=frontend` to pods with label `role=backend` on port 80.\nDeny all other ingress traffic to backend.\n",
    "verifyScript": "",
    "setupScript": "",
    "tasks": [
      {
        "markdown": "# Task 1: Node Affinity\nCreate a deployment `nginx` with 2 replicas. Ensure they land on nodes with label `disk=ssd`.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 2: Sidecar Container\nEdit the pod `logging-pod`. Add a sidecar container image `busybox` that runs `tail -f /var/log/app.log`.\nThe volume `log-volume` is already mounted at `/var/log`.\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 3: ClusterUpgrade\nUpgrade the control plane to version 1.29.0.\nRemember to drain the node first!\n\n",
        "verify": "",
        "setup": ""
      },
      {
        "markdown": "# Task 4: Network Policy Access\nAllow traffic from pods with label `role=frontend` to pods with label `role=backend` on port 80.\nDeny all other ingress traffic to backend.\n",
        "verify": "",
        "setup": ""
      }
    ]
  }
};
